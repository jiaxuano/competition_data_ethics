{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "import time\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gather links for each competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.kaggle.com/competitions?sortOption=reward&page='\n",
    "\n",
    "# Create a WebDriver instance for Chrome\n",
    "\n",
    "\n",
    "def kaggle_comp_links(start_page, end_page):\n",
    "    base_url = 'https://www.kaggle.com/competitions?sortOption=reward&page='\n",
    "    top_competitions = []\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.maximize_window()\n",
    "\n",
    "    # Note, it seems that we can only scrape 17 pages at a time. Adjust range() to adjust pages you want to scrape\n",
    "    for page in range(start_page, end_page+1):\n",
    "        time.sleep(1.5)\n",
    "        if page == 1:\n",
    "            url = 'https://www.kaggle.com/competitions?sortOption=reward'  # First page URL\n",
    "        else:\n",
    "            url = f'{base_url}{page}'  # Subsequent pages URL\n",
    "        \n",
    "        # Visit the page\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Optional: Click all competitions button if needed - assuming this needs to be clicked each time\n",
    "        if page == 1:\n",
    "            try:\n",
    "                button = WebDriverWait(driver, 20).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, '//*[@id=\"site-content\"]/div[2]/div/div[4]/div/div[2]/div/div[1]/button[1]'))\n",
    "                )\n",
    "                button.click()\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to click 'All Competitions' button on page {page}: {str(e)}\")\n",
    "        \n",
    "        # Wait for the links to be visible and collect them\n",
    "        try:\n",
    "            competition_links = WebDriverWait(driver, 20).until(\n",
    "                EC.presence_of_all_elements_located((By.XPATH, '//*[@id=\"site-content\"]/div[2]/div/div[5]/div/div/div/ul/li/div/a'))\n",
    "            )\n",
    "            top_competitions.extend([link.get_attribute('href') for link in competition_links])\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to collect links on page {page}: {str(e)}\")\n",
    "    driver.quit()\n",
    "    return top_competitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Difficulty in scraping all 34 pages at once, split into two part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_competitions_1 = kaggle_comp_links(1, 18)\n",
    "top_competitions_2 = kaggle_comp_links(19, 34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 315)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_competitions_1), len(top_competitions_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.kaggle.com/competitions/ai-mathematical-olympiad-progress-prize-2',\n",
       " 'https://www.kaggle.com/competitions/passenger-screening-algorithm-challenge']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_competitions_1[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scraping data from each competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_extraction(top_competitions):\n",
    "    competition_data = []\n",
    "\n",
    "    for url in top_competitions:\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(url)\n",
    "        time.sleep(1)  # Ensure the page loads completely\n",
    "        \n",
    "        # Extract competition names\n",
    "        try:\n",
    "            competition_name = WebDriverWait(driver, 2).until(\n",
    "                EC.visibility_of_element_located((By.XPATH, '//*[@id=\"site-content\"]/div[2]/div/div/div[2]/div[2]/div[1]/h1'))\n",
    "            ).text\n",
    "        except:\n",
    "            competition_name = \"Competition name not found\"\n",
    "\n",
    "        # Extract the overview text\n",
    "        try:\n",
    "            overview_text = WebDriverWait(driver, 2).until(\n",
    "                EC.visibility_of_element_located((By.XPATH, '//*[@id=\"abstract\"]/div[1]/div[2]/div/p'))\n",
    "            ).text\n",
    "        except:\n",
    "            overview_text = \"Overview text not found\"\n",
    "\n",
    "        # Extract all paragraphs in the description section\n",
    "        try:\n",
    "            description_paragraphs = WebDriverWait(driver, 2).until(\n",
    "                EC.presence_of_all_elements_located((By.XPATH, '//*[@id=\"description\"]/div/div[2]/div/div/p'))\n",
    "            )\n",
    "            description_text = ' '.join([para.text for para in description_paragraphs])\n",
    "        except:\n",
    "            description_text = \"Description text not found\"\n",
    "\n",
    "        try:\n",
    "            driver.get(url + '/data')\n",
    "            time.sleep(1)\n",
    "            dataset_paragraphs = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.XPATH, '//*[@id=\"site-content\"]/div[2]/div/div/div[6]/div[1]/div[1]/div/div[2]/div/div[1]/div/div/div/p'))\n",
    "            )\n",
    "            dataset_description = ' '.join([para.text for para in dataset_paragraphs])\n",
    "        except:\n",
    "            dataset_description = \"Dataset description not found\"\n",
    "\n",
    "        # Store the competition url, overview text, and description text\n",
    "        competition_data.append({\n",
    "            'name' : competition_name,\n",
    "            'url': url,\n",
    "            'overview_text': overview_text,\n",
    "            'description_text': description_text,\n",
    "            'dataset_text' : dataset_description\n",
    "        })\n",
    "    return competition_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'CHALEARN Gesture Challenge 2', 'url': 'https://www.kaggle.com/competitions/GestureChallenge2', 'overview_text': 'Overview text not found', 'description_text': 'This competition is identitical to the first round of the CHALEARN gesture challenge, the only difference is that is will be judged on new fresh final evaluation data. Keep informed of new data releases and new events, sign up to the gesturechallenge group. This challenge is organized by CHALEARN and is sponsored in part by Microsoft (Kinect for Xbox 360). Other sponsors include Texas Instrument. This effort was initiated by the DARPA Deep Learning program and is supported by the US National Science Foundation (NSF) under grants ECCS 1128436 and ECCS 1128296 , the EU Pascal2 network of excellence. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors.', 'dataset_text': '  We are portraying a single user in front of a fixed Kinect (TM) camera, interacting with a computer by performing gestures:  We recorded both an RGB and Depth images (representing the distance of the objects to the camera).  View more examples...      '}\n",
      "{'name': 'CPROD1: Consumer PRODucts contest #1', 'url': 'https://www.kaggle.com/competitions/cprod1', 'overview_text': 'Overview text not found', 'description_text': \"A significant proportion of web usage relates to discussions, research, and purchase of consumer products. Currently, hundreds of thousands of blogs, forums, product review sites, and e-commerce merchants currently exist, in part, to service consumer's need to access product related information and demand to share experiences with products. The goal of this competition is to determine the state-of-the-art methods to automatically recognize product mentions in such textual content and to also disambiguate which product(s) in product catalogs are being referenced. Specifically, the task is to automatically identify all mentions of consumer products in a largely user generated collection of web-content, and to correctly identify the product(s) that each product mention refers to from a large catalog of products. The datasets provided includes hundreds of thousands of text items, a product catalog with over fifteen million products, and hundreds of manually annotated product mentions to support data-driven approaches. The prize pool for the contest is $10,000 and is divided as follows: $6,000 for first, $3,000 for second and $1,000 for third place submissions. Note that the contest is colocated with the ICDM-2012 conference. There will be a workshop on the contest results on December 10th.\", 'dataset_text': 'The CPROD1 competition involves the release of six data files to contestants. Five of the files are provided immediately, while the model evalulation set of text items will be released later to determine the contest winners. Files are in two formats: JSON format and .CSV format The six files are as follows: Sample Perl Code is provided in the file CPROD1_baseline.120707.pl, which also produces a simple benchmark. The files to be used for model training have been bundled together in TrainingSet.zip/.7z: products.json, training-annotated-text.json, training-disambiguated-text.json, training-non-annotated-text.json. The file PublicLeaderboardSet.zip/.7z contains leaderboard-text.json for determining the leaderboard rankings. Below we describe the key data entities involved (text items, products, and disambiguated product mentions), along with the process used to generate the data. For the contest, a “text item” stands for a tokenized representation of a portion or entirety of a web page or a web-forum postings page. We processed each web item to create text items as follows: Here is an example of a text_item: \"TextItem\": {\"0c1edc5b2ed5abb25e25b966ccdb01d2\": [\"Here\",\"\\'s\",\"an\",\"example\",\"of\",\"a\",\"(\",\"pre-tokenized\",\")\",\"text”, “item\",\".\",\"\", \" \", \"Check\",\"out\",\"the\",\"new\",\"iPhone\",\"4s\",\"!\"]}  Notice: 1) how the word \"Here\\'s\" has been divided into two tokens: \"Here\" and \"’s\"; 2) how the end-of-sentence punctuation have been placed into their own tokens; and 3) how sentences have been seperated by both a \"” token representing an end-of-sentence and a \" \" token representing an end of paragraph. For the contest a “product item” is a semi-structured record that represents some purchasable consumer product from either the consumer electronics (CE) or automotive (AU) verticals. Each record has: 1) a unique string-based identifier, and 2) an array composed of a string-based “name”, a two character-based product category, and a two digit-based price.  A sample of some of the products records is presented below (in tabular format): {\"1QlV3Pe6T0W\":[\"iphone 3\",\"CE\",399.95],\"op6rsUEYjWc\":[\"ifone case\",\"CE\",21.25],\"P7Ntsvaer1Y\":[\"hawk break pads for SUVs\",\"AU\",110.00]}   For the contest a disambiguated product mention is a structured record composed of two fields: a product mention identifier, and a space-separated set of product item identifiers. The identiffier represents some specific product mention within some specific text item, for example: 0c1edc5b2ed5abb25e25b966ccdb01d2:0-2 represents the product mention that begins on the first token and ends on the third token in textitem 0c1edc5b2ed5abb25e25b966ccdb01d2. (product mentions are always complete substrings of one or more tokens). Finally, the set of space-separated product identifiers represents the products within the product catalog that have been deemed to refer to the same product as the mention.   We used the following process to annotate text items. The annotation task involved two phases: 1) the identification of product mention\\nwithin text items, and 2) the labeling of product records for each annotated product mention with True match (or False match). During the first phase a set of text items were randomly selected. Each text item was reviewed by at least two different annotators. In cases where there was disagreement about mentions a third annotator broke ties. During the second phase the human annotators were asked to classify which products were legitimate references for each of the product mentions. This phase was significantly more time consuming so only a small portion of product candidates were reviewed by two or more annotators.   We randomly separated the annotated text items into training set, leaderboard set and model evaluation set using 50%, 25% and 25% proportions.'}\n",
      "{'name': 'EMC Israel Data Science Challenge', 'url': 'https://www.kaggle.com/competitions/emc-data-science', 'overview_text': 'Overview text not found', 'description_text': 'The EMC source code classification challenge requires you to classify source code files according to the projects they belong to. Given a set of source code files collected from various open source projects, how well can unseen source code files from the same set of open source projects can be classified? Possible real-world applications:', 'dataset_text': 'Dataset description not found'}\n",
      "{'name': 'Practice Fusion Diabetes Classification', 'url': 'https://www.kaggle.com/competitions/pf2012-diabetes', 'overview_text': 'Overview text not found', 'description_text': \"In the first phase of this prediction challenge Practice Fusion invited anyone with an interest in using electronic medical record data to improve public health to submit and vote on ideas for prediction problems based on a new dataset of 10,000 de-identified medical records. The votes are in and Shea Parkes' top voted submission has won. Practice Fusion is now sponsoring the second and final phase of the challenge inspired by the winning problem: Identify patients diagnosed with Type 2 Diabetes Mellitus. Over 25 million people, or nearly 8.3% of the entire United States population, have diabetes. Diabetes is also associated with a wide range of complications from heart disease and stroke to blindness and kidney disease. Predicting who has diabetes will lead to a better understanding of these complications and the common comorbidities that diabetics suffer. The Challenge: Given a de-identified data set of patient electronic health records, build a model to determine who has a diabetes diagnosis, as defined by ICD9 codes 250, 250.0, 250.*0 or 250.*2 (e.g., 250, 250.0, 250.00, 250.10, 250.52, etc).\", 'dataset_text': \"Update: this dataset has been removed at the request of the host. The goal of this competition is to build a model that identifies who in the test set has a diagnosis of Type 2 diabetes mellitus (T2DM). Diagnosis of T2DM is defined by a set of ICD9 codes: {'250', '250.0', 250.*0, and 250.*2} where 250.*0 means '250.00', '250.10', '250.20', ... '250.90' and 250.*2 means '250.02', '250.12', ... '250.92'. Note that ICD9 codes 250.*1 and 250.*3 are for Type I diabetes mellitus and are not to be classified. ICD9 codes are found in the table SyncDiagnosis. The trainingSet and testSet each consist of 17 different files, 2 common files and 15 data set-specific files. They are in comma separated value (csv) format. Please refer to the data set dictionary for a description of the table elements and for a chart showing how the tables are connected. There are a total of 9,948 patients in the training set and 4,979 patients in the test set. In the training set file training_SyncPatient.csv, an indicator column has been added to show who has a diagnosis of Type 2 diabetes mellitus. Also provided are the data tables in a SQLite database along with the script used to create the database. These are found in the file compDataAsSQLiteDB. Starter code (in R) that works with the SQLite database and performs a simple data flattening tranformation is provided in the files sample_code.R and sample_code_library.R. This code was used to generate the Random Forest Benchmark (also provided, randomForest-Benchmark.csv ). This code is by no means complete, but is provided to help you get started analyzing the data and creating models.\"}\n",
      "{'name': 'Cause-effect pairs', 'url': 'https://www.kaggle.com/competitions/cause-effect-pairs', 'overview_text': 'Overview text not found', 'description_text': 'Come to our NIPS workshop (dec 9 or 10 in Tahoe).                                The problem of attributing causes to effects is pervasive in science, medicine, economy and almost every aspects of our everyday life involving human reasoning and decision making. What affects your health? the economy? climate changes? The gold standard to establish causal relationships is to perform randomized controlled experiments. However, experiments are costly while non-experimental \"observational\" data collected routinely around the world are readily available. Unraveling potential cause-effect relationships from such observational data could save a lot of time and effort. Consider for instance a target variable B, like occurence of \"lung cancer\" in patients. The goal would be to find whether a factor A, like \"smoking\", might cause B. The objective of the challenge is to rank pairs of variables {A, B} to prioritize experimental verifications of the conjecture that A causes B. As is known, \"correlation does not mean causation\". More generally, observing a statistical dependency between A and B does not imply that A causes B or that B causes A;  A and B could be consequences of a common cause. But, is it possible to determine from the joint observation of samples of two variables A and B that A should be a cause of B? There are new algorithms that have appeared in the literature in the past few years that tackle this problem. This challenge is an opportunity to evaluate them and propose new techniques to improve on them. We provide hundreds of pairs of real variables with known causal relationships from domains as diverse as chemistry, climatology, ecology, economy, engineering, epidemiology, genomics, medicine, physics. and sociology. Those are intermixed with controls (pairs of independent variables and pairs of variables that are dependent but not causally related) and semi-artificial cause-effect pairs (real variables mixed in various ways to produce a given outcome). This challenge is limited to pairs of variables deprived of their context. Thus constraint-based methods relying on conditional independence tests and/or graphical models are not applicable. The goal is to push the state-of-the art in complementary methods, which can eventually disambiguate Markov equivalence classes. If you are skeptical that this is possible, try this quiz: Examine the plot below of values of variable B plotted as a function of values of variable A. Can you guess which one is a cause of the other? Hint: Some non-linear functions are non-invertible.   July 1: A new data release was made to address a normalization problem and the deadline was extended. Scores on the public leaderboard prior to July 1 were decreased by 0.5. Please make new submissions with the new validation set. The competition is open to new teams.', 'dataset_text': 'This is the July 1, 2013 final data release.  The data provided on this page is in csv format, suitable to be read by: Archived data and data in the split format (one pair per file) are also available. The file CEfinal_basic_python_benchmark.csv provides a sample submission. We released the final test data and an equivalent amount of training and validation data distributed similarly. The test data is encrypted, the decryption key will be revealed at the end of the development phase. The new validation set is replacing the old validation set on the leaderboard and all the scores are reset to 0.5, please re-submit results on the new validation set. The new data include pairs of variables generated in a similar way as those of SUP2data and pairs of real variables from various sources. The final data is different from the original training and validation data with respect to normalization and quantization of variables to address a problem of bias in the original data. NEW: May-June 2013 supplementary data release: We provide three additional training datasets artificially generated: SUP1data, SUP2data, and SUP3data. Those training datasets have normalized numerical variables and have balanced number of unique values across all classes. SUP1data includes ~6000 pairs of numerical variables. SUP2 includes ~6000 pairs of mixed variables (numerical, categorical, binary). SUP3 data includes 81 pairs of real cause-effect pairs and 81 control pairs A|B and A-B generated from the real pairs. March 2013 data release: archived. In CEfinal_xxx_text.zip, you will find the following files:'}\n"
     ]
    }
   ],
   "source": [
    "competition_data_2 = page_extraction(top_competitions_2)\n",
    "# Output the data\n",
    "for data in competition_data_2[:5]:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to ../Data/kaggle_data_first_page_2.json\n"
     ]
    }
   ],
   "source": [
    "output_directory = '../Data'\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Define the output file path\n",
    "output_file_path = os.path.join(output_directory, 'kaggle_data_first_page_2.json')\n",
    "\n",
    "# Write the data to a JSON file\n",
    "with open(output_file_path, 'w') as json_file:\n",
    "    json.dump(competition_data_2, json_file, indent=4)\n",
    "\n",
    "print(f\"Data successfully written to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "334"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../Data/kaggle_data_first_page_2.json', 'r') as file:\n",
    "    kaggle_comps = json.load(file)\n",
    "\n",
    "len(kaggle_comps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Eval.Ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Open the major website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "website = 'https://eval.ai/web/challenges/list'\n",
    "# Create a WebDriver instance for Chrome\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "# Visit the website\n",
    "driver.get(website)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gather the competition webpage links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://eval.ai/web/challenges/challenge-page/2429\n",
      "https://eval.ai/web/challenges/challenge-page/2418\n"
     ]
    }
   ],
   "source": [
    "# Wait for the page to load (this might require adjusting depending on page load time)\n",
    "driver.implicitly_wait(10)  # Adjust the wait time as necessary\n",
    "\n",
    "base_xpath = '//*[@id=\"page-wrap\"]/div/div/div/ui-view/ui-view/section/div[2]/div'\n",
    "\n",
    "# List to hold links\n",
    "competition_links = []\n",
    "\n",
    "# Loop through the first two competition divs\n",
    "for i in range(1, 3):  # Since XPath index starts at 1 and we need first two competitions\n",
    "    competition_xpath = f'{base_xpath}[{i}]/a'\n",
    "    # Find the <a> element and get the href attribute\n",
    "    competition_link = driver.find_element(By.XPATH, competition_xpath).get_attribute('href')\n",
    "    competition_links.append(competition_link)\n",
    "\n",
    "# Print the links\n",
    "for data in competition_links:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scraping from each competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://eval.ai/web/challenges/challenge-page/2429',\n",
       "  'overview': 'Surgical action triplet detection To detect surgical activities as triplets of {`instruments, verb, target`} where :',\n",
       "  'name': 'CholecTriplet Challenge Detection Evaluation'},\n",
       " {'url': 'https://eval.ai/web/challenges/challenge-page/2418',\n",
       "  'overview': \"OpenAD is the first open-world 3D object detection benchmark for autonomous driving. We meticulously selected 2,000 scenes from 5 public datasets and annotated 6,597 3D corner cases for these scenes. Together with the original annotations of these scenes, there are 19,761 objects belonging to 206 different categories. You can utilize OpenAD to evaluate your model's open-world capabilities, encompassing scene generalization, cross-vehicle-type adaptability, open-vocabulary proficiency, and corner case detection aptitude. We provide a toolkit to organize data, load data, and evaluate your model with simple commands. Access the data and code here.\",\n",
       "  'name': 'OpenAD - 3D Object Detection'}]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "competition_data_eval = []\n",
    "for url in competition_links:\n",
    "    driver.get(url)\n",
    "    time.sleep(1)  # Ensure the page loads completely\n",
    "    try:\n",
    "        paragraphs_xpath = '//*[@id=\"page-wrap\"]/div/div/div/ui-view/ui-view/ui-view/section/div/div[2]/div/div/p'\n",
    "\n",
    "        # Wait until the presence of all paragraph elements is located\n",
    "        description_paragraphs = WebDriverWait(driver, 2).until(\n",
    "            EC.presence_of_all_elements_located((By.XPATH, paragraphs_xpath))\n",
    "        )\n",
    "\n",
    "        # Extract text from each paragraph\n",
    "        competition_overview = ' '.join([paragraph.text for paragraph in description_paragraphs])\n",
    "    except:\n",
    "        competition_overview = \"Overview text not found\"\n",
    "\n",
    "    try: \n",
    "        name_xpath = '//*[@id=\"page-wrap\"]/div/div/div/ui-view/ui-view/section/div/div[1]/div[2]/div/h4'\n",
    "        competition_name = WebDriverWait(driver, 2).until(\n",
    "            EC.visibility_of_element_located((By.XPATH, name_xpath))\n",
    "        ).text\n",
    "    except:\n",
    "        name = \"Name of competition not found\"\n",
    "    competition_data_eval.append({'url' : url, \n",
    "                                'overview' : competition_overview,\n",
    "                                'name' : competition_name})        \n",
    "\n",
    "competition_data_eval\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## drivendata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "website = 'https://www.drivendata.org/competitions/search/?sort=total_prize_purse'\n",
    "# Create a WebDriver instance for Chrome\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "# Visit the website\n",
    "driver.get(website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.drivendata.org/competitions/group/nist-federated-learning/\n",
      "https://www.drivendata.org/competitions/group/nih-nia-alzheimers-adrd-competition/\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Wait for the competition list div to load and locate it using its ID\n",
    "    competition_list_div = WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.ID, \"competition-list\"))\n",
    "    )\n",
    "\n",
    "    # Find all <a> tags within nested layers of the competition list div that have the specific class\n",
    "    competition_links = competition_list_div.find_elements(By.XPATH, \".//a[@class='text-decoration-none'][@href]\")\n",
    "\n",
    "    # Extract href attributes from the first five links only (corrected limit comment)\n",
    "    hrefs = [link.get_attribute('href') for link in competition_links[:2]]  # Limit to first five links\n",
    "\n",
    "    # Output the collected links\n",
    "    for href in hrefs:\n",
    "        print(href)\n",
    "except:\n",
    "    # Clean up: close the browser window\n",
    "    hrefs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Competition: https://www.drivendata.org/competitions/group/nist-federated-learning/\n",
      "  Sub-Competition: https://www.drivendata.org/competitions/98/nist-federated-learning-1/\n",
      "  Sub-Competition: https://www.drivendata.org/competitions/search/?category=privacy\n",
      "  Sub-Competition: https://www.drivendata.org/competitions/search/?type=privacy\n",
      "  Sub-Competition: https://www.drivendata.org/competitions/105/nist-federated-learning-2-financial-crime-federated/\n",
      "  Sub-Competition: https://www.drivendata.org/competitions/144/nist-federated-learning-2-financial-crime-centralized/\n",
      "  Sub-Competition: https://www.drivendata.org/competitions/103/nist-federated-learning-2-pandemic-forecasting-federated/\n",
      "  Sub-Competition: https://www.drivendata.org/competitions/145/nist-federated-learning-2-pandemic-forecasting-centralized/\n",
      "  Sub-Competition: https://www.drivendata.org/competitions/139/nist-federated-learning-3-red-teams/\n",
      "Main Competition: https://www.drivendata.org/competitions/group/nih-nia-alzheimers-adrd-competition/\n",
      "  Sub-Competition: https://www.drivendata.org/competitions/301/prepare-challenge-phase-2-report-arena/\n",
      "  Sub-Competition: https://www.drivendata.org/competitions/search/?category=None\n",
      "  Sub-Competition: https://www.drivendata.org/competitions/299/competition-nih-alzheimers-acoustic-2/\n",
      "  Sub-Competition: https://www.drivendata.org/competitions/300/competition-nih-alzheimers-sdoh-2/\n",
      "  Sub-Competition: https://www.drivendata.org/competitions/253/competition-nih-alzheimers-adrd-1/\n",
      "  Sub-Competition: https://www.drivendata.org/competitions/search/?category=health\n",
      "  Sub-Competition: https://www.drivendata.org/competitions/search/?type=health\n"
     ]
    }
   ],
   "source": [
    "main_competition_links = hrefs\n",
    "\n",
    "# Dictionary to hold all sub-competition links for each main competition\n",
    "all_sub_competition_links = {}\n",
    "\n",
    "# Iterate over each main competition link\n",
    "for main_link in main_competition_links:\n",
    "    driver.get(main_link)\n",
    "    try:\n",
    "        # Wait for the sub-competition divs to load\n",
    "        sub_competition_divs = WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, \"competition-subgroup\"))\n",
    "        )\n",
    "        # Collect all hrefs from <a> tags within each subgroup\n",
    "        sub_competition_hrefs = []\n",
    "        for div in sub_competition_divs:\n",
    "            sub_competition_links = div.find_elements(By.XPATH, \".//a[@href]\")\n",
    "            for link in sub_competition_links:\n",
    "                href = link.get_attribute('href')\n",
    "                if href not in sub_competition_hrefs:\n",
    "                    sub_competition_hrefs.append(href)\n",
    "\n",
    "        # Store the collected sub-competition links\n",
    "        all_sub_competition_links[main_link] = sub_competition_hrefs\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {main_link}: {str(e)}\")\n",
    "        all_sub_competition_links[main_link] = []\n",
    "\n",
    "# Output the collected links for each competition\n",
    "for main_link, sub_links in all_sub_competition_links.items():\n",
    "    print(f\"Main Competition: {main_link}\")\n",
    "    for link in sub_links:\n",
    "        print(f\"  Sub-Competition: {link}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI CROWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "# Navigate to the website\n",
    "driver.get('https://www.aicrowd.com/challenges')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to scroll to the bottom of the page\n",
    "\n",
    "for _ in range(17):\n",
    "    # Scroll down to the bottom of the page\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    # Wait for a short period to ensure the page has loaded the content\n",
    "    time.sleep(3)  # Adjust the sleep time if necessary based on the page's response time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.aicrowd.com/challenges/brick-by-brick-2024\n",
      "https://www.aicrowd.com/challenges/sounding-video-generation-svg-challenge-2024\n",
      "https://www.aicrowd.com/challenges/meta-comprehensive-rag-benchmark-kdd-cup-2024\n",
      "https://www.aicrowd.com/challenges/amazon-kdd-cup-2024-multi-task-online-shopping-challenge-for-llms\n",
      "https://www.aicrowd.com/challenges/generative-interior-design-challenge-2024\n"
     ]
    }
   ],
   "source": [
    "links = driver.find_elements(By.XPATH, '//*[@id=\"challenges-div\"]//a[contains(@class, \"card-img-overlay\")]')\n",
    "# //*[@id=\"challenges-div\"]/div[1]/div/div[2]/div[1]/h5/a\n",
    "# Extract href attributes from each link element\n",
    "urls = [link.get_attribute('href') for link in links]\n",
    "\n",
    "# Output or process the URLs\n",
    "for url in urls[:5]:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "317"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "aicrowd_competitions_data = []\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "\n",
    "for url in urls:\n",
    "    driver.get(url)\n",
    "    time.sleep(1)  # Ensure the page loads completely\n",
    "\n",
    "    # Extract all paragraph texts under the specified div for overview\n",
    "    paragraphs = driver.find_elements(By.XPATH, '//*[@data-controller=\"challenge-overview\"]//p')\n",
    "    overview_text = \" \".join([p.text for p in paragraphs if p.text])\n",
    "\n",
    "    # Navigate to the rules page\n",
    "    nav_links = driver.find_elements(By.XPATH, '//a[contains(@class, \"nav-link\")]')\n",
    "    rules_page_url = None\n",
    "    for link in nav_links:\n",
    "        if 'rules' in link.get_attribute('href').lower():  # Assuming the URL contains the word 'rules'\n",
    "            rules_page_url = link.get_attribute('href')\n",
    "            break\n",
    "\n",
    "    if rules_page_url:\n",
    "        driver.get(rules_page_url)\n",
    "    time.sleep(1)  # Ensure the rules page loads\n",
    "\n",
    "    # Extract all paragraph texts for rules\n",
    "    paragraphs = driver.find_elements(By.XPATH, '/html/body/div[2]/main/div[2]/div/div/div/div/p')\n",
    "    rules_text = \" \".join([p.text for p in paragraphs if p.text])\n",
    "\n",
    "    # Create a dictionary for this competition and append to the list\n",
    "    competition_data = {\n",
    "        'url': url,\n",
    "        'overview': overview_text,\n",
    "        'rules': rules_text\n",
    "    }\n",
    "    aicrowd_competitions_data.append(competition_data)\n",
    "\n",
    "# Save data to a JSON file\n",
    "# with open('competitions_data.json', 'w') as file:\n",
    "#     json.dump(aicrowd_competitions_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "317"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aicrowd_competitions_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/aicrowd_raw.json', 'w') as outfile:\n",
    "        json.dump(aicrowd_competitions_data, outfile, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
