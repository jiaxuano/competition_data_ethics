[
    {
        "name": "AI Mathematical Olympiad - Progress Prize 2",
        "url": "https://www.kaggle.com/competitions/ai-mathematical-olympiad-progress-prize-2",
        "overview_text": "The goal of this competition is to create algorithms and models that can solve tricky math problems written in LaTeX format. Your participation will help to advance AI models\u2019 mathematical reasoning skills and drive frontier knowledge.",
        "description_text": "Note: This is the second AIMO Progress Prize competition. It builds upon the first AIMO Progress Prize competition, which was won in July 2024 by Project Numina. This second competition has an increased prize pool, a new dataset of problems, increased compute for participants and updated rules for using open-source LLMs. The ability to reason mathematically is a critical milestone for AI. Mathematical reasoning is the foundation for solving many complex problems, from engineering marvels to intricate financial models. However, current AI capabilities are limited in this area. The AI Mathematical Olympiad (AIMO) Prize is a $10mn fund to spur the open development of AI models capable of performing as well as top human participants in the International Mathematical Olympiad (IMO). This second AIMO Progress Prize competition has 110 math problems in algebra, combinatorics, geometry and number theory. The difficulty has been increased from the first competition, and the problems are now around the National Olympiad level. The problems have also been designed to be 'AI hard' in terms of the mathematical reasoning required, which was tested against current open LLMs' capabilities. To address the challenge of train-test leakage, the competition uses novel math problems created by an international team of problem solvers. Using this transparent and fair evaluation framework, the competition will help to strengthen the benchmarks for assessing AI models' mathematical reasoning skills, without the risk of contamination from training data.\nThis latest AIMO Progress Prize competition offers an exciting opportunity to drive innovation in the field of AI for Math, while also fostering healthy competition and supporting open science. Join us as we work towards a future where AI models\u2019 mathematical reasoning skills are accurately and reliably assessed, driving progress and innovation.",
        "dataset_text": "The competition data comprises 110 mathematics problems similar in style to those of the AIME. The answer to each problem is a non-negative integer between 0 and 999. You should arrive at this number by taking the problem solution modulo 1000. If, for instance, you believe the solution to a problem is 2034, your predicted answer should be 34. You should expect the difficulty of the problems to be roughly at the level of a national Olympiad, although some problems are slightly easier and some are slightly harder. All problems are text-only with mathematical notation in LaTeX. Please see the Overview, Section Note on Language and Notation for details on the notational conventions used. Although some problems may involve geometry, diagrams are not used in any problem. The public test set comprises exactly 50 problems, and the private test set comprises a further distinct set of 50 problems. We also provide a selection of 10 problems for use as reference, called \"reference data\". A pdf with full solutions to the problems from the reference data can be found below. The problems in the private and public sets have been selected to balance both difficulty and subject area. Their difficulty and subject distribution is similar. Please note that this is a Code Competition. You must submit to this competition using the provided Python evaluation API, which serves the test set question by question in random order. To use the API, follow the example in this notebook: AIMO 2 Submission Demo. We give a few placeholder problems in the publicly visible test.csv file to help you author your submissions. These problems are not meant to be representative of the problems in the actual test set. When your submission is scored, this placeholder test data will be replaced with the actual test data. Because of the limited number of problems available, we are taking special precautions to secure the test set against probing. Among other things, during the submission period, the test set will comprise only the 50 public set problems. Once the competition ends, we will rerun all model submissions on the 50 private set problems. You should attempt to make sure your submission will complete successfully on the 50 new private set problems. This may mean ensuring your submission is robust to unexpected inputs, or managing runtime and memory usage."
    },
    {
        "name": "Passenger Screening Algorithm Challenge",
        "url": "https://www.kaggle.com/competitions/passenger-screening-algorithm-challenge",
        "overview_text": "Overview text not found",
        "description_text": "While long lines and frantically shuffling luggage into plastic bins isn\u2019t a fun experience, airport security is a critical and necessary requirement for safe travel. No one understands the need for both thorough security screenings and short wait times more than U.S. Transportation Security Administration (TSA). They\u2019re responsible for all U.S. airport security, screening more than two million passengers daily. As part of their Apex Screening at Speed Program, DHS has identified high false alarm rates as creating significant bottlenecks at the airport checkpoints. Whenever TSA\u2019s sensors and algorithms predict a potential threat, TSA staff needs to engage in a secondary, manual screening process that slows everything down. And as the number of travelers increase every year and new threats develop, their prediction algorithms need to continually improve to meet the increased demand. Currently, TSA purchases updated algorithms exclusively from the manufacturers of the scanning equipment used. These algorithms are proprietary, expensive, and often released in long cycles. In this competition, TSA is stepping outside their established procurement process and is challenging the broader data science community to help improve the accuracy of their threat prediction algorithms. Using a dataset of images collected on the latest generation of scanners, participants are challenged to identify the presence of simulated threats under a variety of object types, clothing types, and body types. Even a modest decrease in false alarms will help TSA significantly improve the passenger experience while maintaining high levels of security. This is a two-stage competition. Please read our two-stage FAQs to understand more about what this means. All persons contained in the dataset are volunteers who have agreed to have their images used for this competition. The images may contain sensitive content. We kindly request that you conduct yourself with professionalism, respect, and maturity when working with this data.",
        "dataset_text": "This dataset contains a large number of body scans acquired by a new generation of millimeter wave scanner called the High Definition-Advanced Imaging Technology (HD-AIT) system. The competition task is to predict the probability that a given body zone (out of 17 total body zones) has a threat present. The images in the dataset are designed to capture real scanning conditions. They are comprised of volunteers wearing different clothing types (from light summer clothes to heavy winter clothes), different body mass indices, different genders, different numbers of threats, and different types of threats. Due to restrictions on revealing the types of threats for which the TSA screens, the threats in the competition images are \"inert\" objects with varying material properties. These materials were carefully chosen to simulate real threats. The volunteers used in the first and second stage of the competition will be different (i.e. your algorithm should generalize to unseen people). In addition, you should not make assumptions about the number, distribution, or location of threats in the second stage. All volunteers have agreed to have their images used for this competition. The images may contain sensitive content. We kindly request that you conduct yourself with professionalism, respect, and maturity when working with this data. The data for stage one is more than three terabytes in size. Stage two will have a similar size. Since most internet connections can not reliably download this much data, we are making the full dataset available on multi-regional Google Cloud Storage. Two of the four file formats are downloadable from the competition page directly: All four file formats represent the same underlying scan. They are simply different representations of a 3D image. Kaggle will provide example python code (as a Kernel) that shows how to read the images. (Competition Close Update: This data is no longer available for access at the request of the sponsor) You can read more about accessing Google Cloud Storage buckets here. The large size and proprietary format of this dataset may seem daunting at first. We suggest you start with the smallest version of the images. Over time, the file formats will become familiar. Note that it is possible to work on--and potentially even win--this competition without the raw data files that make up the bulk of the dataset. The data for each scan performed by the HD-AIT system is referred to as an HD-AIT Frame. A frame consists of the following four binary files: Each file is named with a scan Id. These file types are described in more detail below. Due to the dataset size, we have grouped the files into directories based on the file type. This will allow you to start with a small-but-complete set of image files (the .aps files) and work your way up to larger image files, as necessary. The four files generated by the HD-AIT program set have a common file structure. All four files are binary and include a 512 byte header followed by the file's data. The header mostly contains technical scan parameters and is largely identical across all images. With the exception of the field 'data_scale_factor', we do not expect information from the header to be necessary or useful for the competition task. We have preserved the file formats used by the TSA in order to make the result of the competition more readily compatible with the images generated by their scanners. After the binary header, the data is stored sequentially. When referring to the data storage order, the notation indicates, left to right, most significant (largest stride) to least significant (shortest stride) axes. For example, a data order defined as AYX means that the angular axis is the largest stride. Such a file would consist of a series of YX planes incremented in angle. The 'Projected Image' algorithm computes 3D images for 90-degree segments of data that are equally spaced around the region scanned. A maximum value projection of the result of each of these computations is written sequentially into a single file. The result of this is an image file that, when played back plane-by-plane, appears like the object is spinning on the screen. The data type of this file is 16bit unsigned integer. Data scaling is achieved by multiplying each pixel value by the 'data_scale_factor' field in the header. In summary, the 'Projected Image Angle Sequence' file is a series of 2D mmWave snapshots equally spaced in angle around the object. The 'Combined Image' algorithm computes eight 3D images that are equally spaced around the region scanned, and then combines these images into one composite 3D volumetric image. This computation is written into the .a3d file. When played back plane-by-plane, this image displays cross-section slices through the object at sequential heights. The data type of this file is 16bit unsigned integer. Data scaling is achieved by multiplying each pixel value by the 'data_scale_factor' field in the header. In summary, the \u2018Combined Image 3D\u2019 file is the full composite 3D image volume stored in sequential height slices order. For visualization purposes, an 'Angle Sequence File' is rendered from the previously described 'Combined Image 3D' data. A projection through the 3D data at sequential angular increments is written contiguously into a single file. Similar to the 'Projected Image Angle Sequence' file, the result of this rendering is an image file that, when played back plane-by-plane, appears like the object is spinning on the screen. The data type of this file is 16bit unsigned integer. Data scaling is achieved by multiplying each pixel value by the 'data_scale_factor' field in the header. In summary, the 'Combined Image Angle Sequence' file is a series of 2D mmWave images equally spaced in angle around the object. The three other types of images (.aps, .a3d, .a3daps) are processed projections based on the raw data file. These projection types were created by engineers with expertise in signal proccessing and are a more readily useful representation of the images. For this reason, and due to its vast size, using the raw data is optional and potentially redudundant with the other data formats. The raw data is captured as follows. A synthetic focusing algorithm performs a waveform calibration operation as the first step. The algorithm combines a set of pre-scan calibrations files to reduce the effect of common mode direct coupling signals and to align the phase component of each waveform captured from each of the array elements. The result of this operation is a calibrated version of the raw data in units of volts. The data is complex floating point words ordered in frequency planes. Every scan in the dataset has a unique (hashed) Id. Every Id will have four associated image files, grouped into folders by their type."
    },
    {
        "name": "Konwinski Prize",
        "url": "https://www.kaggle.com/competitions/konwinski-prize",
        "overview_text": "I'm Andy, and I\u2019m giving $1M to the first team that exceeds 90% on a new version of the SWE-bench benchmark containing GitHub issues we collect after we freeze submissions. I want to see what a contamination-free leaderboard looks like. Your challenge is to build an AI that crushes this yet-to-be-collected set of SWE-bench issues.",
        "description_text": "I fell in love with SWE-bench the moment I saw it. What a great idea: have AIs solve real Issues from popular GitHub repos. SWE-bench felt more difficult, grounded, and relevant than other AI benchmarks. But I've always wondered how the leaderboard would change if the test set weren\u2019t public. So for this competition we will collect a new test set after the submission deadline.\nI also believe in the power of open source communities, so for this competition cash will only be awarded to submissions that use open source code and open weight models.\nAutomating this task will let human software engineers spend lots more time designing new features, reforming abstractions, interfacing with users, and other tasks that are more inherently human (and, for many of us, more fun). If we get this right, we can spend less time fixing bugs and more time building.\nNow let\u2019s get AI actually solving our GitHub issues.",
        "dataset_text": "Your challenge is to develop an agent that can resolve real-world GitHub issues. This competition builds on the SWE-bench benchmark by using Kaggle's forecasting format to ensure that all of the git issues used for the private test set cannot did not exist when the submitted model was trained. This is a Code Competition. When your submission is scored the example test data will be replaced with the full test set. You must submit to this competition using the provided Python evaluation API, which serves the test set instance by instance in random order. To use the API, follow the example in this notebook: K Prize Submission Demo. The competition will proceed in two phases: data.a_zip A renamed .zip archive, created to sidestep constraints on filenames and nested archives. This unzips to create the data folder discussed below. data/data.parquet The train set metadata, which includes a limited to a handful of examples. You are encouraged to source additional codebases for training your models. Most of the metadata provided here is only available for the train set. data/*/ All other subdirectories in the data are used by the evaluation API and to configure the evaluation environments. All of the evaluation environments run Python 3.11 and will install it if necessary, but only on Ubuntu 20 or 22. kprize_setup/ Files used for installing this competition's adaptation of the swebench library. Note that this won't currently work on Windows. kaggle_evaluation/ Files that implement the evaluation API. Some of the implementation details may be of interest for offline testing but we recommend beginning with the demo submission notebook. You are strongly encouraged to run the API in a Docker container based on Kaggle's image when running locally to avoid issues with your existing Python environment. If necessary the API will install uv plus several libraries (listed in kprize_setup/pip_packages), and create new Python environments. Note that we've planning on making further updates to the evaluation API and kprize library to improve the runtime and provide more useful tooling. These updates aren't expected to interfere with the core submission loop. Please see this forum post for details."
    },
    {
        "name": "Zillow Prize: Zillow\u2019s Home Value Prediction (Zestimate)",
        "url": "https://www.kaggle.com/competitions/zillow-prize-1",
        "overview_text": "Overview text not found",
        "description_text": " Zillow\u2019s Zestimate home valuation has shaken up the U.S. real estate industry since first released 11 years ago. A home is often the largest and most expensive purchase a person makes in his or her lifetime. Ensuring homeowners have a trusted way to monitor this asset is incredibly important. The Zestimate was created to give consumers as much information as possible about homes and the housing market, marking the first time consumers had access to this type of home value information at no cost. \u201cZestimates\u201d are estimated home values based on 7.5 million statistical and machine learning models that analyze hundreds of data points on each property. And, by continually improving the median margin of error (from 14% at the onset to 5% today), Zillow has since become established as one of the largest, most trusted marketplaces for real estate information in the U.S. and a leading example of impactful machine learning. Zillow Prize, a competition with a one million dollar grand prize, is challenging the data science community to help push the accuracy of the Zestimate even further. Winning algorithms stand to impact the home values of 110M homes across the U.S. In this million-dollar competition, participants will develop an algorithm that makes predictions about the future sale prices of homes. The contest is structured into two rounds, the qualifying round which opens May 24, 2017 and the private round for the 100 top qualifying teams that opens on Feb 1st, 2018. In the qualifying round, you\u2019ll be building a model to improve the Zestimate residual error. In the final round, you\u2019ll build a home valuation algorithm from the ground up, using external data sources to help engineer new features that give your model an edge over the competition. Because real estate transaction data is public information, there will be a three-month sales tracking period after each competition round closes where your predictions will be evaluated against the actual sale prices of the homes. The final leaderboard won\u2019t be revealed until the close of the sales tracking period.",
        "dataset_text": "In this competition, Zillow is asking you to predict the log-error between their Zestimate and the actual sale price, given all the features of a home. The log error is defined as \ud835\udc59\ud835\udc5c\ud835\udc54\ud835\udc52\ud835\udc5f\ud835\udc5f\ud835\udc5c\ud835\udc5f=\ud835\udc59\ud835\udc5c\ud835\udc54(\ud835\udc4d\ud835\udc52\ud835\udc60\ud835\udc61\ud835\udc56\ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc52)\u2212\ud835\udc59\ud835\udc5c\ud835\udc54(\ud835\udc46\ud835\udc4e\ud835\udc59\ud835\udc52\ud835\udc43\ud835\udc5f\ud835\udc56\ud835\udc50\ud835\udc52)\nl\nl\n(\ni\n)\nl\n(\nS\nl\ni\n) and it is recorded in the transactions file train.csv. In this competition, you are going to predict the logerror for the months in Fall 2017. Since all the real estate transactions in the U.S. are publicly available, we will close the competition (no longer accepting submissions) before the evaluation period begins."
    },
    {
        "name": "ARC Prize 2024",
        "url": "https://www.kaggle.com/competitions/arc-prize-2024",
        "overview_text": "In this competition, you\u2019ll develop AI systems to efficiently learn new skills and solve open-ended problems, rather than depend exclusively on AI systems trained with extensive datasets. The top submissions will show improvement toward human reasoning benchmarks.",
        "description_text": "Current AI systems can not generalize to new problems outside their training data, despite extensive training on large datasets. LLMs have brought AI to the mainstream for a large selection of known tasks. However, progress towards Artificial General Intelligence (AGI) has stalled. Improvements in AGI could enable AI systems that think and invent alongside humans. The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) benchmark measures an AI system's ability to efficiently learn new skills. Humans easily score 85% in ARC, whereas the best AI systems only score 34%. The ARC Prize competition encourages researchers to explore ideas beyond LLMs, which depend heavily on large datasets and struggle with novel problems. This competition includes several components. The competition as described here carries a prize of $100,000 with an additional $500,000 available if any team can beat a score of 85% on the leaderboard. Further opportunities outside of Kaggle are also available with associated prizes- to learn more visit ARCprize.org. Your work could contribute to new AI problem-solving applicable across industries. Vastly improved AGI will likely reshape human-machine interactions. Winning solutions will be open-sourced to promote transparency and collaboration in the field of AGI.",
        "dataset_text": "The objective of this competition is to create an algorithm that is capable of solving abstract reasoning tasks. Critically, these are novel tasks: tasks that the algorithm has never seen before. Hence, simply memorizing a set of reasoning templates will not suffice. The format is different from the previous competition, so please read this information carefully, and refer to supplementary documentation as needed. When looking at a task, a \"test-taker\" has access to inputs and outputs of the demonstration pairs (train pairs), plus the input(s) of the test pair(s). The goal is to construct the output grid(s) corresponding to the test input grid(s), using 2 trials for each test input. \"Constructing the output grid\" involves picking the height and width of the output grid, then filling each cell in the grid with a symbol (integer between 0 and 9, which are visualized as colors). Only exact solutions (all cells match the expected answer) can be said to be correct. Any additional information, as well as an interactive app to explore the objective of this competition is found at the ARCPrize.org. It is highly recommended that you explore the interactive app, as the best way to understand the objective of the competition. The information is stored in two files: Each task contains a dictionary with two fields: A \"pair\" is a dictionary with two fields: A \"grid\" is a rectangular matrix (list of lists) of integers between 0 and 9 (inclusive). The smallest possible grid size is 1x1 and the largest is 30x30. The data on this page should be used to develop and evaluate your models. When notebooks are submitted for rerun, they are scored using 100 unseen tasks found in the rerun file named arc-agi_test_challenges.json. The rerun tasks will contain train pairs of inputs and outputs as well as the tasks test input. Your algorithm must predict the test output. The majority of the 100 tasks used for leaderboard score only have one test input that will require a corresponding output prediction, although for a small number of tasks, you will be asked to make predictions for two test inputs."
    },
    {
        "name": "AI Mathematical Olympiad - Progress Prize 1",
        "url": "https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize",
        "overview_text": "The goal of this competition is to create algorithms and models that can solve tricky math problems written in LaTeX format. Your participation will help to advance AI models\u2019 mathematical reasoning skills and drive frontier knowledge.",
        "description_text": "The ability to reason mathematically is a critical milestone for AI. Mathematical reasoning is the foundation for solving many complex problems, from engineering marvels to intricate financial models. However, current AI capabilities are limited in this area. The AI Mathematical Olympiad (AIMO) Prize is a new $10mn prize fund to spur the open development of AI models capable of performing as well as top human participants in the International Mathematical Olympiad (IMO).\nThis competition includes 110 problems similar to an intermediate-level high school math challenge. The Gemma 7B benchmark for these problems is 3/50 on the public and private test sets. The assessment of AI models' mathematical reasoning skills faces a significant hurdle, the issue of train-test leakage. Models trained on Internet-scale datasets may inadvertently encounter test questions during training, skewing the evaluation process. To address this challenge, this competition uses a dataset of 110 novel math problems, created by an international team of problem solvers, recognizing the need for a transparent and fair evaluation framework. The dataset encompasses a range of difficulty levels, from simple arithmetic to algebraic thinking and geometric reasoning. This will help to strengthen the benchmarks for assessing AI models' mathematical reasoning skills, without the risk of contamination from training data. This competition offers an exciting opportunity to benchmark open AI models against each other and foster healthy competition and innovation in the field. By addressing this initial benchmarking problem, you will contribute to advancing AI capabilities and help to ensure that its potential benefits outweigh the risks. Join us as we work towards a future where AI models\u2019 mathematical reasoning skills are accurately and reliably assessed, driving progress and innovation across industries.",
        "dataset_text": "The competition data comprises 110 mathematics problems similar in style to those of the AIME.\nThe answer to each problem is a non-negative integer, which you should report modulo 1000. If, for instance, you believe the answer to a problem is 2034, your prediction should be 34.\nYou should expect the difficulty of the problems to be a bit easier than AIME and targeted between AIME and AMC'12 level.\nAll problems are text-only with mathematical notation in LaTeX. Please see the AIMO Prize - Note on Language and Notation.pdf handout for details on the notational conventions used. Although some problems may involve geometry, diagrams are not used in any problem.\nThe public test set comprises exactly 50 problems, and the private test set comprises a distinct set of 50 problems. We also provide a selection of 10 problems for use as training data. The problems in the two test sets have been selected to balance both difficulty and subject area.\nPlease note that this is a Code Competition. We give a few placeholder problems in the publicly visible test.csv file to help you author your submissions. These problems are not meant to be representative of the problems in the actual test set. When your submission is scored, this placeholder test data will be replaced with the actual test data.\nBecause of the limited number of problems available, we are taking special precautions to secure the test set against probing. Among other things, during the submission period the test set will comprise only the 50 public set problems. Once the competition ends, when we rerun submissions, the test set will comprise only the 50 private set problems. You should attempt to make sure your submission will complete successfully on the 50 new private set problems. This may mean ensuring your submission is robust to unexpected inputs, or managing runtime and memory usage."
    },
    {
        "name": "Data Science Bowl 2017",
        "url": "https://www.kaggle.com/competitions/data-science-bowl-2017",
        "overview_text": "Overview text not found",
        "description_text": "In the United States, lung cancer strikes 225,000 people every year, and accounts for $12 billion in health care costs. Early detection is critical to give patients the best chance at recovery and survival. One year ago, the office of the U.S. Vice President spearheaded a bold new initiative, the Cancer Moonshot, to make a decade's worth of progress in cancer prevention, diagnosis, and treatment in just 5 years. In 2017, the Data Science Bowl will be a critical milestone in support of the Cancer Moonshot by convening the data science and medical communities to develop lung cancer detection algorithms. Using a data set of thousands of high-resolution lung scans provided by the National Cancer Institute, participants will develop algorithms that accurately determine when lesions in the lungs are cancerous. This will dramatically reduce the false positive rate that plagues the current detection technology, get patients earlier access to life-saving interventions, and give radiologists more time to spend with their patients. This year, the Data Science Bowl will award $1 million in prizes to those who observe the right patterns, ask the right questions, and in turn, create unprecedented impact around cancer screening care and prevention. The funds for the prize purse will be provided by the Laura and John Arnold Foundation. Visit DataScienceBowl.com to:\n\u2022 Sign up to receive news about the competition\n\u2022 Learn about the history of the Data Science Bowl and past competitions\n\u2022 Read our latest insights on emerging analytics techniques  The Data Science Bowl is presented by  Laura and John Arnold Foundation\nCancer Imaging Program of the National Cancer Institute\nAmerican College of Radiology\nAmazon Web Services\nNVIDIA National Lung Screening Trial\nThe Cancer Imaging Archive\nDiagnostic Image Analysis Group, Radboud University\nLahey Hospital & Medical Center\nCopenhagen University Hospital Bayes Impact\nBlack Data Processng Associates\nCode the Change\nData Community DC\nDataKind\nGalvanize\nGreat Minds in STEM\nHortonworks\nINFORMS\nLesbians Who Tech\nNSBE\nSociety of Asian Scientists & Engineers\nSociety of Women Engineers\nUniversity of Texas Austin, Business Analytics Program,\nMcCombs School of Business\nUS Dept. of Health and Human Services\nUS Food and Drug Administration\nWomen in Technology\nWomen of Cyberjutsu",
        "dataset_text": "NOTE: due to data set usage restrictions, the data for this competition is no longer available for download. In this dataset, you are given over a thousand low-dose CT images from high-risk patients in DICOM format. Each image contains a series with multiple axial slices of the chest cavity. Each image has a variable number of 2D slices, which can vary based on the machine taking the scan and patient. The DICOM files have a header that contains the necessary information about the patient id, as well as scan parameters such as the slice thickness.  The competition task is to create an automated method capable of determining whether or not the patient will be diagnosed with lung cancer within one year of the date the scan was taken. The ground truth labels were confirmed by pathology diagnosis. The images in this dataset come from many sources and will vary in quality. For example, older scans were imaged with less sophisticated equipment. You should expect the stage 2 data to be, on the whole, more recent and higher quality than the stage 1 data (generally having thinner slice thickness). Ideally, your algorithm should perform well across a range of image quality. Each patient id has an associated directory of DICOM files. The patient id is found in the DICOM header and is identical to the patient name. The exact number of images will differ from case to case, varying according in the number of slices. Images were compressed as .7z files due to the large size of the dataset. The DICOM standard is complex and there are a number of different tools to work with DICOM files. You may find the following resources helpful for managing the competition data:"
    },
    {
        "name": "Deepfake Detection Challenge",
        "url": "https://www.kaggle.com/competitions/deepfake-detection-challenge",
        "overview_text": "Overview text not found",
        "description_text": "This competition is closed for submissions. Participants' selected code submissions were re-run by the host on a privately-held test set and the private leaderboard results have been finalized. Late submissions will not be opened, due to an inability to replicate the unique design of this competition. Deepfake techniques, which present realistic AI-generated videos of people doing and saying fictional things, have the potential to have a significant impact on how people determine the legitimacy of information presented online. These content generation and modification technologies may affect the quality of public discourse and the safeguarding of human rights\u2014especially given that deepfakes may be used maliciously as a source of misinformation, manipulation, harassment, and persuasion. Identifying manipulated media is a technically demanding and rapidly evolving challenge that requires collaborations across the entire tech industry and beyond.  AWS, Facebook, Microsoft, the Partnership on AI\u2019s Media Integrity Steering Committee, and academics have come together to build the Deepfake Detection Challenge (DFDC). The goal of the challenge is to spur researchers around the world to build innovative new technologies that can help detect deepfakes and manipulated media. Challenge participants must submit their code into a black box environment for testing. Participants will have the option to make their submission open or closed when accepting the prize. Open proposals will be eligible for challenge prizes as long as they abide by the open source licensing terms. Closed proposals will be proprietary and not be eligible to accept the prizes. Regardless of which track is chosen, all submissions will be evaluated in the same way. Results will be shown on the leaderboard. The PAI Steering Committee has emphasized the need to ensure that all technical efforts incorporate attention to how the resulting code and products based on it can be made as accessible and useful as possible to key frontline defenders of information quality such as journalists and civic leaders around the world. The DFDC results will be a contribution to this effort and building a robust response to the emergent threat deepfakes pose globally.",
        "dataset_text": "This competition is closed for submissions. Participants' selected code submissions were re-run by the host on a privately-held test set and the private leaderboard results have been finalized. Late submissions will not be opened, due to an inability to replicate the unique design of this competition. This code competition's training set is not available directly on Kaggle, as its size is prohibitively large to train in Kaggle. Instead, it's strongly recommended that you train offline and load the externally trained model as an external dataset into Kaggle Notebooks to perform inference on the Test Set. Review Getting Started for more detailed information. The full training set is just over 470 GB. We've made it available as one giant file, as well as 50 smaller files, each ~10 GB in size. You must accept the competition's rules to gain access to any of the links below.  Full dataset: Dataset split into smaller chunks: A small sample training set has been made available below. You'll also need the sample_submission.csv, as it indicates the IDs you'll be making predictions for. The data is comprised of .mp4 files, split into compressed sets of ~10GB apiece. A metadata.json accompanies each set of .mp4 files, and contains filename, label (REAL/FAKE), original and split columns, listed below under Columns. You will be predicting whether or not a particular video is a deepfake. A deepfake could be either a face or voice swap (or both). In the training data, this is denoted by the string \"REAL\" or \"FAKE\" in the label column. In your submission, you will predict the probability that the video is a fake. To understand the datasets available for this competition, review the Getting Started information."
    },
    {
        "name": "Vesuvius Challenge - Ink Detection",
        "url": "https://www.kaggle.com/competitions/vesuvius-challenge-ink-detection",
        "overview_text": "Overview text not found",
        "description_text": "Join the $1,000,000+ Vesuvius Challenge to resurrect an ancient library from the ashes of a volcano. In this competition you are tasked with detecting ink from 3D X-ray scans and reading the contents. Thousands of scrolls were part of a library located in a Roman villa in Herculaneum, a town next to Pompeii. This villa was buried by the Vesuvius eruption nearly 2000 years ago. Due to the heat of the volcano, the scrolls were carbonized, and are now impossible to open without breaking them. These scrolls were discovered a few hundred years ago and have been waiting to be read using modern techniques. There is a $700,000 grand prize available to the first team that can read these scrolls from a 3D X-ray scan. This Kaggle competition hosts the Ink Detection Progress Prize. Prizes breakdown:  One of the scrolls, which cannot be physically opened (source) This Kaggle competition hosts the Ink Detection Progress Prize ($100,000 in prizes), which is about the sub-problem of detecting ink from 3d x-ray scans of fragments of papyrus which became detached from some of the excavated scrolls. This subcontest is run on Kaggle since it's a more traditional data science / machine learning problem of building a model that can be verified against known ground truth data. The ink used in the Herculaneum scrolls does not show up readily in X-ray scans. But we have found that machine learning models can detect it. Luckily, we have ground truth data. Since the discovery of the Herculaneum Papyri almost 300 years ago, people have tried opening them, often with disastrous results. Many scrolls were destroyed in this process, but ink can be seen on some broken-off fragments, especially under infrared light. The dataset contains 3d x-ray scans of four such fragments at 4\u00b5m resolution, made using a particle accelerator, as well as infrared photographs of the surface of the fragments showing visible ink. These photographs have been aligned with the x-ray scans. We also provide hand-labeled binary masks indicating the presence of ink in the photographs. Get started exploring this problem:  Vesuvius Challenge is created by Nat Friedman and Daniel Gross, and is hosted in collaboration with EduceLab. We appreciate Kaggle\u2019s help with this Ink Detection competition. For more information on the organizing team, partner organizations, and other background, check out scrollprize.org. ",
        "dataset_text": "Your challenge is to recover where ink is present from 3d x-ray scans of detached fragments of ancient papyrus scrolls. This is an important subproblem in the overall task of solving the Vesuvius Challenge. This is a Code Competition. When your submitted notebook is scored, the actual test data will be made available to your notebook. Before that, the test/ directory will contain dummy data. This is done to keep the actual test data secret. For an example program, see the tutorial notebook. "
    },
    {
        "name": "Heritage Health Prize",
        "url": "https://www.kaggle.com/competitions/hhp",
        "overview_text": "Overview text not found",
        "description_text": "Please note: This competition is over! The leaderboard now displays the final results. This means that the only people can download the data or make submissions are people who accepted the competition rules prior to 06:59:59 UTC on October 4, 2012. Individuals who had accepted to rules but not yet formed a team at that date may join a team or create their own team (consisting of them only). No teams may merge at this point. ------------------ More than 71 million individuals in the United States are admitted to hospitals each year, according to the latest survey from the American Hospital Association. Studies have concluded that in 2006 well over $30 billion was spent on unnecessary hospital admissions. Is there a better way? Can we identify earlier those most at risk and ensure they get the treatment they need? The Heritage Provider Network (HPN) believes that the answer is \"yes\u201d. To achieve its goal of developing a breakthrough algorithm that uses available patient data to predict and prevent unnecessary hospitalizations, HPN is sponsoring the Heritage Health Prize Competition (the \u201cCompetition\u201d). HPN believes that incentivized competition is the best way to achieve the radical breakthroughs necessary to begin fixing America\u2019s health care system. The winning team will create an algorithm that predicts how many days a patient will spend in a hospital in the next year. Once known, health care providers can develop new care plans and strategies to reach patients before emergencies occur, thereby reducing the number of unnecessary hospitalizations. This will result in increasing the health of patients while decreasing the cost of care. In short, a winning solution will change health care delivery as we know it \u2013 from an emphasis on caring for the individual after they get sick to a true health care system. The Competition runs for two years and offers a US $3 million Grand Prize, as well as six Milestone Prizes totaling $230,000, which are awarded in varying amounts at three designated intervals during the Competition.",
        "dataset_text": "Note: This competition is NOW CLOSED. DATA IS NO LONGER AVAILABLE FOR ANY REASON.  IMPORTANT NOTE: The information provided below is intended only to provide general guidance to participants in the Heritage Health Prize Competition and is subject to the Competition Official Rules. Any capitalized term not defined below is defined in the Competition Official Rules. Please consult the Competition Official Rules for complete details. Heritage Provider Network is providing Competition Entrants with deidentified member data collected during a forty-eight month period that is allocated among three data sets (the \"Data Sets\"). Competition Entrants will use the Data Sets to develop and test their algorithms for accurately predicting the number of days that the members will spend in a hospital (inpatient or emergency room visit) during the 12-month period following the Data Set cut-off date. HHP_release3.zip contains the latest files, so you can ignore HHP_release2.zip. SampleEntry.CSV shows you how an entry should look. Data Sets will be released to Entrants after registration on the Website according to the following schedule: Entrants are welcome to use other data to develop and test their algorithms and entries until 11:59:59 UTC on April 4, 2012 if the data are (i) freely available to all other Entrants and (i) published (or a link provided) to the data in the External Data portion of the Forum within one (1) week of an entry submission using the other data. Entrants may not use any data other than the Data Sets after 11:59:59 UTC on April 4, 2012 without prior approval. Tables\nEach of the Data Sets will be comprised of tables as follows: For more information on Competition Data, please see the Official Rules (particularly Rules 5-7), the FAQs or the Forum or send us a message through the \"Contact Us\" function. "
    },
    {
        "name": "GE Flight Quest",
        "url": "https://www.kaggle.com/competitions/flight",
        "overview_text": "Overview text not found",
        "description_text": " Did you know airlines are constantly looking for ways to make flights more efficient? From gate conflicts to operational challenges to air traffic management, the dynamics of a flight can change quickly and lead to costly delays. There is good news. Advancements in real-time big data analysis are changing the course of flight as we know it. Imagine if the pilot could augment their decision-making process with \u201creal time business intelligence,\u201d\u2014information available in the cockpit that would allow them to make adjustments to their flight patterns.  Your challenge, should you decide to accept it:  Use the different data sets found on this page under Get the Data to develop a usable and scalable algorithm that delivers a real-time flight profile to the pilot, helping them make flights more efficient and reliably on time. ",
        "dataset_text": " The Flight Quest data is temporally split into five datasets that will be released over the course of the competition. The names and dates of the flight data contained in each dataset are as follows: For each day in the test period (first for the public leaderboard, and later for the final evaluation), we will select a random time (uniformly chosen between 9am EST and 9pm EST) and select all of the flights in the air at that cutoff time. You will be provided with relevant data for each day that would be available at the chosen cutoff time. Your model must be structured so that it makes each test day's final test data set predictions based on no information in the final evaluation test data other than the information from that day, which will be in an appropriately named folder. (Reworded for clarification on 11/30/2012. See forum for explanation. The relevant data is divided into folders by day. For these purposes, a \"day\" includes all of the time from 1amPST/4amEST/9amUTC on that day until the same time on the next day. So Nov. 11, 2012 is considered to be from 9am UTC on Nov. 11, 2012 until 9am UTC on Nov. 12, 2012.\n\nFor a fuller description of the data, visit the Flight Quest Data wiki (information on the wiki is solely for background information and does not form part of the official documentation for Flight Quest).\n\nData provided by FlightStats, Inc., and other sources."
    },
    {
        "name": "Flight Quest 2: Flight Optimization, Milestone Phase",
        "url": "https://www.kaggle.com/competitions/flight2-milestone",
        "overview_text": "Overview text not found",
        "description_text": "Did you know airlines are constantly looking for ways to make flights more efficient? From gate conflicts to operational challenges to air traffic management, the dynamics of a flight can change quickly and lead to costly delays. There is good news. Advancements in real-time big data analysis are changing the course of flight as we know it. Imagine if the pilot could augment their decision-making process with \u201creal time business intelligence\u201d \u2014 information available in the cockpit that would allow them to make adjustments to their flight patterns.  Your challenge, should you decide to accept it:  Use the different data sets found on this page under Get the Data to develop a usable and scalable algorithm that delivers a real-time flight profile to the pilot, helping them make flights more efficient and reliably on time. For background on the competition structure, visit the Basic Structure page. Be sure to check the Forums regularly to stay on top of the latest competition news.",
        "dataset_text": "The code linked above is provided subject to the limited License to Code contained in the Competition Rules. See the included license.txt for more information. Flight Quest Phase 2 data is temporally split into 4 datasets that will be released over the course of the competition. Flight data in each dataset is for the following time periods: The leaderboard will be based on data from the following time periods: Guide To Files Naming convention: Revised files will have \"RevX\" in the name where X represents the revision number. Previous versions of the file will be removed. For a fuller description of the data, visit the Flight Stats Data page, or contribute to the Flight Quest Data wiki page. (Information on the wiki is solely for background and does not form part of the official documentation for Flight Quest.) Visit the Timeline to learn about when data will be released. Datasets listed here are provided by Flight Stats, Inc. Please see the Flight Stats Data wiki page for another source of approved weather data."
    },
    {
        "name": "Flight Quest 2: Flight Optimization, Main Phase",
        "url": "https://www.kaggle.com/competitions/flight2-main",
        "overview_text": "Overview text not found",
        "description_text": " Did you know airlines are constantly looking for ways to make flights more efficient? From gate conflicts to operational challenges to air traffic management, the dynamics of a flight can change quickly and lead to costly delays. There is good news. Advancements in real-time big data analysis are changing the course of flight as we know it. Imagine if the pilot could augment their decision-making process with \u201creal time business intelligence\u201d \u2014 information available in the cockpit that would allow them to make adjustments to their flight patterns.  Your challenge, should you decide to accept it:  Use the different data sets found on this page under Get the Data to develop a usable and scalable algorithm that delivers a real-time flight profile to the pilot, helping them make flights more efficient and reliably on time. For background on the competition structure, visit the Basic Structure page. Be sure to check the Forums regularly to stay on top of the latest competition news.",
        "dataset_text": "The code linked above is provided subject to the limited License to Code contained in the Competition Rules. See the included license.txt for more information. Files used and needed by the Flight Simulator can be found on the Flight Quest Data wiki. Flight Quest Phase 2 data is temporally split into 4 datasets that will be released over the course of the competition. Flight data in each dataset is for the following time periods: The leaderboard will be based on data from the following time periods: Guide To Files Naming convention: Revised files will have \"RevX\" in the name where X represents the revision number. Previous versions of the file will be removed. For a fuller description of the data in AggregateTrain and AggregateTestFeatures, visit the Flight Stats Data page.  Visit the Timeline to learn about when data will be released. Datasets listed here are provided, in part, by Flight Stats, Inc. Please see the Flight Stats Data wiki page for another source of approved weather data."
    },
    {
        "name": "Flight Quest 2: Flight Optimization, Final Phase",
        "url": "https://www.kaggle.com/competitions/flight2-final",
        "overview_text": "Overview text not found",
        "description_text": "The submissions to the Final Phase leaderboard closed on Sunday 11:59 pm UTC February 23, 2014.  The teams on the leaderboard carried forward from the Main Phase of the Flight Quest 2 competition. ",
        "dataset_text": "The code linked above is provided subject to the limited License to Code contained in the Competition Rules. See the included license.txt for more information. Files used and needed by the Flight Simulator can be found on the Flight Quest Data wiki. Guide To Files Naming convention: Revised files will have \"RevX\" in the name where X represents the revision number. Previous versions of the file will be removed. For a fuller description of the data in AggregateTrain and AggregateTestFeatures, visit the Flight Stats Data page. Datasets listed here are provided, in part, by Flight Stats, Inc. Please see the Flight Stats Data wiki page for another source of approved weather data."
    },
    {
        "name": "Second Annual Data Science Bowl",
        "url": "https://www.kaggle.com/competitions/second-annual-data-science-bowl",
        "overview_text": "Overview text not found",
        "description_text": "We all have a heart. Although we often take it for granted, it's our heart that gives us the moments in life to imagine, create, and discover. Yet cardiovascular disease threatens to take away these moments. Each day, 1,500 people in the U.S. alone are diagnosed with heart failure\u2014but together, we can help. We can use data science to transform how we diagnose heart disease. By putting data science to work in the cardiology field, we can empower doctors to help more people live longer lives and spend more time with those that they love. Declining cardiac function is a key indicator of heart disease. Doctors determine cardiac function by measuring end-systolic and end-diastolic volumes (i.e., the size of one chamber of the heart at the beginning and middle of each heartbeat), which are then used to derive the ejection fraction (EF). EF is the percentage of blood ejected from the left ventricle with each heartbeat. Both the volumes and the ejection fraction are predictive of heart disease. While a number of technologies can measure volumes or EF, Magnetic Resonance Imaging (MRI) is considered the gold standard test to accurately assess the heart's squeezing ability.  The challenge with using MRI to measure cardiac volumes and derive ejection fraction, however, is that the process is manual and slow. A skilled cardiologist must analyze MRI scans to determine EF. The process can take up to 20 minutes to complete\u2014time the cardiologist could be spending with his or her patients. Making this measurement process more efficient will enhance doctors' ability to diagnose heart conditions early, and carries broad implications for advancing the science of heart disease treatment. The 2015 Data Science Bowl challenges you to create an algorithm to automatically measure end-systolic and end-diastolic volumes in cardiac MRIs. You will examine MRI images from more than 1,000 patients. This data set was compiled by the National Institutes of Health and Children's National Medical Center and is an order of magnitude larger than any cardiac MRI data set released previously. With it comes the opportunity for the data science community to take action to transform how we diagnose heart disease. This is not an easy task, but together we can push the limits of what's possible. We can give people the opportunity to spend more time with the ones they love, for longer than ever before. The Data Science Bowl is presented by:  The National Heart, Lung, and Blood Institute (NHLBI) provided the MRI images for this competition. Special thanks to NHLBI Intramural Investigators Dr. Michael Hansen and Dr. Andrew Arai. Additional support for the Data Science Bowl was provided by NVIDIA: ",
        "dataset_text": "In this dataset, you are given hundreds of cardiac MRI images in DICOM format. These are 2D cine images that contain approximately 30 images across the cardiac cycle. Each slice is acquired on a separate breath hold. This is important since the registration from slice to slice is expected to be imperfect. The competition task is to create an automated method capable of determining the left ventricle volume at two points in time: after systole, when the heart is contracted and the ventricles are at their minimum volume, and after diastole, when the heart is at its largest volume.  The volumes at systole, \\\\(V_S\\\\), and diastole, \\\\(V_D\\\\), form the basis of an important clinical measurement known as the ejection fraction: 100\u2217\n\ud835\udc49\n\ud835\udc37\n\u2212\n\ud835\udc49\n\ud835\udc46\n\ud835\udc49\n\ud835\udc37\n.\nV\nV This quantity represents the fraction of outbound blood pumped from the heart with each heartbeat. An ejection fraction that is too low can signify a wide range of cardiac problems. Variations in anatomy, function, image quality, and acquisition make automated quantification of left ventricle size a challenging problem. You will encounter this variation in the competition dataset, which aims to provide a diverse representation of cases. It contains patients from young to old, images from numerous hospitals, and hearts from normal to abnormal cardiac function. A computational method which is robust to these variations could both validate and automate the cardiologists' manual measurement of ejection fraction. This is a two-stage competition. In the first stage, you are building models based on the training dataset, and testing your models by submitting predictions on the validation set. Two weeks before the final deadline, you will submit your model to Kaggle. At this point, the second stage of the competition starts. Kaggle will release the final test dataset, on which you will run your models. The final standings are based on this final test set. Each case has an associated directory of DICOM files. The exact number of images will differ from case to case, either varying in the number of slices, the views which are captured, or the number of frames in the time sequences. The main view for assessing ventricle size is the short axis stack, which contains images taken in a plane perpendicular to the long axis of the left ventricle. These have the prefix \"sax_\" in the competition dataset. Most cases also have alternative views, which you should feel free to incorporate into your methodology. The structure is as follows: The DICOM standard is complex and there are a number of different tools to work with DICOM files. You may find the following resources helpful for managing the competition data: We will add to this section as relevant common questions arise. How do I know where the left ventricle is? How do I compute its volume? Watch this video for a primer on the anatomy and process used by clinicians:  I see more than one series at the same slice location. How should we deal with those cases?  Generally, a slice location is repeated if there is an artifact on the images. You can use either slice but the odds are that the last slice at a given slice location is the best the technologist could acquire. Some MRI images are not consistent (in size, shape, or structure). What should we do about these? We have opted to include as many cases as possible in this dataset. As this is real data from many sources, it is bound to have some amount of unwanted variability. You should do your best to handle these files. Since this is a two stage competition and the test set may have unseen abnormalities, we recommend including some form of error catching as you write your code. The data for the Data Science Bowl is available for research and academic pursuits. Please cite as \u2018Data Science Bowl Cardiac Challenge Data\u2019."
    },
    {
        "name": "Google - American Sign Language Fingerspelling Recognition",
        "url": "https://www.kaggle.com/competitions/asl-fingerspelling",
        "overview_text": "Overview text not found",
        "description_text": "The goal of this competition is to detect and translate American Sign Language (ASL) fingerspelling into text. You will create a model trained on the largest dataset of its kind, released specifically for this competition. The data includes more than three million fingerspelled characters produced by over 100 Deaf signers captured via the selfie camera of a smartphone with a variety of backgrounds and lighting conditions. Your work may help move sign language recognition forward, making AI more accessible for the Deaf and Hard of Hearing community. Voice-enabled assistants open the world of useful and sometimes life-changing features of modern devices. These revolutionary AI solutions include automated speech recognition (ASR) and machine translation. Unfortunately, these technologies are often not accessible to the more than 70 million Deaf people around the world who use sign language to communicate, nor to the 1.5+ billion people affected by hearing loss globally. Fingerspelling uses hand shapes that represent individual letters to convey words. While fingerspelling is only a part of ASL, it is often used for communicating names, addresses, phone numbers, and other information commonly entered on a mobile phone. Many Deaf smartphone users can fingerspell words faster than they can type on mobile keyboards. In fact, ASL fingerspelling can be substantially faster than typing on a smartphone\u2019s virtual keyboard (57 words/minute average versus 36 words/minute US average). But sign language recognition AI for text entry lags far behind voice-to-text or even gesture-based typing, as robust datasets didn't previously exist. Technology that understands sign language fits squarely within Google's mission to organize the world's information and make it universally accessible and useful. Google\u2019s AI principles also support this idea and encourage Google to make products that empower people, widely benefit current and future generations, and work for the common good. This collaboration between Google and the Deaf Professional Arts Network will explore AI solutions that can be scaled globally (such as other sign languages), and support individual user experience needs while interacting with products. Your participation in this competition could help provide Deaf and Hard of Hearing users the option to fingerspell words instead of using a keyboard. Besides convenient text entry for web search, map directions, and texting, there is potential for an app that can then translate this input using sign language-to-speech technology to speak the words. Such an app would enable the Deaf and Hard of Hearing community to communicate with hearing non-signers more quickly and smoothly.",
        "dataset_text": "The goal of this competition is to detect and translate American Sign Language (ASL) fingerspelling into text. This competition requires submissions to be made in the form of TensorFlow Lite models. You are welcome to train your model using the framework of your choice as long as you convert the model checkpoint into the tflite format prior to submission. Please see the evaluation page for details. Update: the rerun dataset has been published here. [train/supplemental_metadata].csv character_to_prediction_index.json [train/supplemental]_landmarks/ The landmark data. The landmarks were extracted from raw videos with the MediaPipe holistic model. Not all of the frames necessarily had visible hands or hands that could be detected by the model.\nThe landmark files contain the same data as in the ASL Signs competition (minus the row ID column) but reshaped into a wide format. This allows you to take advantage of the Parquet format to entirely skip loading landmarks that you aren't using. Landmark data should not be used to identify or re-identify an individual. Landmark data is not intended to enable any form of identity recognition or store any unique biometric identification."
    },
    {
        "name": "LLM Prompt Recovery",
        "url": "https://www.kaggle.com/competitions/llm-prompt-recovery",
        "overview_text": "LLMs are commonly used to rewrite or make stylistic changes to text. The goal of this competition is to recover the LLM prompt that was used to transform a given text.",
        "description_text": "NLP workflows increasingly involve rewriting text, but there's still a lot to learn about how to prompt LLMs effectively. This machine learning competition is designed to be a novel way to dig deeper into this problem. The challenge: recover the LLM prompt used to rewrite a given text. You\u2019ll be tested against a dataset of 1300+ original texts, each paired with a rewritten version from Gemma, Google\u2019s new family of open models.",
        "dataset_text": "The competition dataset comprises text passages that have been rewritten by the Gemma 7b-it LLM with undisclosed prompts. The goal of the competition is to determine what prompts were used. Please note that this is a Code Competition. When your submission is scored, this example test data will be replaced with the full test set. Expect roughly 1,400 original texts in the test set. [train/test].csv sample_submission.csv A submission file in the correct format."
    },
    {
        "name": "National Data Science Bowl",
        "url": "https://www.kaggle.com/competitions/datasciencebowl",
        "overview_text": "Overview text not found",
        "description_text": "Plankton are critically important to our ecosystem, accounting for more than half the primary productivity on earth and nearly half the total carbon fixed in the global carbon cycle. They form the foundation of aquatic food webs including those of large, important fisheries. Loss of plankton populations could result in ecological upheaval as well as negative societal impacts, particularly in indigenous cultures and the developing world. Plankton\u2019s global significance makes their population levels an ideal measure of the health of the world\u2019s oceans and ecosystems. (opens in a new tab)\"> Traditional methods for measuring and monitoring plankton populations are time consuming and cannot scale to the granularity or scope necessary for large-scale studies. Improved approaches are needed. One such approach is through the use of an underwater imagery sensor. This towed, underwater camera system captures microscopic, high-resolution images over large study areas. The images can then be analyzed to assess species populations and distributions. Manual analysis of the imagery is infeasible \u2013 it would take a year or more to manually analyze the imagery volume captured in a single day. Automated image classification using machine learning tools is an alternative to the manual approach. Analytics will allow analysis at speeds and scales previously thought impossible. The automated system will have broad applications for assessment of ocean and ecosystem health. The National Data Science Bowl challenges you to build an algorithm to automate the image identification process. Scientists at the Hatfield Marine Science Center and beyond will use the algorithms you create to study marine food webs, fisheries, ocean conservation, and more. This is your chance to contribute to the health of the world\u2019s oceans, one plankton at a time. The National Data Science Bowl is presented by\nwith data provided by the Hatfield Marine Science Center at Oregon State University.",
        "dataset_text": "In total, Oregon State University\u2019s Hatfield Marine Science Center has captured nearly 50 million plankton images over an 18-day period. This is more than 80 terabytes of data! They need your help creating an automated classification process to better understand the image contents. For this competition, Hatfield scientists have prepared a large collection of labeled images, approximately 30k of which are provided as a training set. Each raw image was run through an automatic process to extract regions of interest, resulting in smaller images that contain a single organism/entity. You must create an algorithm that assigns class probabilities to a given image. Several characteristics of this problem make this classification difficult:  The file train.zip contains labeled images for training. Each folder represents a class. Images within the folder belong to that class. Classes were chosen by the Hatfield experts and represent scientifically meaningful groupings of organisms and objects (see the below FAQs). You will notice that it is possible for different classes to contain the same underlying organism. In these cases, they have been separated based on other factors of interest (e.g. one may represent an organism in motion vs. one that is still). You are permitted to use domain knowledge about the class relationships in your methodology. The official list of classes is the following: The file test.zip contains the test set images, for which you must predict class probabilities. To deter hand labeling of images, Kaggle has supplemented the test set with additional \"ignored\" images. These are not counted in the scoring. sampleSubmission.csv provides the appropriate format for generating a valid submission. plankton_identification.pdf is provided as a rough guide to understand relationships between the classes. The tree-like diagram indicates morphological and biological connections between groups. Dashed lines indicate a weak(er) relationship and solid lines a stronger relationship. written by Jessica Luo, PhD Candidate, Hatfield Marine Science Center We collected the images between May-June 2014 in the Straits of Florida using the In Situ Ichthyoplankton Imaging System (ISIIS), a towed, underwater imaging system using shadowgraph imagery with a line-scan camera. A high-resolution continuous image is parsed into 2048 x 2048 pixel frames, which were corrected using flat-fielding. Frames were then thresholded and segmented into regions of interest (ROIs). These ROI segments are the images provided for this competition. We made every effort to include a representation of real world data in this dataset. In other words, we did not cherry-pick for the best and clearest images, but used images spanning the gamut from blurry to clear, and tiny to big. No. Because of the shadowgraph imagery technique, organisms will be the same size regardless of distance to the camera. Partial images in our dataset occur due to imperfect segmentation. If an organism is highly transparent (like a jellyfish), then sometimes the thresholding and segmentation process will cut off part of the image. This is fairly rare; typically our segmentation process works very well. Yes, there are relationships between the categories. Some of the relationships are taxonomic, some are behavioral / ecological, and some are based on shape. We diagrammed all of these relationships in the plankton relationships document, which can be accessed along with the dataset. Red boxes indicate our categories, blue boxes indicate major groups, solid lines indicate direct relationships, and dashed lines indicate minor relationships or shape similarities. In some cases, we chose to partition organisms into different categories based on shape. For example, in the appendicularians, the shape indicates a behavior (tail beating). Furthermore, appendicularians are often confused with a whole host of other things - most notably, fish larvae - so we have traditionally separated them out by shape to increase classification accuracy. We have a trained team and have cross-validated the classifications, such that certain images were checked by up to five people. That said, there is an intrinsic amount of variability in classifying ambiguous images. For example, Culverhouse et al. (2003) found that experts are able to maintain 84-95% self-consistency in labeling difficult images. A spot check on the self-consistency in this dataset was on the high end of that range. We believe that this is the best possible dataset of these kinds of plankton images currently in existence. Citation: PF Culverhouse, Williams R, Reguera B, Herry V, Gonz\u00e1lez-Gil S. (2003) \"Do experts make mistakes? A comparison of human and machine identification of dinoflagellates.\" Mar. Ecol. Prog. Ser. 247:17-25."
    },
    {
        "name": "2019 Data Science Bowl",
        "url": "https://www.kaggle.com/competitions/data-science-bowl-2019",
        "overview_text": "Overview text not found",
        "description_text": "Uncover new insights in early childhood education and how media can support learning outcomes. Participate in our fifth annual Data Science Bowl, presented by Booz Allen Hamilton and Kaggle. PBS KIDS, a trusted name in early childhood education for decades, aims to gain insights into how media can help children learn important skills for success in school and life. In this challenge, you\u2019ll use anonymous gameplay data, including knowledge of videos watched and games played, from the PBS KIDS Measure Up! app, a game-based learning tool developed as a part of the CPB-PBS Ready To Learn Initiative with funding from the U.S. Department of Education. Competitors will be challenged to predict scores on in-game assessments and create an algorithm that will lead to better-designed games and improved learning outcomes. Your solutions will aid in discovering important relationships between engagement with high-quality educational media and learning processes. Data Science Bowl is the world\u2019s largest data science competition focused on social good. Each year, this competition gives Kagglers a chance to use their passion to change the world. Over the last four years, more than 50,000+ Kagglers have submitted over 114,000+ submissions, to improve everything from lung cancer and heart disease detection to ocean health. For more information on the Data Science Bowl, please visit DataScienceBowl.com The data used in this competition is anonymous, tabular data of interactions with the PBS KIDS Measure Up! app. Select data, such as a user\u2019s in-app assessment score or their path through the game, is collected by the PBS KIDS Measure Up! app, a game-based learning tool. PBS KIDS is committed to creating a safe and secure environment that family members of all ages can enjoy. The PBS KIDS Measure Up! app does not collect any personally identifying information, such as name or location. All of the data used in the competition is anonymous. To view the full PBS KIDS privacy policy, please visit: pbskids.org/privacy. No one will be able to download the entire data set and the participants do not have access to any personally identifiable information about individual users. The Data Science Bowl and the use of data for this year\u2019s competition has been reviewed to ensure that it meets requirements of applicable child privacy regulations by PRIVO, a leading global industry expert in children\u2019s online privacy. In the PBS KIDS Measure Up! app, children ages 3 to 5 learn early STEM concepts focused on length, width, capacity, and weight while going on an adventure through Treetop City, Magma Peak, and Crystal Caves. Joined by their favorite PBS KIDS characters, children can also collect rewards and unlock digital toys as they play. To learn more about PBS KIDS Measure Up!, please click here. PBS KIDS and the PBS KIDS Logo are registered trademarks of PBS. Used with permission. The contents of PBS KIDS Measure Up! were developed under a grant from the Department of Education. However, those contents do not necessarily represent the policy of the Department of Education, and you should not assume endorsement by the Federal Government. The app is funded by a Ready To Learn grant (PR/AWARD No. U295A150003, CFDA No. 84.295A) provided by the Department of Education to the Corporation for Public Broadcasting.",
        "dataset_text": "Note: This dataset was removed after the competition, per the host's request. In this dataset, you are provided with game analytics for the PBS KIDS Measure Up! app. In this app, children navigate a map and complete various levels, which may be activities, video clips, games, or assessments. Each assessment is designed to test a child's comprehension of a certain set of measurement-related skills. There are five assessments: Bird Measurer, Cart Balancer, Cauldron Filler, Chest Sorter, and Mushroom Sorter. The intent of the competition is to use the gameplay data to forecast how many attempts a child will take to pass a given assessment (an incorrect answer is counted as an attempt). Each application install is represented by an installation_id. This will typically correspond to one child, but you should expect noise from issues such as shared devices. In the training set, you are provided the full history of gameplay data. In the test set, we have truncated the history after the start event of a single assessment, chosen randomly, for which you must predict the number of attempts. Note that the training set contains many installation_ids which never took assessments, whereas every installation_id in the test set made an attempt on at least one assessment. The outcomes in this competition are grouped into 4 groups (labeled accuracy_group in the data): The file train_labels.csv has been provided to show how these groups would be computed on the assessments in the training set. Assessment attempts are captured in event_code 4100 for all assessments except for Bird Measurer, which uses event_code 4110. If the attempt was correct, it contains \"correct\":true. Note that this is a synchronous rerun code competition and the private test set has approximately 8MM rows. You should be mindful of memory in your notebooks to avoid submission errors. These are the main data files which contain the gameplay events. This file gives the specification of the various event types. This file demonstrates how to compute the ground truth for the assessments in the training set. A sample submission in the correct format."
    },
    {
        "name": "Feedback Prize - Evaluating Student Writing",
        "url": "https://www.kaggle.com/competitions/feedback-prize-2021",
        "overview_text": "Overview text not found",
        "description_text": "Writing is a critical skill for success. However, less than a third of high school seniors are proficient writers, according to the National Assessment of Educational Progress. Unfortunately, low-income, Black, and Hispanic students fare even worse, with less than 15 percent demonstrating writing proficiency. One way to help students improve their writing is via automated feedback tools, which evaluate student writing and provide personalized feedback. There are currently numerous automated writing feedback tools, but they all have limitations. Many often fail to identify writing structures, such as thesis statements and support for claims, in essays or do not do so thoroughly. Additionally, the majority of the available tools are proprietary, with algorithms and feature claims that cannot be independently backed up. More importantly, many of these writing tools are inaccessible to educators because of their cost. This problem is compounded for under-serviced schools which serve a disproportionate number of students of color and from low-income backgrounds. In short, the field of automated writing feedback is ripe for innovation that could help democratize education. Georgia State University (GSU) is an undergraduate and graduate urban public research institution in Atlanta. U.S. News & World Report ranked GSU as one of the most innovative universities in the nation. GSU awards more bachelor\u2019s degrees to African-Americans than any other non-profit college or university in the country. GSU and The Learning Agency Lab, an independent nonprofit based in Arizona, are focused on developing science of learning-based tools and programs for social good. In this competition, you\u2019ll identify elements in student writing. More specifically, you will automatically segment texts and classify argumentative and rhetorical elements in essays written by 6th-12th grade students. You'll have access to the largest dataset of student writing ever released in order to test your skills in natural language processing, a fast-growing area of data science.  If successful, you'll make it easier for students to receive feedback on their writing and increase opportunities to improve writing outcomes. Virtual writing tutors and automated writing systems can leverage these algorithms while teachers may use them to reduce grading time. The open-sourced algorithms you come up with will allow any educational organization to better help young writers develop. Georgia State University and the Learning Agency Lab would like to thank the Bill & Melinda Gates Foundation, Schmidt Futures and Chan Zuckerberg Initiative for their support in making this work possible.                           ",
        "dataset_text": "Update You may read more about Feedback Prize 1.0 data from the following publication: The dataset contains argumentative essays written by U.S students in grades 6-12. The essays were annotated by expert raters for elements commonly found in argumentative writing. Note that this is a code competition, in which you will submit code that will be run against an unseen test set. The unseen test set is approximately 10k documents. A small public test sample has been provided for testing your notebooks. Your task is to predict the human annotations. You will first need to segment each essay into discrete rhetorical and argumentative elements (i.e., discourse elements) and then classify each element as one of the following: The training set will consist of individual essays in a folder of .txt files, as well as a .csv file containing the annotated version of these essays. It is important to note that some parts of the essays will be unannotated (i.e., they do not fit into one of the classifications above). Files Submission File Format Competitors will submit a .csv file of their predictions for the essays in the test set with the following format: Please see the Evaluation page for more details."
    },
    {
        "name": "The Nature Conservancy Fisheries Monitoring",
        "url": "https://www.kaggle.com/competitions/the-nature-conservancy-fisheries-monitoring",
        "overview_text": "Overview text not found",
        "description_text": "Nearly half of the world depends on seafood for their main source of protein. In the Western and Central Pacific, where 60% of the world\u2019s tuna is caught, illegal, unreported, and unregulated fishing practices are threatening marine ecosystems, global seafood supplies and local livelihoods. The Nature Conservancy is working with local, regional and global partners to preserve this fishery for the future.  Currently, the Conservancy is looking to the future by using cameras to dramatically scale the monitoring of fishing activities to fill critical science and compliance monitoring data gaps. Although these electronic monitoring systems work well and are ready for wider deployment, the amount of raw data produced is cumbersome and expensive to process manually. The Conservancy is inviting the Kaggle community to develop algorithms to automatically detect and classify species of tunas, sharks and more that fishing boats catch, which will accelerate the video review process. Faster review and more reliable data will enable countries to reallocate human capital to management and enforcement activities which will have a positive impact on conservation and our planet. Machine learning has the ability to transform what we know about our oceans and how we manage them. You can be part of the solution. You can learn more about this competition and The Nature Conservancy in the video below. Your browser does not support the video tag.",
        "dataset_text": "Warning: This data contains graphic contents that some may find disturbing. Please note that this competition is governed by the Competition Rules and this NDA.  In this competition, The Nature Conservancy asks you to help them detect which species of fish appears on a fishing boat, based on images captured from boat cameras of various angles.   Your goal is to predict the likelihood of fish species in each picture. Eight target categories are available in this dataset: Albacore tuna, Bigeye tuna, Yellowfin tuna, Mahi Mahi, Opah, Sharks, Other (meaning that there are fish present but not in the above categories), and No Fish (meaning that no fish is in the picture). Each image has only one fish category, except that there are sometimes very small fish in the pictures that are used as bait.  The dataset was compiled by The Nature Conservancy in partnership with Satlink, Archipelago Marine Research, the Pacific Community, the Solomon Islands Ministry of Fisheries and Marine Resources, the Australia Fisheries Management Authority, and the governments of New Caledonia and Palau. "
    },
    {
        "name": "TensorFlow - Help Protect the Great Barrier Reef",
        "url": "https://www.kaggle.com/competitions/tensorflow-great-barrier-reef",
        "overview_text": "Overview text not found",
        "description_text": "The goal of this competition is to accurately identify starfish in real-time by building an object detection model trained on underwater videos of coral reefs. Your work will help researchers identify species that are threatening Australia's Great Barrier Reef and take well-informed action to protect the reef for future generations. Australia's stunningly beautiful Great Barrier Reef is the world\u2019s largest coral reef and home to 1,500 species of fish, 400 species of corals, 130 species of sharks, rays, and a massive variety of other sea life. Unfortunately, the reef is under threat, in part because of the overpopulation of one particular starfish \u2013 the coral-eating crown-of-thorns starfish (or COTS for short). Scientists, tourism operators and reef managers established a large-scale intervention program to control COTS outbreaks to ecologically sustainable levels. (opens in a new tab)\"> To know where the COTS are, a traditional reef survey method, called \"Manta Tow\", is performed by a snorkel diver. While towed by a boat, they visually assess the reef, stopping to record variables observed every 200m. While generally effective, this method faces clear limitations, including operational scalability, data resolution, reliability, and traceability. The Great Barrier Reef Foundation established an innovation program to develop new survey and intervention methods to provide a step change in COTS Control. Underwater cameras will collect thousands of reef images and AI technology could drastically improve the efficiency and scale at which reef managers detect and control COTS outbreaks. To scale up video-based surveying systems, Australia\u2019s national science agency, CSIRO has teamed up with Google to develop innovative machine learning technology that can analyse large image datasets accurately, efficiently, and in near real-time.  Please cite this short paper if you are using this dataset for research purposes.",
        "dataset_text": "In this competition, you will predict the presence and position of crown-of-thorns starfish in sequences of underwater images taken at various times and locations around the Great Barrier Reef. Predictions take the form of a bounding box together with a confidence score for each identified starfish. An image may contain zero or more starfish. This competition uses a hidden test set that will be served by an API to ensure you evaluate the images in the same order they were recorded within each video. When your submitted notebook is scored, the actual test data (including a sample submission) will be availabe to your notebook. train/ - Folder containing training set photos of the form video_{video_id}/{video_frame_number}.jpg. [train/test].csv - Metadata for the images. As with other test files, most of the test metadata data is only available to your notebook upon submission. Just the first few rows available for download. example_sample_submission.csv - A sample submission file in the correct format. The actual sample submission will be provided by the API; this is only provided to illustrate how to properly format predictions. The submission format is further described on the Evaluation page. example_test.npy - Sample data that will be served by the example API. greatbarrierreef - The image delivery API that will serve the test set pixel arrays. You may need Python 3.7 and a Linux environment to run the example offline without errors."
    },
    {
        "name": "Google AI4Code \u2013 Understand Code in Python Notebooks",
        "url": "https://www.kaggle.com/competitions/AI4Code",
        "overview_text": "Overview text not found",
        "description_text": "The goal of this competition is to understand the relationship between code and comments in Python notebooks. You are challenged to reconstruct the order of markdown cells in a given notebook based on the order of the code cells, demonstrating comprehension of which natural language references which code. Research teams across Google and Alphabet are exploring new ways that machine learning can assist software developers, and want to rally more members of the developer community to help explore this area too. Python notebooks provide a unique learning opportunity, because unlike a lot of standard source code, notebooks often follow narrative format, with comment cells implemented in markdown that explain a programmer's intentions for corresponding code cells. An understanding of the relationships between code and markdown could lend to fresh improvements across many aspects of AI-assisted development, such as the construction of better data filtering and preprocessing pipelines for model training, or automatic assessments of a notebook's readability. We have assembled a dataset of approximately 160,000 public Python notebooks from Kaggle and have teamed up with X, the moonshot factory to design a competition that challenges participants to use this dataset of published notebooks to build creative techniques aimed at better understanding the relationship between comment cells and code cells.  After the submission deadline, Kaggle and X will evaluate the performance of submitted techniques on new, previously unseen notebooks. We're excited to see how the insights learned from this competition affect the future of notebook authorship.",
        "dataset_text": "The dataset for this competition comprises about 160,000 Jupyter notebooks published by the Kaggle community. Jupyter notebooks are the tool of choice for many data scientists for their ability to tell a narrative with both code and natural language. These two types of discourse are contained within cells of the notebook, and we refer to these cells as either code cells or markdown cells (markdown being the text formatting language used by Jupyter). Your task is to predict the correct ordering of the cells in a given notebook whose markdown cells have been shuffled. The notebooks in this dataset have been selected and processed to ensure their suitability for the competition task. All notebooks: This is a code competition, in which you will submit a model or code that will be run against a future test set: To help you author submission code, we include a few example instances selected from the test set. When you submit your notebook for scoring, this example data will be replaced by the actual test data, including the sample_submission.csv file."
    },
    {
        "name": "Google - Unlock Global Communication with Gemma",
        "url": "https://www.kaggle.com/competitions/gemma-language-tuning",
        "overview_text": "This competition invites you to fine-tune Gemma 2 for a specific language or cultural context. By creating clear, easy-to-follow notebooks, you'll empower others to learn and contribute to the development of language models for diverse communities.",
        "description_text": "With over 7,000 languages and countless cultural differences, AI has the potential to foster global understanding. In a step towards broader linguistic inclusion, we're launching a Kaggle competition focused on adapting Gemma 2, Google's open model family, for 73 eligible languages. These languages were selected to represent a diverse range and to align with the expertise of our judging panel for effective evaluation. Our initial focus on these languages will allow us to establish a robust foundation of techniques and resources that will later enable us to support under-resourced languages. You\u2019re challenged to create notebooks that demonstrate the complete process of adapting Gemma 2, including: Your notebooks should be designed to be easily understood and replicated by others, enabling them to adapt Gemma 2 for even more languages and cultural contexts. Consider exploring areas like: Participants will also need to publish their trained models on Kaggle Models. Ready to contribute to a more inclusive and interconnected world? Join the competition today and help us unlock the potential of language AI for everyone!",
        "dataset_text": "This competition does not require use of a dataset. The submission_instructions.txt file is included below for reference."
    },
    {
        "name": "Flu Forecasting",
        "url": "https://www.kaggle.com/competitions/genentech-flu-forecasting",
        "overview_text": "Overview text not found",
        "description_text": " Seasonal influenza, commonly referred to as \u201cthe flu\u201d, affects 5-20% of the United States population ever year, causing over 200,000 people to be hospitalized from associated complications. Influenza is a contagious respiratory illness, which can range in severity from mild cases with cold-like symptoms to death. Flu epidemics are fast-moving and spread rapidly due to rapid viral reproduction and short generation times (time from when an infected person infects another), which makes them very difficult to control. Additionally, there are several different strains of the influenza virus and new viruses constantly evolving. All together, this poses a significant challenge when it comes to predicting when, where and at what level of severity the flu will strike during the flu season. The objective of this competition is to build an algorithm that helps predict where the occurrence, peak and severity of influenza in a given season. This competition is only open to Masters-level participants who meet the eligibility criteria. Visit the Enter the Competition page to view the eligibility criteria and request entrance. * Image courtesy of CDC",
        "dataset_text": "Additional rules and information about the data files can be found in AdditionalInformation.pdf."
    },
    {
        "name": "G-Research Crypto Forecasting",
        "url": "https://www.kaggle.com/competitions/g-research-crypto-forecasting",
        "overview_text": "Overview text not found",
        "description_text": "Over $40 billion worth of cryptocurrencies are traded every day. They are among the most popular assets for speculation and investment, yet have proven wildly volatile. Fast-fluctuating prices have made millionaires of a lucky few, and delivered crushing losses to others. Could some of these price movements have been predicted in advance? In this competition, you'll use your machine learning expertise to forecast short term returns in 14 popular cryptocurrencies. We have amassed a dataset of millions of rows of high-frequency market data dating back to 2018 which you can use to build your model. Once the submission deadline has passed, your final score will be calculated over the following 3 months using live crypto data as it is collected. The simultaneous activity of thousands of traders ensures that most signals will be transitory, persistent alpha will be exceptionally difficult to find, and the danger of overfitting will be considerable. In addition, since 2018, interest in the cryptomarket has exploded, so the volatility and correlation structure in our data are likely to be highly non-stationary. The successful contestant will pay careful attention to these considerations, and in the process gain valuable insight into the art and science of financial forecasting. G-Research is Europe\u2019s leading quantitative finance research firm. We have long explored the extent of market prediction possibilities, making use of machine learning, big data, and some of the most advanced technology available. Specializing in data science and AI education for workforces, Cambridge Spark is partnering with G-Research for this competition. Watch our introduction to the competition below: (opens in a new tab)\"> ",
        "dataset_text": "This dataset contains information on historic trades for several cryptoassets, such as Bitcoin and Ethereum. Your challenge is to predict their future returns. As historic cryptocurrency prices are not confidential this will be a forecasting competition using the time series API. Furthermore the public leaderboard targets are publicly available and are provided as part of the competition dataset. Expect to see many people submitting perfect submissions for fun. Accordingly, the active phase public leaderboard for this competition was not meaningful and was only provided as a convenience for anyone who wants to test their code. The forecasting phase public leaderboard and final private leaderboard will be determined using real market data gathered after the submission period closes."
    },
    {
        "name": "Severstal: Steel Defect Detection",
        "url": "https://www.kaggle.com/competitions/severstal-steel-defect-detection",
        "overview_text": "Overview text not found",
        "description_text": "Steel is one of the most important building materials of modern times. Steel buildings are resistant to natural and man-made wear which has made the material ubiquitous around the world. To help make production of steel more efficient, this competition will help identify defects.  Severstal is leading the charge in efficient steel mining and production. They believe the future of metallurgy requires development across the economic, ecological, and social aspects of the industry\u2014and they take corporate responsibility seriously. The company recently created the country\u2019s largest industrial data lake, with petabytes of data that were previously discarded. Severstal is now looking to machine learning to improve automation, increase efficiency, and maintain high quality in their production. The production process of flat sheet steel is especially delicate. From heating and rolling, to drying and cutting, several machines touch flat steel by the time it\u2019s ready to ship. Today, Severstal uses images from high frequency cameras to power a defect detection algorithm. In this competition, you\u2019ll help engineers improve the algorithm by localizing and classifying surface defects on a steel sheet. If successful, you\u2019ll help keep manufacturing standards for steel high and enable Severstal to continue their innovation, leading to a stronger, more efficient world all around us.",
        "dataset_text": "In this competition you will be predicting the location and type of defects found in steel manufacturing. Images are named with a unique ImageId. You must segment and classify the defects in the test set. Each image may have no defects, a defect of a single class, or defects of multiple classes. For each image you must segment defects of each class (ClassId = [1, 2, 3, 4]). The segment for each defect class will be encoded into a single row, even if there are several non-contiguous defect locations on an image. You can read more about the encoding standard on the Evaluation page. Submissions to this competition must be made through Kernels. After your submission against the Public test set, your kernel will re-run automatically against the entire Public and Private (unseen) test set. Refer to the Kernels Requirement Page for more information."
    },
    {
        "name": "Jane Street Real-Time Market Data Forecasting",
        "url": "https://www.kaggle.com/competitions/jane-street-real-time-market-data-forecasting",
        "overview_text": "In this competition, hosted by Jane Street, you'll build a model using real-world data derived from production systems, which offers a glimpse into the daily challenges of successful trading. This challenge highlights the difficulties in modeling financial markets, including fat-tailed distributions, non-stationary time series, and sudden shifts in market behavior.",
        "description_text": "When approaching modeling problems in modern financial markets, there are many reasons to believe that the problems you are trying to solve are impossible. Even if you put aside the beliefs that the prices of financial instruments rationally reflect all available information, you\u2019ll have to grapple with time series and distributions that have properties you don\u2019t encounter in other sorts of modeling problems. Distributions can be famously fat-tailed, time series can be non-stationary, and data can generally fail to satisfy a lot of the underlying assumptions on which very successful statistical approaches rely. Layer on all of this the fact that the financial markets are ultimately a human endeavor involving a large number of individuals and institutions that are constantly changing with advances in technology and shifts in society, and responding to economic and geopolitical issues as they arise - and you can start to get a sense of the difficulties involved! In this challenge, we ask you to build a model using real-world data derived from some of our production systems. This data gives a very close picture of some of the things we have to do every day to be successful at trading in modern financial markets. We\u2019ve assembled a collection of features and responders related to markets where we run automated trading strategies and are concerned about having good underlying models. To balance crafting a challenging, relevant problem that ties into our business while respecting the proprietary and highly competitive nature of our trading, you will notice that we have anonymized and lightly obfuscated some of the features and responders we present in the data. These modifications don\u2019t change the essence of the problem at hand, but they do allow us to give you a difficult task that meaningfully illustrates the work we do at Jane Street. Jane Street has spent decades relentlessly innovating on all aspects of our trading, and building machine learning models to aid our decision-making. These models help us actively trade thousands of financial products each day across 200+ trading venues around the world. While this challenge only presents a tiny fraction of the quantitative problems Jane Streeters work on daily, we are very interested in seeing how the Kaggle community will approach this challenge, and in engaging with you about your solutions to the problem!",
        "dataset_text": "The competition dataset comprises a set of timeseries with 79 features and 9 responders, anonymized but representing real market data. The goal of the competition is to forecast one of these responders, i.e., responder_6, for up to six months in the future. You must submit to this competition using the provided Python evaluation API, which serves test set data one timestep by timestep. To use the API, follow the example in this notebook. (Note that this API is different from our legacy timeseries API used in past forecasting competitions.) In line with the forecasting task, the competition will proceed in two phases: To help you author robust submissions, during the final weeks of the model training phase we will be extending the public test set to include data closer to the submission deadline. Predictions on this extended set will not be scored. At the start of the forecasting phase, the unscored public test set will be extended up to the final day of the model training phase and the private set updated roughly every two weeks. Submissions will be rescored at the time of each update. During the forecasting phase, the evaluation API will serve test data from the beginning of the public set to the end of the private set. You must make predictions at every timestep, but, in this phase, only predictions on the private set are scored. (You may predict 0.0 on the unscored segments, if you like.) Each row in the {train/test}.parquet dataset corresponds to a unique combination of a symbol (identified by symbol_id) and a timestamp (represented by date_id and time_id). You will be provided with multiple responders, with responder_6 being the only responder used for scoring. The date_id column is an integer which represents the day of the event, while time_id represents a time ordering. It's important to note that the real time differences between each time_id are not guaranteed to be consistent. The symbol_id column contains encrypted identifiers. Each symbol_id is not guaranteed to appear in all time_id and date_id combinations. Additionally, new symbol_id values may appear in future test sets."
    },
    {
        "name": "LLM - Detect AI Generated Text",
        "url": "https://www.kaggle.com/competitions/llm-detect-ai-generated-text",
        "overview_text": "In recent years, large language models (LLMs) have become increasingly sophisticated, capable of generating text that is difficult to distinguish from human-written text. In this competition, we hope to foster open research and transparency on AI detection techniques applicable in the real world.",
        "description_text": "Can you help build a model to identify which essay was written by middle and high school students, and which was written using a large language model? With the spread of LLMs, many people fear they will replace or alter work that would usually be done by humans. Educators are especially concerned about their impact on students\u2019 skill development, though many remain optimistic that LLMs will ultimately be a useful tool to help students improve their writing skills. At the forefront of academic concerns about LLMs is their potential to enable plagiarism. LLMs are trained on a massive dataset of text and code, which means that they are able to generate text that is very similar to human-written text. For example, students could use LLMs to generate essays that are not their own, missing crucial learning keystones. Your work on this competition can help identify telltale LLM artifacts and advance the state of the art in LLM text detection. By using texts of moderate length on a variety of subjects and multiple, unknown generative models, we aim to replicate typical detection scenarios and incentivize learning features that generalize across models. Vanderbilt University, together with The Learning Agency Lab, an independent nonprofit based in Arizona, have collaborated with Kaggle on this competition Vanderbilt University and the Learning Agency Lab would like to thank the Bill & Melinda Gates Foundation, Schmidt Futures, and Chan Zuckerberg Initiative for their support in making this work possible.                          ",
        "dataset_text": "The competition dataset comprises about 10,000 essays, some written by students and some generated by a variety of large language models (LLMs). The goal of the competition is to determine whether or not essay was generated by an LLM. All of the essays were written in response to one of seven essay prompts. In each prompt, the students were instructed to read one or more source texts and then write a response. This same information may or may not have been provided as input to an LLM when generating an essay. Essays from two of the prompts compose the training set; the remaining essays compose the hidden test set. Nearly all of the training set essays were written by students, with only a few generated essays given as examples. You may wish to generate more essays to use as training data. Please note that this is a Code Competition. The data in test_essays.csv is only dummy data to help you author your solutions. When your submission is scored, this example test data will be replaced with the full test set. There are about 9,000 essays in the test set, both student written and LLM generated."
    },
    {
        "name": "Home Credit - Credit Risk Model Stability",
        "url": "https://www.kaggle.com/competitions/home-credit-credit-risk-model-stability",
        "overview_text": "The goal of this competition is to predict which clients are more likely to default on their loans. The evaluation will favor solutions that are stable over time.",
        "description_text": "The absence of a credit history might mean a lot of things, including young age or a preference for cash. Without traditional data, someone with little to no credit history is likely to be denied. Consumer finance providers must accurately determine which clients can repay a loan and which cannot and data is key. If data science could help better predict one\u2019s repayment capabilities, loans might become more accessible to those who may benefit from them the most.\nCurrently, consumer finance providers use various statistical and machine learning methods to predict loan risk. These models are generally called scorecards. In the real world, clients' behaviors change constantly, so every scorecard must be updated regularly, which takes time. The scorecard's stability in the future is critical, as a sudden drop in performance means that loans will be issued to worse clients on average. The core of the issue is that loan providers aren't able to spot potential problems any sooner than the first due dates of those loans are observable. Given the time it takes to redevelop, validate, and implement the scorecard, stability is highly desirable. There is a trade-off between the stability of the model and its performance, and a balance must be reached before deployment.\nFounded in 1997, competition host Home Credit is an international consumer finance provider focusing on responsible lending primarily to people with little or no credit history. Home Credit broadens financial inclusion for the unbanked population by creating a positive and safe borrowing experience. We previously ran a competition with Kaggle that you can see here.\nYour work in helping to assess potential clients' default risks will enable consumer finance providers to accept more loan applications. This may improve the lives of people who have historically been denied due to lack of credit history.",
        "dataset_text": "In this competition, you will be predicting default of clients based on internal and external information that are available for each client. Scoring is performed using custom metric that not only evaluates the AUC of predictions but also considers the stability of predictions model across the data range of the test set. To better understand this metric, please refer to the Evaluation tab. This dataset contains a large number of tables as a result of utilizing diverse data sources and the varying levels of data aggregation used while preparing the dataset. Note: All files listed below are found in both .csv and .parquet formats. Base tables\nBase tables store the basic information about the observation and case_id. This is a unique identification of every observation and you need to use it to join the other tables to base tables. Train FIles: Test Files: static_0\nProperties: depth=0, internal data source\nTrain Files: Test Files: static_cb_0\nProperties: depth=0, external data source\nTrain Files: Test Files: applprev_1\nProperties: depth=1, internal data source\nTrain Files: Test Files: other_1\nProperties: depth=1, internal data source\nTrain Files: Test Files: tax_registry_a_1\nProperties: depth=1, external data source, Tax registry provider A\nTrain Files: Test Files: tax_registry_b_1\nProperties: depth=1, external data source, Tax registry provider B\nTrain Files: Test FIles: tax_registry_c_1\nProperties: depth=1, external data source, Tax registry provider C\nTrain Files: Test Files: credit_bureau_a_1\nProperties: depth=1, external data source, Credit bureau provider A\nTrain Files: Test Files: credit_bureau_b_1\nProperties: depth=1, external data source, Credit bureau provider B\nTrain Files: Test files: deposit_1\nProperties: depth=1, internal data source\nTrain Files: Test Files: person_1\nProperties: depth=1, internal data source\nTrain Files: Test Files: debitcard_1\nProperties: depth=1, internal data source\nTrain Files: Test Files: applprev_2\nProperties: depth=2, internal data source\nTrain Files: Test Files: person_2\nProperties: depth=2, internal data source\nTrain Files: Test Files: credit_bureau_a_2\nProperties: depth=2, external data source, Credit bureau provider A\nTrain Files: Test Files: credit_bureau_b_2\nProperties: depth=2, external data source, Credit bureau provider B\nTrain Files: Test Files: Please be aware that the same naming conventions apply to the test files. It's worth noting that some external data providers might not be available for future (test) evaluations, which is anticipated. Each group of tables can comprise one or more individual tables. If a group contains more than one table, they are divided based on WEEK_NUM. This division was implemented to restrict the maximum size of the tables.\nDepth values: You can read more about Credit bureau (CB) here https://en.wikipedia.org/wiki/Credit_bureau. Special columns: All other raw columns in the tables serve as predictors. Their definitions can be found in the file feature_definitions.csv. For depth=0 tables, predictors can be directly used as features. However, for tables with depth>0, you may need to employ aggregation functions that will condense the historical records associated with each case_id into a single feature. In case num_group1 or num_group2 stands for person index (this is clear with predictor definitions) the zero index has special meaning. When num_groupN=0 it is the applicant (the person who applied for a loan). Various predictors were transformed, therefore we have the following notation for similar groups of transformations Please note that transformations within a group are denoted by a capital letter at the end of the predictor name (e.g., maxdbddpdtollast6m_4187119P). We hope that this will simplify the manipulation with predictors. Edits: Test dataset was closed, please read our post with details in https://www.kaggle.com/competitions/home-credit-credit-risk-model-stability/discussion/482474"
    },
    {
        "name": "Two Sigma: Using News to Predict Stock Movements",
        "url": "https://www.kaggle.com/competitions/two-sigma-financial-news",
        "overview_text": "Overview text not found",
        "description_text": "Can we use the content of news analytics to predict stock price performance? The ubiquity of data today enables investors at any scale to make better investment decisions. The challenge is ingesting and interpreting the data to determine which data is useful, finding the signal in this sea of information. Two Sigma is passionate about this challenge and is excited to share it with the Kaggle community. As a scientifically driven investment manager, Two Sigma has been applying technology and data science to financial forecasts for over 17 years. Their pioneering advances in big data, AI, and machine learning have pushed the investment industry forward. Now, they're eager to engage with Kagglers in this continuing pursuit of innovation. By analyzing news data to predict stock prices, Kagglers have a unique opportunity to advance the state of research in understanding the predictive power of the news. This power, if harnessed, could help predict financial outcomes and generate significant economic impact all over the world. Data for this competition comes from the following sources: The THOMSON REUTERS Kinesis Logo and THOMSON REUTERS are trademarks of Thomson Reuters and its affiliated companies in the United States and other countries and used herein under license.",
        "dataset_text": "In this competition, you will be predicting future stock price returns based on two sources of data: Each asset is identified by an assetCode (note that a single company may have multiple assetCodes). Depending on what you wish to do, you may use the assetCode, assetName, or time as a way to join the market data to news data. Since this is a Kernels-only, time-based competition, you will not interact directly with the data files as you would in a standard Kaggle competition. You should refer to the submission instructions for details on how to fetch data and make predictions. As noted in the instructions, you will encounter synthetic future data within competition data. This is included to simulate the volume, timeline, and the computational burden that real future data will introduce. The custom python module also makes it simple to understand what steps are necessary to participate, telling you which assetsCodes to forecast at what time and, by extenstion, which days are market trading days. During stage one, the leaderboard will show performance on a historical period from 2017-01-01 to 2018-07-31. During stage two, Kaggle will re-run participants' selected Kernels on approximately six months of future data. The data is stored and retrieved as Pandas dataframes in the Kernels environment. Columns types are optimized to minimize space in memory. The data includes a subset of US-listed instruments. The set of included instruments changes daily and is determined based on the amount traded and the availability of information. This means that there may be instruments that enter and leave this subset of data. There may therefore be gaps in the data provided, and this does not necessarily imply that that data does not exist (those rows are likely not included due to the selection criteria). The marketdata contains a variety of returns calculated over different timespans. All of the returns in this set of marketdata have these properties: Within the marketdata, you will find the following columns: The news data contains information at both the news article level and asset level (in other words, the table is intentionally not normalized). Market data provided by Intrinio.  News data provided by Thomson Reuters. Copyright \u00a9, Thomson Reuters, 2017. All Rights Reserved. Use, duplication, or sale of this service, or data contained herein, except as described in the Competition Rules, is strictly prohibited. The THOMSON REUTERS Kinesis Logo and THOMSON REUTERS are trademarks of Thomson Reuters and its affiliated companies in the United States and other countries and used herein under license. "
    },
    {
        "name": "TGS Salt Identification Challenge",
        "url": "https://www.kaggle.com/competitions/tgs-salt-identification-challenge",
        "overview_text": "Overview text not found",
        "description_text": " Several areas of Earth with large accumulations of oil and gas also have huge deposits of salt below the surface. But unfortunately, knowing where large salt deposits are precisely is very difficult. Professional seismic imaging still requires expert human interpretation of salt bodies. This leads to very subjective, highly variable renderings. More alarmingly, it leads to potentially dangerous situations for oil and gas company drillers. To create the most accurate seismic images and 3D renderings, TGS (the world\u2019s leading geoscience data company) is hoping Kaggle\u2019s machine learning community will be able to build an algorithm that automatically and accurately identifies if a subsurface target is salt or not.",
        "dataset_text": "Seismic data is collected using reflection seismology, or seismic reflection. The method requires a controlled seismic source of energy, such as compressed air or a seismic vibrator, and sensors record the reflection from rock interfaces within the subsurface. The recorded data is then processed to create a 3D view of earth\u2019s interior. Reflection seismology is similar to X-ray, sonar and echolocation. A seismic image is produced from imaging the reflection coming from rock boundaries. The seismic image shows the boundaries between different rock types. In theory, the strength of reflection is directly proportional to the difference in the physical properties on either sides of the interface. While seismic images show rock boundaries, they don't say much about the rock themselves; some rocks are easy to identify while some are difficult. There are several areas of the world where there are vast quantities of salt in the subsurface. One of the challenges of seismic imaging is to identify the part of subsurface which is salt. Salt has characteristics that makes it both simple and hard to identify. Salt density is usually 2.14 g/cc which is lower than most surrounding rocks. The seismic velocity of salt is 4.5 km/sec, which is usually faster than its surrounding rocks. This difference creates a sharp reflection at the salt-sediment interface. Usually salt is an amorphous rock without much internal structure. This means that there is typically not much reflectivity inside the salt, unless there are sediments trapped inside it. The unusually high seismic velocity of salt can create problems with seismic imaging. The data is a set of images chosen at various locations chosen at random in the subsurface. The images are 101 x 101 pixels and each pixel is classified as either salt or sediment. In addition to the seismic images, the depth of the imaged location is provided for each image. The goal of the competition is to segment regions that contain salt."
    },
    {
        "name": "SIIM-FISABIO-RSNA COVID-19 Detection",
        "url": "https://www.kaggle.com/competitions/siim-covid19-detection",
        "overview_text": "Overview text not found",
        "description_text": "Five times more deadly than the flu, COVID-19 causes significant morbidity and mortality. Like other pneumonias, pulmonary infection with COVID-19 results in inflammation and fluid in the lungs. COVID-19 looks very similar to other viral and bacterial pneumonias on chest radiographs, which makes it difficult to diagnose. Your computer vision model to detect and localize COVID-19 would help doctors provide a quick and confident diagnosis. As a result, patients could get the right treatment before the most severe effects of the virus take hold.  Currently, COVID-19 can be diagnosed via polymerase chain reaction to detect genetic material from the virus or chest radiograph. However, it can take a few hours and sometimes days before the molecular test results are back. By contrast, chest radiographs can be obtained in minutes. While guidelines exist to help radiologists differentiate COVID-19 from other types of infection, their assessments vary. In addition, non-radiologists could be supported with better localization of the disease, such as with a visual bounding box. As the leading healthcare organization in their field, the Society for Imaging Informatics in Medicine (SIIM)'s mission is to advance medical imaging informatics through education, research, and innovation. SIIM has partnered with the Foundation for the Promotion of Health and Biomedical Research of Valencia Region (FISABIO), Medical Imaging Databank of the Valencia Region (BIMCV) and the Radiological Society of North America (RSNA) for this competition. In this competition, you\u2019ll identify and localize COVID-19 abnormalities on chest radiographs. In particular, you'll categorize the radiographs as negative for pneumonia or typical, indeterminate, or atypical for COVID-19. You and your model will work with imaging data and annotations from a group of radiologists. If successful, you'll help radiologists diagnose the millions of COVID-19 patients more confidently and quickly. This will also enable doctors to see the extent of the disease and help them make decisions regarding treatment. Depending upon severity, affected patients may need hospitalization, admission into an intensive care unit, or supportive therapies like mechanical ventilation. As a result of better diagnosis, more patients will quickly receive the best care for their condition, which could mitigate the most severe effects of the virus.   FISABIO, The Foundation for the Promotion of Health and Biomedical Research of Valencia Region The Foundation for the Promotion of Health and Biomedical Research of Valencia Region, FISABIO, is a non-profit scientific and healthcare entity, whose primary purpose is to encourage, to promote and to develop scientific and technical health and biomedical research in Valencia Region. FISABIO integrates and manages the Health Research Map of the Centre for Public Health Research, Dr. Peset University Hospital Foundation, Alicante University General Hospital Foundation, Elche University General Hospital Foundation, and the Mediterranean Ophthalmological Foundation. The BIMCV facility is connected with a multi-level vendor neutral archive (VNA). The imaging population facility is storing data from the Valencia Region, which accounts for more than 5.1 million habitants. Radiological Society of North America (RSNA) The Radiological Society of North America (RSNA) is a non-profit organization that represents 31 radiologic subspecialties from 145 countries around the world. RSNA promotes excellence in patient care and health care delivery through education, research and technological innovation. RSNA provides high-quality educational resources, publishes five top peer-reviewed journals, hosts the world\u2019s largest radiology conference and is dedicated to building the future of the profession through the RSNA Research & Education (R&E) Foundation, which has funded $66 million in grants since its inception. RSNA also supports and facilitates artificial intelligence (AI) research in medical imaging by sponsoring an ongoing series of AI challenge competitions.",
        "dataset_text": "In this competition, we are identifying and localizing COVID-19 abnormalities on chest radiographs. This is an object detection and classification problem. For each test image, you will be predicting a bounding box and class for all findings. If you predict that there are no findings, you should create a prediction of \"none 1 0 0 1 1\" (\"none\" is the class ID for no finding, and this provides a one-pixel bounding box with a confidence of 1.0). Further, for each test study, you should make a determination within the following labels: To make a prediction of one of the above labels, create a prediction string similar to the \"none\" class above: e.g. atypical 1 0 0 1 1 Please see the Evaluation page for more details about formatting predictions. The images are in DICOM format, which means they contain additional data that might be useful for visualizing and classifying. The train dataset comprises 6,334 chest scans in DICOM format, which were de-identified to protect patient privacy.\nAll images were labeled by a panel of experienced radiologists for the presence of opacities as well as overall appearance. Note that all images are stored in paths with the form study/series/image. The study ID here relates directly to the study-level predictions, and the image ID is the ID used for image-level predictions. The hidden test dataset is of roughly the same scale as the training dataset. train_study_level.csv train_image_level.csv The BIMCV-COVID19 Data used by this challenge were originally published by the Medical Imaging Databank of the Valencia Region (BIMCV) in cooperation with The Foundation for the Promotion of Health and Biomedical Research of Valencia Region (FISABIO), and the Regional Ministry of Innovation, Universities, Science and Digital Society (Generalitat Valenciana), however the images were completely re-annotated using different annotation types. Users of this data must abide by the BIMCV-COVID19 Dataset research Use Agreement. Paper Reference: BIMCV COVID-19+: a large annotated dataset of RX and CT images from COVID-19 patients The MIDRC-RICORD Data used by this challenge were originally published by The Cancer Imaging Archive. The images were re-annotated for this challenge using a different annotation schema. Users of this data must abide by the TCIA Data Usage Policy and the Creative Commons Attribution-NonCommercial 4.0 International License under which it has been published. Attribution should include references to citations listed on the TCIA citation information page (page bottom). Paper Reference: The RSNA International COVID-19 Open Radiology Database (RICORD) Want to learn more about the curation, annotation methodology and characteristics of the dataset used in this challenge? Read The 2021 SIIM-FISABIO-RSNA Machine Learning COVID-19 Challenge: Annotation and Standard Exam Classification of COVID-19 Chest Radiographs.\nhttps://doi.org/10.31219/osf.io/532ek To cite: 1. Lakhani P, Mongan J, Singhal C, Zhou Q, Andriole KP, Auffermann WF, Prasanna P, Pham T, Peterson M, Bergquist PJ, Cook TS, Ferraciolli SF, de Antonio Corradi GC, Takahashi M, Workman SS, Parekh M, Kamel S, Galant JH, Mas-Sanchez A, Ben\u00edtez EC, S\u00e1nchez-Valverde M, Jaques L, Panadero M, Vidal M, Culi\u00e1\u00f1ez-Casas M, Angulo-Gonzalez DM, Langer SG, de la Iglesia Vaya M, Shih G. The 2021 SIIM-FISABIO-RSNA Machine Learning COVID-19 Challenge: Annotation and Standard Exam Classification of COVID-19 Chest Radiographs. [Internet]. OSF Preprints; 2021. Available from: osf.io/532ek"
    },
    {
        "name": "Optiver Realized Volatility Prediction",
        "url": "https://www.kaggle.com/competitions/optiver-realized-volatility-prediction",
        "overview_text": "Overview text not found",
        "description_text": "Volatility is one of the most prominent terms you\u2019ll hear on any trading floor \u2013 and for good reason. In financial markets, volatility captures the amount of fluctuation in prices. High volatility is associated to periods of market turbulence and to large price swings, while low volatility describes more calm and quiet markets. For trading firms like Optiver, accurately predicting volatility is essential for the trading of options, whose price is directly related to the volatility of the underlying product. As a leading global electronic market maker, Optiver is dedicated to continuously improving financial markets, creating better access and prices for options, ETFs, cash equities, bonds and foreign currencies on numerous exchanges around the world. Optiver\u2019s teams have spent countless hours building sophisticated models that predict volatility and continuously generate fairer options prices for end investors. However, an industry-leading pricing algorithm can never stop evolving, and there is no better place than Kaggle to help Optiver take its model to the next level. In the first three months of this competition, you\u2019ll build models that predict short-term volatility for hundreds of stocks across different sectors. You will have hundreds of millions of rows of highly granular financial data at your fingertips, with which you'll design your model forecasting volatility over 10-minute periods. Your models will be evaluated against real market data collected in the three-month evaluation period after training. Through this competition, you'll gain invaluable insight into volatility and financial market structure. You'll also get a better understanding of the sort of data science problems Optiver has faced for decades. We look forward to seeing the creative approaches the Kaggle community will apply to this ever complex but exciting trading challenge. In order to make Kagglers better prepared for this competition, Optiver's data scientists have created a tutorial notebook debriefing competition data and relevant financial concepts of this trading challenge. Also, Optiver's online course can tell you more about financial market and market making.\n(opens in a new tab)\"> For more information about exciting data science opportunities at Optiver, check out their data science landing page here or e-mail their recruiting team directly at datascience@optiver.com. ",
        "dataset_text": "This dataset contains stock market data relevant to the practical execution of trades in the financial markets. In particular, it includes order book snapshots and executed trades. With one second resolution, it provides a uniquely fine grained look at the micro-structure of modern financial markets. This is a code competition where only the first few rows of the test set are available for download. The rows that are visible are intended to illustrate the hidden test set format and folder structure. The remainder will only be available to your notebook when it is submitted. The hidden test set contains data that can be used to construct features to predict roughly 150,000 target values. Loading the entire dataset will take slightly more than 3 GB of memory, by our estimation. This is also a forecasting competition, where the final private leaderboard will be determined using data gathered after the training period closes, which means that the public and private leaderboards will have zero overlap. During the active training stage of the competition a large fraction of the test data will be filler, intended only to ensure the hidden dataset has approximately the same size as the actual test data. The filler data will be removed entirely during the forecasting phase of the competition and replaced with real market data. book_[train/test].parquet A parquet file partitioned by stock_id. Provides order book data on the most competitive buy and sell orders entered into the market. The top two levels of the book are shared. The first level of the book will be more competitive in price terms, it will then receive execution priority over the second level. trade_[train/test].parquet A parquet file partitioned by stock_id. Contains data on trades that actually executed. Usually, in the market, there are more passive buy/sell intention updates (book updates) than actual trades, therefore one may expect this file to be more sparse than the order book. train.csv The ground truth values for the training set. test.csv Provides the mapping between the other data files and the submission file. As with other test files, most of the data is only available to your notebook upon submission with just the first few rows available for download. sample_submission.csv - A sample submission file in the correct format."
    },
    {
        "name": "NFL Health & Safety - Helmet Assignment",
        "url": "https://www.kaggle.com/competitions/nfl-health-and-safety-helmet-assignment",
        "overview_text": "Overview text not found",
        "description_text": "The National Football League (NFL) and Amazon Web Services (AWS) are teaming up to develop the best sports injury surveillance and mitigation program. In previous competitions, Kaggle has helped detect helmet impacts. As a next step, the NFL wants to assign specific players to each helmet, which would help accurately identify each player's \u201cexposures\u201d throughout a football play.  Currently, the NFL manually annotates a subset of plays each year to determine a sample of exposures for each player. To expand this program, the current player assignment requires a field map to determine player locations. The NFL is interested in matching this model's accuracy without the need for the mapping step. The league is calling on Kagglers to invent a better way to identify individual players. The National Football League is America's most popular sports league. Founded in 1920, the NFL developed the model for the successful modern sports league and is committed to advancing progress in the diagnosis, prevention, and treatment of sports-related injuries. Health and safety efforts include support for independent medical research and engineering advancements as well as a commitment to work to better protect players and make the game safer, including enhancements to medical protocols and improvements to how our game is taught and played. For more information about the NFL's health and safety efforts, please visit www.NFL.com/PlayerHealthandSafety. In this competition, you\u2019ll identify and assign football players\u2019 helmets from video footage. In particular, you'll create algorithms capable of assigning detected helmet impacts to correct players via tracking information. Successful submissions should aim for 90% accuracy. If successful, you'll support the NFL in its efforts to efficiently improve player safety. If the league no longer has to manually label each exposure, it would dramatically increase the speed and scale at which they could answer complex research questions related to helmet impact. Automatic player detection would also allow the NFL to back-calculate historic exposure trends, allowing for deeper insights into how to mitigate them in the future. ",
        "dataset_text": "In this competition, you are tasked with assigning the correct player from game footage. Each play has two associated videos, showing a sideline and endzone view, and the videos are aligned so that frames correspond between the videos. The training set videos are in train/ with corresponding labels in train_labels.csv, while the videos for which you must predict are in the test/ folder. To aid with helmet detection, you are also provided an ancillary dataset of images showing helmets with labeled bounding boxes. These files are located in images and the bounding boxes in image_labels.csv. This year we are also providing baseline helmet detection boxes for the training and test set. train_baseline_helmets.csv is the output from a baseline helmet detection model which was trained on the images and labels from the images folder. train_player_tracking.csv provides 10 Hz tracking data for each player on the field during the provided plays. This is a code competition. When you submit, your model will be rerun on a set of 15 unseen plays located in a holdout test set. The publicly provided test videos are simply a set of mock plays (copied from the training set) which are not used in scoring. The associated test_baseline_helmets.csv and test_player_tracking.csv are available to your model when submitting. Note: the dataset provided for this competition has been carefully designed for the purposes of training computer vision models and therefore contains plays that have much higher incidence of helmet impacts than is normal. This dataset should not be used to make inferences about the incidence of helmet impact rates during football games, as it is not a representative sample of those rates. [train/test] mp4 videos of each play. Each play has two copies, one shot from the endzone and the other shot from the sideline. The video pairs are matched frame for frame in time, but different players may be visible in each view. You only need to make predictions for the view that a player is actually visible in. train_labels.csv Helmet tracking and collision labels for the training set. Note: The Sideline and Endzone views have been time-synced such that the snap occurs 10 frames into the video. This time alignment should be considered to be accurate to within +/- 3 frames or 0.05 seconds (video data is recorded at approximately 59.94 frames per second). sample_submission.csv A valid sample submission file. images/ contains supplemental photos comparable to the frames of the train/test videos for use making a helmet detector. image_labels.csv contains the bounding boxes corresponding to the images. [train/test]_baseline_helmets.csv contains imperfect baseline predictions for helmet boxes. The model used to create these files was trained only on the additional images found in the images folder. [train/test]_player_tracking.csv Each player wears a sensor that allows us to precisely locate them on the field; that information is reported in these two files. "
    },
    {
        "name": "Ubiquant Market Prediction",
        "url": "https://www.kaggle.com/competitions/ubiquant-market-prediction",
        "overview_text": "Overview text not found",
        "description_text": "Regardless of your investment strategy, fluctuations are expected in the financial market. Despite this variance, professional investors try to estimate their overall returns. Risks and returns differ based on investment types and other factors, which impact stability and volatility. To attempt to predict returns, there are many computer-based algorithms and models for financial market trading. Yet, with new techniques and approaches, data science could improve quantitative researchers' ability to forecast an investment's return.  Ubiquant Investment (Beijing) Co., Ltd is a leading domestic quantitative hedge fund based in China. Established in 2012, they rely on international talents in math and computer science along with cutting-edge technology to drive quantitative financial market investment. Overall, Ubiquant is committed to creating long-term stable returns for investors. In this competition, you\u2019ll build a model that forecasts an investment's return rate. Train and test your algorithm on historical prices. Top entries will solve this real-world data science problem with as much accuracy as possible. If successful, you could improve the ability of quantitative researchers to forecast returns. This will enable investors at any scale to make better decisions. You may even discover you have a knack for financial datasets, opening up a world of new opportunities in many industries. See more information about Ubiquant below: (opens in a new tab)\">",
        "dataset_text": "This dataset is no longer available for download. This dataset contains features derived from real historic data from thousands of investments. Your challenge is to predict the value of an obfuscated metric relevant for making trading decisions. This is a code competition that relies on a time-series API to ensure models do not peek forward in time. To use the API, follow the instructions on the Evaluation page. When you submit your notebook, it will be rerun on an unseen test. This is also a forecasting competition, where the final private leaderboard will be determined using data gathered after the training period closes. train.csv example_test.csv - Random data provided to demonstrate what shape and format of data the API will deliver to your notebook when you submit. example_sample_submission.csv - An example submission file provided so the publicly accessible copy of the API provides the correct data shape and format. supplemental_train.csv - Once submissions are locked on April 18th, the hidden test set copy of this file will be replaced with the data currently used for the public leaderboard. Until then it will contain randomly generated noise of the correct shape to assist with debugging. ubiquant/ - The image delivery API that will serve the test set. You may need Python 3.7 and a Linux environment to run the example test set through the API offline without errors. Time-series API Details"
    },
    {
        "name": "American Express - Default Prediction",
        "url": "https://www.kaggle.com/competitions/amex-default-prediction",
        "overview_text": "Overview text not found",
        "description_text": "Whether out at a restaurant or buying tickets to a concert, modern life counts on the convenience of a credit card to make daily purchases. It saves us from carrying large amounts of cash and also can advance a full purchase that can be paid over time. How do card issuers know we\u2019ll pay back what we charge? That\u2019s a complex problem with many existing solutions\u2014and even more potential improvements, to be explored in this competition. Credit default prediction is central to managing risk in a consumer lending business. Credit default prediction allows lenders to optimize lending decisions, which leads to a better customer experience and sound business economics. Current models exist to help manage risk. But it's possible to create better models that can outperform those currently in use. American Express is a globally integrated payments company. The largest payment card issuer in the world, they provide customers with access to products, insights, and experiences that enrich lives and build business success. In this competition, you\u2019ll apply your machine learning skills to predict credit default. Specifically, you will leverage an industrial scale data set to build a machine learning model that challenges the current model in production. Training, validation, and testing datasets include time-series behavioral data and anonymized customer profile information. You're free to explore any technique to create the most powerful model, from creating features to using the data in a more organic way within a model. If successful, you'll help create a better customer experience for cardholders by making it easier to be approved for a credit card. Top solutions could challenge the credit default prediction model used by the world's largest payment card issuer\u2014earning you cash prizes, the opportunity to interview with American Express, and potentially a rewarding new career.",
        "dataset_text": "The objective of this competition is to predict the probability that a customer does not pay back their credit card balance amount in the future based on their monthly customer profile. The target binary variable is calculated by observing 18 months performance window after the latest credit card statement, and if the customer does not pay due amount in 120 days after their latest statement date it is considered a default event. The dataset contains aggregated profile features for each customer at each statement date. Features are anonymized and normalized, and fall into the following general categories: with the following features being categorical: Your task is to predict, for each customer_ID, the probability of a future payment default (target = 1). Note that the negative class has been subsampled for this dataset at 5%, and thus receives a 20x weighting in the scoring metric."
    },
    {
        "name": "1st and Future - Player Contact Detection",
        "url": "https://www.kaggle.com/competitions/nfl-player-contact-detection",
        "overview_text": "Overview text not found",
        "description_text": "The goal of this competition is to detect external contact experienced by players during an NFL football game. You will use video and player tracking data to identify moments with contact to help improve player safety.  The National Football League (NFL) has teamed up with Amazon Web Services (AWS) to strengthen its commitment to predict player injuries. The NFL aspires to have the best injury surveillance and mitigation program in any sport. With your machine learning and computer vision skills, you can help the NFL accurately identify when players experience contact throughout a football play. In prior years, the NFL challenged the Kaggle community to create helmet impact detection and identification algorithms. This year the NFL looks to automatically identify all moments when players experience contact. This competition will be successful if we can reliably detect moments when players are in contact with one another and when a player\u2019s body is in contact with the ground. Currently, the NFL uses its tracking system to monitor a large number of statistics about players\u2019 load during the season. The league has a solution that predicts contact between players, but it only leverages the player tracking data. This competition hopes to improve the predictive power by including video in addition to tracking data. Categorizing ground contact will also provide a more comprehensive view of impacts, improving analysis for player health and safety. More accurate data is an important step toward the NFL\u2019s injury surveillance and mitigation goals. With complete contact detection, the league can identify correlations between certain types of contact and injury, a contributor to future prevention. Your efforts could help mitigate unsafe situations to reduce injury to all players. The National Football League is America's most popular sports league. Founded in 1920, the NFL developed the model for the successful modern sports league and is committed to advancing progress in the diagnosis, prevention, and treatment of sports-related injuries. This competition is part of the Digital Athlete, a joint effort between the NFL and AWS to build a virtual, 360-degree representation of an NFL player\u2019s experience. The Digital Athlete hopes to generate a precise picture of what they need when it comes to preventing and recovering from injuries while performing at their best. Health and safety efforts include support for independent medical research and engineering advancements as well as a commitment to work to better protect players and make the game safer, including enhancements to medical protocols and improvements to how our game is taught and played. For more information about the NFL's health and safety efforts, please visit the NFL Player Health and Safety website. ",
        "dataset_text": "In this competition, you are tasked with predicting moments of contact between player pairs, as well as when players make non-foot contact with the ground using game footage and tracking data. Each play has four associated videos. Two videos, showing a sideline and endzone view, are time synced and aligned with each other. Additionally, an All29 view is provided but not guaranteed to be time synced. The training set videos are in train/ with corresponding labels in train_labels.csv, while the videos for which you must predict are in the test/ folder. This year we are also providing baseline helmet detection and assignment boxes for the training and test set. train_baseline_helmets.csv is the output from last year's winning player assignment model. train_player_tracking.csv provides 10 Hz tracking data for each player on the field during the provided plays. train_video_metadata.csv contains timestamps associated with each Sideline and Endzone view for syncing with the player tracking data. This is a code competition. When you submit, your model will be rerun on a set of 61 unseen plays located in a holdout test set. The publicly provided test videos are simply a set of mock plays (copied from the training set) which are not used in scoring. The associated test_baseline_helmets.csv, test_player_tracking.csv, and test_video_metadata.csv are available to your model when submitting. A sample_submission.csv will be available when submitting and will contain all rows required for a valid submission. [train/test] mp4 videos of each play. Each play has three videos. The two main view are shot from the endzone and sideline. Sideline and Endzone video pairs are matched frame for frame in time, but different players may be visible in each view. This year, an additional view is provided, All29 which should include view of every player involved in the play. All29 video is not guaranteed to be time synced with the sideline and endzone. These videos all contain a frame rate of 59.94 HZ. The moment of snap occurs 5 seconds into the video. train_labels.csv Contains a row for every combination of players, and players with the ground for each 0.1 second timestamp in the play. Note: Labels may not be exact but are expected to be within +/-10Hz from the actual moment of contact. Labels were created in a multistep process including quality checks, however there still may be mislabels. You should expect the test labels to be of similar quality as the training set labels. sample_submission.csv A valid sample submission file. [train/test]_baseline_helmets.csv contains imperfect baseline predictions for helmet boxes and player assignments for the Sideline and Endzone video view. The model used to create these predictions are from the winning solution from last year's competition and can be used to leverage your predictions. [train/test]_player_tracking.csv Each player wears a sensor that allows us to locate them on the field; that information is reported in these two files.  [train/test]_video_metadata.csv Metadata for each sideline and endzone video file including the timestamp information to be used to sync with player tracking data."
    },
    {
        "name": "Optiver - Trading at the Close",
        "url": "https://www.kaggle.com/competitions/optiver-trading-at-the-close",
        "overview_text": "In this competition, you are challenged to develop a model capable of predicting the closing price movements for hundreds of Nasdaq listed stocks using data from the order book and the closing auction of the stock. Information from the auction can be used to adjust prices, assess supply and demand dynamics, and identify trading opportunities.",
        "description_text": "Stock exchanges are fast-paced, high-stakes environments where every second counts. The intensity escalates as the trading day approaches its end, peaking in the critical final ten minutes. These moments, often characterised by heightened volatility and rapid price fluctuations, play a pivotal role in shaping the global economic narrative for the day. Each trading day on the Nasdaq Stock Exchange concludes with the Nasdaq Closing Cross auction. This process establishes the official closing prices for securities listed on the exchange. These closing prices serve as key indicators for investors, analysts and other market participants in evaluating the performance of individual securities and the market as a whole. Within this complex financial landscape operates Optiver, a leading global electronic market maker. Fueled by technological innovation, Optiver trades a vast array of financial instruments, such as derivatives, cash equities, ETFs, bonds, and foreign currencies, offering competitive, two-sided prices for thousands of these instruments on major exchanges worldwide. In the last ten minutes of the Nasdaq exchange trading session, market makers like Optiver merge traditional order book data with auction book data. This ability to consolidate information from both sources is critical for providing the best prices to all market participants. In this competition, you are challenged to develop a model capable of predicting the closing price movements for hundreds of Nasdaq listed stocks using data from the order book and the closing auction of the stock. Information from the auction can be used to adjust prices, assess supply and demand dynamics, and identify trading opportunities. Your model can contribute to the consolidation of signals from the auction and order book, leading to improved market efficiency and accessibility, particularly during the intense final ten minutes of trading. You'll also get firsthand experience in handling real-world data science problems, similar to those faced by traders, quantitative researchers and engineers at Optiver.",
        "dataset_text": "This dataset contains historic data for the daily ten minute closing auction on the NASDAQ stock exchange. Your challenge is to predict the future price movements of stocks relative to the price future price movement of a synthetic index composed of NASDAQ-listed stocks. This is a forecasting competition using the time series API. The private leaderboard will be determined using real market data gathered after the submission period closes. [train/test].csv The auction data. The test data will be delivered by the API. All size related columns are in USD terms. All price related columns are converted to a price move relative to the stock wap (weighted average price) at the beginning of the auction period. sample_submission A valid sample submission, delivered by the API. See this notebook for a very simple example of how to use the sample submission. revealed_targets When the first time_id for each date (i.e. when seconds_in_bucket equals zero) the API will serve a dataframe providing the true target values for the entire previous date. All other rows contain null values for the columns of interest. public_timeseries_testing_util.py An optional file intended to make it easier to run custom offline API tests. See the script's docstring for details. You will need to edit this file before using it. example_test_files/ Data intended to illustrate how the API functions. Includes the same files and columns delivered by the API. The first three date ids are repeats of the last three date ids in the train set, to enable an illustration of how the API functions. optiver2023/ Files that enable the API. Expect the API to deliver all rows in under five minutes and to reserve less than 0.5 GB of memory. The first three date ids delivered by the API are repeats of the last three date ids in the train set, to better illustrate how the API functions. You must make predictions for those dates in order to advance the API but those predictions are not scored."
    },
    {
        "name": "Open Problems \u2013 Single-Cell Perturbations",
        "url": "https://www.kaggle.com/competitions/open-problems-single-cell-perturbations",
        "overview_text": "The goal of this competition is to predict how small molecules change gene expression in different cell types.",
        "description_text": "Human biology can be complex, in part due to the function and interplay of the body's approximately 37 trillion cells, which are organized into tissues, organs, and systems. However, recent advances in single-cell technologies have provided unparalleled insight into the function of cells and tissues at the level of DNA, RNA, and proteins. Yet leveraging single-cell methods to develop medicines requires mapping causal links between chemical perturbations and the downstream impact on cell state. These experiments are costly and labor intensive, and not all cells and tissues are amenable to high-throughput transcriptomic screening. If data science could help accurately predict chemical perturbations in new cell types, it could accelerate and expand the development of new medicines. Several methods have been developed for drug perturbation prediction, most of which are variations on the autoencoder architecture (Dr.VAE, scGEN, and ChemCPA). However, these methods lack proper benchmarking datasets with diverse cell types to determine how well they generalize. The largest available training dataset is the NIH-funded Connectivity Map (CMap), which comprises over 1.3M small molecule perturbation measurements. However, the CMap includes observations of only 978 genes, less than 5% of all genes. Furthermore, the CMap data is comprised almost entirely of measurements in cancer cell lines, which may not accurately represent human biology. Competition host Open Problems in Single-Cell Analysis is a non-profit scientific collaboration aiming to drive innovation in single-cell data science. They are partnering to host this competition with Cellarity, a first-of-its-kind therapeutics company that develops medicines by studying and altering the cellular signatures of disease. Although it is impossible to measure all perturbations in all cells, we hypothesize that it is possible to measure a subset of combinations and infer the rest. Today, we are far from this goal, but we hope that this competition will serve as an important proof of concept. Your work in helping to accurately predict chemical perturbations in new cell types could accelerate the discovery and enable the creation of new medicines to treat or cure disease.",
        "dataset_text": "For this competition, we designed and generated a novel single-cell perturbational dataset in human peripheral blood mononuclear cells (PBMCs). We selected 144 compounds from the Library of Integrated Network-Based Cellular Signatures (LINCS) Connectivity Map dataset (PMID: 29195078) and measured single-cell gene expression profiles after 24 hours of treatment. The experiment was repeated in three healthy human donors, and the compounds were selected based on diverse transcriptional signatures observed in CD34+ hematopoietic stem cells (data not released). We performed this experiment in human PBMCs because the cells are commercially available with pre-obtained consent for public release and PBMCs are a primary, disease-relevant tissue that contains multiple mature cell types (including T-cells, B-cells, myeloid cells, and NK cells) with established markers for annotation of cell types. To supplement this dataset, we also measured cells from each donor at baseline with joint scRNA and single-cell chromatin accessibility measurements using the 10x Multiome assay. We hope that the addition of rich multi-omic data for each donor and cell type at baseline will help establish biological priors that explain the susceptibility of particular genes to exhibit perturbation responses in difference biological contexts. To understand the dataset, it is important to know the design of the plates used to measure the treamtment effect. PBMCs from donors were thawed and plated on 96-well plates. Two columns of the plates were dedicated to positive controls (dabrfenib and belinostat) and one column was dedicated to a negative control (DMSO). The positive controls were selected because they tend to have a large impact on transcription, and the negative control is used as a solvent for the compounds used in this study. The remaining wells on the plate are allocated to each of 72 compounds. The full dataset comprises 2 different compound plates per donor for a total of 6 plates.  Note, each well contains PBMCs, which are a collection of different cell types. These include T cells, B cells, NK cells, and Myeloid cells like Macrophages and Monocytes. Based on the gene expression data measured in scRNA, we can computationally assign each cell to a cell type. Note, because we only measure ~350 cells per well and because compounds may have a toxic effect on some cells types, we don't always observe every cell type in every well. Another technical variable that will impact the raw data in this experiment is the chemical tagging of each well in each row of the plate, and then pooling all samples in each row into a single pool for sequencing. This is called Cell Multiplexing, and you can read more about it on the 10x Genomics website What is Cell Multiplexing?. What you need to know is that this creates some technical bias linking all the wells in each row of a plate. One purpose of including two positive controls and one negative control in each row of the plate is to allow us to account for this source of noise when we calculate differential expression. In this competition setup, participants are tasked with modelling differential expression (DE), which enables us to estimate the impact of an experimental perturbation on the expression level of every gene in the transcription (18211 genes in this dataset). We estimate the impact of each compound by first averaging the raw gene expression counts in each cell of a specific type in each sample, which is called pseudobulking in the single-cell literature. We then fit a linear model to the pseudobulked counts data using Limma and include the library (row), plate, and donor as technical covariates and compound as the experimental covariate. Here, pseudobulked means we summed the raw counts for all cells of a given type for each well in the experiment. A diagram of this process is shown below:  The output of this model is an estimated fold-change in gene expression and a multiple-testing corrected p-value that a given gene's expression is dependent on the compound experimental variable. There is a long rabbit hole to go down in the world of differential expression testing. We don't have a complete mechanistic model of the data generative process of collecting scRNA data, and many groups disagree on the best way to account for nuisance variables or technical noise. We picked limma because it performs well in our testing. Note, there is an opportunity for privacy leakage from the test set if we release the raw counts data and the differential expression analysis computed on all samples. To protect against this, we fit the differential expression model twice. To generate the training data, we fit the DE model on only the samples from the training set. To generate the private and public test data, we fit the DE model to all samples in the experiment. This keeps the test data private and ensures the test data is the most accurate. Your task is to predict differential expression values for Myeloid and B cells for a majority of compounds. You will train your model on data from all 144 compounds in T cells (CD4+, CD8+, regulatory) and NK cells and a 10% of compounds in the Myeloid and B cells. This mirrors a scientific context where you might want to make predictions into new cell types while taking only 1/10th of the measurements in that cell type. Train: Public Test: Private Test: So the distribution of train / test split across cell types looks like this (download the image for a full size figure): Note that there is no additional test data beyond the indicated cell_type / sm_name pairs. The input to your model will be a tuple of cell_type and sm_name and the output of your model will be predicted signed -log10(p-values) for all 18211 genes. We also provide the raw data for the training split, along with 10x Multiome data for the donors at baseline. This raw data is not necessary to compete for the leaderboard prize. The de_train.parquet file comprises the main competition data. It contains values for a number of cell_type / sm_name pairs. Your goal is to predict corresponding values for the cell_type / sm_name pairs given in id_map.csv. Note: there is no DE data for the DMSO sample, because it is the negative control. All DE output is calculated in reference to the DMSO, i.e. the DE analysis asks \"how confident am I that each gene increased or decreased relative to DMSO due to the compound treatment\"."
    },
    {
        "name": "The Hewlett Foundation: Automated Essay Scoring",
        "url": "https://www.kaggle.com/competitions/asap-aes",
        "overview_text": "Overview text not found",
        "description_text": "The William and Flora Hewlett Foundation (Hewlett) is sponsoring the Automated Student Assessment Prize (ASAP).  Hewlett is appealing to data scientists and machine learning specialists to help solve an important social problem.  We need fast, effective and affordable solutions for automated grading of student-written essays. Hewlett is sponsoring the following prizes: You are provided access to hand scored essays, so that you can build, train and test scoring engines against a wide field of competitors.  Your success depends upon how closely you can deliver scores to those of human expert graders.  While we believe that these financial incentives are important, we also intend to introduce top performers both to leading vendors in the industry and/or an established base of interested buyers.  Hewlett is opening the field of automated student assessment to you.  We want to induce a breakthrough that is both personally satisfying and game-changing for improving public education. Today, state departments of education are developing new forms of testing and grading methods, to assess the new common core standards.  In this environment the need for more sophisticated and affordable options is vital.  For example, we know that essays are an important expression of academic achievement, but they are expensive and time consuming for states to grade them by hand.  So, we are frequently limited to multiple-choice standardized tests.  We believe that automated scoring systems can yield fast, effective and affordable solutions that would allow states to introduce essays and other sophisticated testing tools.  We believe that you can help us pave the way towards a breakthrough.  ASAP is designed to achieve the following goals: The graded essays are selected according to specific data characteristics.  On average, each essay is approximately 150 to 550 words in length.  Some are more dependent upon source materials than others.  This range of essay type is provided so that we can better understand the strengths of your solution.  It is our intent to showcase quality and reliability, based on how well you can match expert human graders for each essay. You will be provided with training data for each essay prompt.  The number of training essays does vary.  For example, the lowest amount of training data is 1,190 essays, randomly selected from a total of 1,982.  The data will contain ASCII formatted text for each essay followed by one or more human scores, and (where necessary) a final resolved human score.  Where it is relevant, you are provided with more than one human score, so that you may evaluate the reliability of the human scorers, but - keep in mind - that you will be predicting to the resolved score.  Also, please note that most essays are scored using a holistic scoring rubric.  However, one data set uses a trait scoring rubric.  The variability is intended to test the limits of your scoring engine\u2019s capabilities. Following a period of 3 months to build and/or train your engine, you will be provided with test data that will contain new essays, randomly selected for blind evaluation.  However, you will notice that the rater and resolved score columns will be blank.  You will be asked to supply, based on your engine's predictions for each essay, your score in the resolved score column and then submit your new data set on this site. As part of the file that you will submit with your predictive scores, you will be asked to submit additional information.  We would like to understand both the time and capital that you\u2019ve spent developing your engine, the profile of your team (or you as an individual if you are working alone) and the projected cost to implement your solution on a larger scale, along with any known limitations.  Basically, you will have the opportunity to present your case for who you are, why your model is commercially viable and to what extent you can use your model to satisfy the interests of potential buyers.  This other information will not be used to determine any prize rewards, and it is optional.  But, if you provide it, it will be used to evaluate whether or not your model should be presented to state departments of education and others who stand to benefit from your work. Also, please note that it is our intention to stage other follow-on ASAP phases in the months ahead.  We are starting with graded essays and will follow with new data: In every instance, we seek to drive innovation for new solutions to automated student assessment.  We hope that you will enjoy this process.  May the best model win!",
        "dataset_text": "Code for evaluation metric and benchmarks For this competition, there are eight essay sets. Each of the sets of essays was generated from a single prompt. Selected essays range from an average length of 150 to 550 words per response. Some of the essays are dependent upon source information and others are not. All responses were written by students ranging in grade levels from Grade 7 to Grade 10. All essays were hand graded and were double-scored. Each of the eight data sets has its own unique characteristics. The variability is intended to test the limits of your scoring engine's capabilities. The training data is provided in three formats: a tab-separated value (TSV) file, a Microsoft Excel 2010 spreadsheet, and a Microsoft Excel 2003 spreadsheet.  The current release of the training data contains essay sets 1-6.  Sets 7-8 will be released on February 10, 2012.  Each of these files contains 28 columns:           Anonymization We have made an effort to remove personally identifying information from the essays using the Named Entity Recognizer (NER) from the Stanford Natural Language Processing group and a variety of other approaches. The relevant entities are identified in the text and then replaced with a string such as \"@PERSON1.\" The entitities identified by NER are: \"PERSON\", \"ORGANIZATION\", \"LOCATION\", \"DATE\", \"TIME\", \"MONEY\", \"PERCENT\" Other replacements made: \"MONTH\" (any month name not tagged as a date by the NER), \"EMAIL\" (anything that looks like an e-mail address), \"NUM\" (word containing digits or non-alphanumeric symbols), and \"CAPS\" (any capitalized word that doesn't begin a sentence, except in essays where more than 20% of the characters are capitalized letters), \"DR\" (any word following \"Dr.\" with or without the period, with any capitalization, that doesn't fall into any of the above), \"CITY\" and \"STATE\" (various cities and states). Here are some hypothetical examples of replacements made:"
    },
    {
        "name": "The Hewlett Foundation: Short Answer Scoring",
        "url": "https://www.kaggle.com/competitions/asap-sas",
        "overview_text": "Overview text not found",
        "description_text": "This competition has completed, congratulations to the preliminary winners and the other participants! The William and Flora Hewlett Foundation (Hewlett Foundation) is sponsoring the Automated Student Assessment Prize (ASAP) in hopes of discovering new tools to support schools and teachers. The competition aspires to solve the problem of the high cost and the slow turnaround of hand scoring thousands of written responses in standardized tests.  As a result many schools exclude written responses in favor of multiple-choice questions, which are less able to assess students\u2019 critical reasoning and writing skills.  ASAP has been designed to help determine whether computerized systems are capable of grading written content accurately for schools and teachers to adopt those solutions.  ASAP aspires to inform key decision makers, who are already considering adopting these systems, by delivering a fair, impartial and open series of trials to test current capabilities and to drive greater awareness when outcomes warrant further consideration. Critical reasoning is one of a suite of skills that experts believe students must be taught to succeed in the new century. The Hewlett Foundation makes grants to educators and nonprofit organizations in support of these skills, which it calls \u201cdeeper learning.\u201d They include the mastery of core academic content, critical reasoning and problem solving, working collaboratively, communicating effectively, and learning how to learn independently. With ASAP, Hewlett is appealing to data scientists to help solve an important problem in the field of educational testing.  Hewlett is sponsoring the following prizes as part of Phase Two: $50,000:  1st place\n$25,000:  2nd place\n$15,000:  3rd place\n$  7,500:  4th place\n$  2,500:  5th place In May of this year, $100,000 in prizes was rewarded for ASAP, Phase One, and we have launched ASAP, Phase Two, with the same intentions.  During Phase One, we focused on systems to support the grading of student written essays. This time, we\u2019re offering a similar competition, only focused on short answer responses.  We welcome you to learn more about our previous phase at www.kaggle.com/c/asap-aes During Phase Two, you are provided access to graded short answer responses and their corresponding prompts, so that you can build, train and test your scoring engines against a wide field of competitors. Your success depends upon how closely you can align your scores to those of human expert graders.  While we believe that a pool of $100,000 in potential financial incentives are important, we also intend to secure and distribute your solutions to the public, in hopes of elevating the field of automated assessment through your contributions.  We want you to induce a breakthrough that is both personally satisfying and game-changing for improving public education. We have already learned that automated assessment systems can yield fast, effective and affordable solutions that would allow states to introduce new testing tools capable of assessing deeper measures of learning.  We believe that you can help us pave the way towards better student assessment.  ASAP is designed to achieve the following goals: The Phase Two graded content is selected according to specific characteristics.  On average, each answer is approximately 50 words in length.  Some are more dependent upon source materials than others, and the answers cover a broad range of disciplines (from English Language Arts to Science).  The range of answer types is provided so that we can better understand the strengths of your solution.  It is our intent to showcase quality and reliability, based on how well you can align with expert human graders for each response. You will be provided with training data for each prompt.  Most training sets will consist of about 1,800 responses that have been randomly selected from a sample of approximately 3,000.  The number of training data may vary.  The data will contain ASCII formatted text for each response followed by two hand scores.  The first score is the final score and the one that you are trying to predict. The second score was used to determine reliability of the first score. The second score did not in any way influence the first (final) score. You are provided with both scores, so that you may evaluate the reliability of the hand scoring.  Further instruction and clarification regarding the data is available on the DATA tab. Following a period of 2.5 months to train your scoring engine, you will be provided with test data that will contain approximately 6,000 new responses (600 per data set), randomly selected for blind evaluation.  However, you will notice that the score columns will be blank.  You will be asked to supply, based on your engine's predictions for each response, your score for each response and to submit your new scored data set on this site. As part of the ZIP file that you will submit with your predictive scores, you will be asked to submit a technical METHODS PAPER. We would like to understand your specific approach to developing your scorig engine, along with any known limitations. Basically, you will have the opportunity to present your scoring engine to the world, so that others may build upon it.  Your technical METHODS PAPER will not be used to determine any prize rewards, but it is a required component of your final submission. Also, please note that it is our intention to continue staging other follow-on ASAP phases in the months ahead.  We have started with graded essays (Phase 1), and we are now focusing on short answers (Phase 2); we are developing plans for a third phase, and we\u2019re planning to launch a phase to demonstrate efficacy of systems capable of offering formative feedback as part of classroom applications: In every instance, we seek to drive innovation for new solutions to student assessment, to support teachers in evaluating critical reasoning skills.  We hope that you will enjoy this process.  May the best model win!",
        "dataset_text": "Code for benchmarks For this competition, there are ten data sets. Each of the data sets was generated from a single prompt. Selected respones have an average length of 50 words per response. Some of the essays are dependent upon source information and others are not. All responses were written by students primarily in Grade 10. All responses were hand graded and were double-scored. Each of the eight data sets has its own unique characteristics. The variability is intended to test the limits of your scoring engine's capabilities. The training data is provided in a tab-separated value (TSV) file containing the following columns: The private leaderboard set will not be released until August 30, 2012. The public leaderboard and private leaderboard files each have the following columns: In addition, a Microsoft Word 2010 Readme file describes each essay set. The Readme file contains the prompt that the essays in the data file were generated from. If applicable, the Readme file also includes the source information for essays that required students to read and respond to an excerpt."
    },
    {
        "name": "GE Hospital Quest",
        "url": "https://www.kaggle.com/competitions/hospital",
        "overview_text": "Overview text not found",
        "description_text": "  Despite every good intention, far too often, frustration and confusion are common reactions to a hospital visit. Many factors must come together to avoid things like long wait times, poor communication, repetitive paperwork, procedure delays, damaged or lost equipment, delayed discharge, and more. It is estimated that there is $100 billion wasted annually in healthcare inefficiencies, distracting facilities from their primary focus \u2010 patient care.\nNow more than ever, we have the ability to improve on the efficiencies within hospitals. Think you\u2019ve got the cure? Your challenge:  Contribute to the design of the ultimate patient experience. While medicine should be left to the professionals, many aspects of hospital operations are ripe for rethinking. In this Quest, focus on operational (non-medical) solutions that can promote an improved health care system experience for patient and family. ",
        "dataset_text": "Dataset description not found"
    },
    {
        "name": "Risky Business",
        "url": "https://www.kaggle.com/competitions/risky-business",
        "overview_text": "Overview text not found",
        "description_text": "Improve credit risk models by predicting the probability of default on a consumer credit product in the next 18 months. More accurate credit risk evaluations allow issuers of credit to be able to responsibly extend and manage credit lines for their customers. The goal of this contest is to make the most accurate ranking of customers' credit risk given key data related to customer behavior.  How well can you detect the early signs of credit distress? This competition is only open to Masters-level participants who meet the eligibility criteria. Visit the Enter the Competition page to view the eligibility criteria and request entrance.",
        "dataset_text": "For information about the data files, please refer to AdditionalCompetitionInformation.pdf."
    },
    {
        "name": "Diabetic Retinopathy Detection",
        "url": "https://www.kaggle.com/competitions/diabetic-retinopathy-detection",
        "overview_text": "Overview text not found",
        "description_text": "Diabetic retinopathy is the leading cause of blindness in the working-age population of the developed world. It is estimated to affect over 93 million people.  The US Center for Disease Control and Prevention estimates that 29.1 million people in the US have diabetes and the World Health Organization estimates that 347 million people have the disease worldwide. Diabetic Retinopathy (DR) is an eye disease associated with long-standing diabetes. Around 40% to 45% of Americans with diabetes have some stage of the disease. Progression to vision impairment can be slowed or averted if DR is detected in time, however this can be difficult as the disease often shows few symptoms until it is too late to provide effective treatment. Currently, detecting DR is a time-consuming and manual process that requires a trained clinician to examine and evaluate digital color fundus photographs of the retina. By the time human readers submit their reviews, often a day or two later, the delayed results lead to lost follow up, miscommunication, and delayed treatment. Clinicians can identify DR by the presence of lesions associated with the vascular abnormalities caused by the disease. While this approach is effective, its resource demands are high. The expertise and equipment required are often lacking in areas where the rate of diabetes in local populations is high and DR detection is most needed. As the number of individuals with diabetes continues to grow, the infrastructure needed to prevent blindness due to DR will become even more insufficient. The need for a comprehensive and automated method of DR screening has long been recognized, and previous efforts have made good progress using image classification, pattern recognition, and machine learning. With color fundus photography as input, the goal of this competition is to push an automated detection system to the limit of what is possible \u2013 ideally resulting in models with realistic clinical potential. The winning models will be open sourced to maximize the impact such a model can have on improving DR detection. This competition is sponsored by the California Healthcare Foundation.  Retinal images were provided by EyePACS, a free platform for retinopathy screening. ",
        "dataset_text": "You are provided with a large set of high-resolution retina images taken under a variety of imaging conditions. A left and right field is provided for every subject. Images are labeled with a subject id as well as either left or right (e.g. 1_left.jpeg is the left eye of patient id 1). A clinician has rated the presence of diabetic retinopathy in each image on a scale of 0 to 4, according to the following scale: 0 - No DR\n1 - Mild\n2 - Moderate\n3 - Severe\n4 - Proliferative DR Your task is to create an automated analysis system capable of assigning a score based on this scale. The images in the dataset come from different models and types of cameras, which can affect the visual appearance of left vs. right. Some images are shown as one would see the retina anatomically (macula on the left, optic nerve on the right for the right eye). Others are shown as one would see through a microscope condensing lens (i.e. inverted, as one sees in a typical live eye exam). There are generally two ways to tell if an image is inverted: Like any real-world data set, you will encounter noise in both the images and labels. Images may contain artifacts, be out of focus, underexposed, or overexposed. A major aim of this competition is to develop robust algorithms that can function in the presence of noise and variation. Due to the extremely large size of this dataset, we have separated the files into multi-part archives. We recommend using 7zip or keka to extract.  Note that the rules do not allow sharing of the data outside of Kaggle, including bittorrent (why not?)."
    },
    {
        "name": "Springleaf Marketing Response",
        "url": "https://www.kaggle.com/competitions/springleaf-marketing-response",
        "overview_text": "Overview text not found",
        "description_text": "Springleaf puts the humanity back into lending by offering their customers personal and auto loans that help them take control of their lives and their finances. Direct mail is one important way Springleaf's team can connect with customers whom may be in need of a loan.  Direct offers provide huge value to customers who need them, and are a fundamental part of Springleaf's marketing strategy. In order to improve their targeted efforts, Springleaf must be sure they are focusing on the customers who are likely to respond and be good candidates for their services. Using a large set of anonymized features, Springleaf is asking you to predict which customers will respond to a direct mail offer. You are challenged to construct new meta-variables and employ feature-selection methods to approach this dauntingly wide dataset.",
        "dataset_text": "See this example R Script that trains an XGBoost model and creates a submission You are provided a high-dimensional dataset of anonymized customer information. Each row corresponds to one customer. The response variable is binary and labeled \"target\". You must predict the target variable for every row in the test set. The features have been anonymized to protect privacy and are comprised of a mix of continuous and categorical features. You will encounter many \"placeholder\" values in the data, which represent cases such as missing values. We have intentionally preserved their encoding to match with internal systems at Springleaf. The meaning of the features, their values, and their types are provided \"as-is\" for this competition; handling a huge number of messy features is part of the challenge here."
    },
    {
        "name": "Ultrasound Nerve Segmentation",
        "url": "https://www.kaggle.com/competitions/ultrasound-nerve-segmentation",
        "overview_text": "Overview text not found",
        "description_text": "Even the bravest patient cringes at the mention of a surgical procedure. Surgery inevitably brings discomfort, and oftentimes involves significant post-surgical pain. Currently, patient pain is frequently managed through the use of narcotics that bring a bevy of unwanted side effects. This competition's sponsor is working to improve pain management through the use of indwelling catheters that block or mitigate pain at the source. Pain management catheters reduce dependence on narcotics and speed up patient recovery. Accurately identifying nerve structures in ultrasound images is a critical step in effectively inserting a patient\u2019s pain management catheter. In this competition, Kagglers are challenged to build a model that can identify nerve structures in a dataset of ultrasound images of the neck. Doing so would improve catheter placement and contribute to a more pain free future. ",
        "dataset_text": "The task in this competition is to segment a collection of nerves called the Brachial Plexus (BP) in ultrasound images. You are provided with a large training set of images where the nerve has been manually annotated by humans. Annotators were trained by experts and instructed to annotate images where they felt confident about the existence of the BP landmark. Please note these important points:"
    },
    {
        "name": "Two Sigma Financial Modeling Challenge",
        "url": "https://www.kaggle.com/competitions/two-sigma-financial-modeling",
        "overview_text": "Overview text not found",
        "description_text": "How can we use the world\u2019s tools and intelligence to forecast economic outcomes that can never be entirely predictable? This question is at the core of countless economic activities around the world \u2013 including at Two Sigma Investments, who has been applying technology and systematic strategies to financial trading since 2001. For over 15 years, Two Sigma has been at the forefront of applying technology and data science to financial forecasts. While their pioneering advances in big data, AI, and machine learning in the financial world have been pushing the industry forward, as with all other scientific progress, they are driven to make continual progress. Through this exclusive partnership, Two Sigma is excited to explore what untapped value Kaggle's diverse data science community can discover in the financial markets. Economic opportunity depends on the ability to deliver singularly accurate forecasts in a world of uncertainty. By accurately predicting financial movements, Kagglers will learn about scientifically-driven approaches to unlocking significant predictive capability. Two Sigma is excited to find predictive value and gain a better understanding of the skills offered by the global data science crowd. Welcome to Kaggle's very first Code Competition! In contrast to our traditional competitions, where competitors submit only prediction outputs, participants in Code Competitions will submit their code via Kaggle Kernels. All kernels are private by default in Code Competitions. You can build your models in Kernels by running them on a training set and, once you're ready to submit your code, your model's performance will be evaluated against the test set and your score and public leaderboard position revealed. As with our traditional competitions, we still maintain a private leaderboard test set, which your code is also evaluated against for final scoring, but is not revealed until the competition closes. Since Code Competitions are brand new, we ask for your patience if you encounter bugs or frustrating platform quirks. Please report any issues you find in the forums and we'll do our best to respond. You do. Even though you are submitting code, the intellectual property exchange here works similarly to a standard prediction competition, whereby prize winners have the option to grant a non-exclusive license in exchange for a prize. There is a new addition to the terms for Code Competitions: Kaggle and the competition host reserve a right to review submissions \"for purposes related to evaluation and scoring in this Competition, including but not limited to the assessment of potential cheating behavior.\" Please refer to the official competition rules for full details. Note: there is no cost of entry for participation.",
        "dataset_text": "This dataset contains anonymized features pertaining to a time-varying value for a financial instrument. Each instrument has an id. Time is represented by the 'timestamp' feature and the variable to predict is 'y'. No further information will be provided on the meaning of the features, the transformations that were applied to them, the timescale, or the type of instruments that are included in the data. Moreover, in accordance with competition rules, participants must not use data other than the data linked from the competition website for the purpose of use in this competition to develop and test their models and submissions. Data is saved and accessed as a .h5 file in the Kernels environment. We have used the .h5 file format instead of the standard .csv format to achieve faster read speeds. The training set file is available for download and offline modeling outside of Kernels. The test set is not available for download. In addition to the data, you will need to familiarize yourself with the Kernels environment and the competition data API. The API is designed to prevent accessing data beyond the timestamp for which you are predicting and informs you which ids require predictions at which timestamps. The API also provides a \"reward\" for each timestamp, in the form of an average R value over the predicted values for the previous day. You may choose to use this reward to do reinforcement-style learning. Your code should expect and handle missing values. We have setup the kernels environment such that the code structure you use for training on the test set (clicking \"Run\") will ideally work for submissions on the test set (clicking \"Submit\"). To achieve this, we have partitioned the training set such that the first half is provided as a training set at the start of a run, and the latter half is streamed through the API, as though it is a holdout set. In other words:"
    },
    {
        "name": "Dstl Satellite Imagery Feature Detection",
        "url": "https://www.kaggle.com/competitions/dstl-satellite-imagery-feature-detection",
        "overview_text": "Overview text not found",
        "description_text": "The proliferation of satellite imagery has given us a radically improved understanding of our planet. It has enabled us to better achieve everything from mobilizing resources during disasters to monitoring effects of global warming. What is often taken for granted is that advancements such as these have relied on labeling features of significance like building footprints and roadways fully by hand or through imperfect semi-automated methods. As these large, complex datasets continue to increase exponentially in number, the Defence Science and Technology Laboratory (Dstl) is seeking novel solutions to alleviate the burden on their image analysts. In this competition, Kagglers are challenged to accurately classify features in overhead imagery. Automating feature labeling will not only help Dstl make smart decisions more quickly around the defense and security of the UK, but also bring innovation to computer vision methodologies applied to satellite imagery.",
        "dataset_text": "In this competition, Dstl provides you with 1km x 1km satellite images in both 3-band and 16-band formats. Your goal is to detect and classify the types of objects found in these regions.  There are two types of imagery spectral content provided in this competition. The 3-band images are the traditional RGB natural color images. The 16-band images contain spectral information by capturing wider wavelength channels. This multi-band imagery is taken from the multispectral (400 \u2013 1040nm) and short-wave infrared (SWIR) (1195-2365nm) range. All images are in GeoTiff format and might require GeoTiff viewers (such as QGIS) to view. Please refer to our tutorial on how to programmatically view the images. All imagery credit to: Satellite Imagery \u00a9 DigitalGlobe, Inc. In a satellite image, you will find lots of different objects like roads, buildings, vehicles, farms, trees, water ways, etc. Dstl has labeled 10 different classes: Every object class is described in the form of Polygons and MultiPolygons, which are simply a list of polygons. We provide two different formats for these shapes: GeoJson and WKT. These are both open source formats for geo-spatial shapes.  Your submission will be in the WKT format.  In this dataset that we provide, we create a set of geo-coordinates that are in the range of x = [0,1] and y = [-1,0]. These coordinates are transformed such that we obscure the location of where the satellite images are taken from. The images are from the same region on Earth. To utilize these images, we provide the grid coordinates of each image so you know how to scale them and align them with the images in pixels. You need the Xmax and Ymin for each image to do the scaling (provided in our grid_sizes.csv) Please refer to our tutorial on how to programmatically view the images."
    },
    {
        "name": "Google Cloud & YouTube-8M Video Understanding Challenge",
        "url": "https://www.kaggle.com/competitions/youtube8m",
        "overview_text": "Overview text not found",
        "description_text": "Video captures a cross-section of our society. And major advances in analyzing and understanding video have the potential to touch all aspects of life from learning and communication to entertainment and play. In this competition, Google is inviting the Kaggle community to join efforts to accelerate research in large-scale video understanding, while giving participants access to the Google Cloud Machine Learning Engine. Today, one of the greatest obstacles to rapid improvements in video understanding research has been the lack of large-scale, labeled datasets open to the public. For example, the availability of large, labeled datasets such as ImageNet has enabled continued breakthroughs in machine learning and machine perception. To that end, Google\u2019s recent release of the YouTube-8M (YT-8M) dataset represents a significant step in this direction. Making this resource open to everyone from students and industry professionals is expected to kickstart innovation in areas such as representation learning and video modeling architectures. In this competition, you are challenged to develop classification algorithms which accurately assign video-level labels using the new and improved YT-8M V2 dataset. The dataset was created from over 7 million YouTube videos (450,000 hours of video) and includes video labels from a vocabulary of 4716 classes (3.4 labels/video on average). It also comes with pre-extracted audio & visual features from every second of video (3.2B feature vectors in total). By taking part, Kagglers will not only play a pivotal role in setting state-of-the-art benchmarks, but also improve search and organization of video archives.  Because Cloud ML is currently a beta product, Google welcomes the opportunity to hear your feedback about using the tool. Please share your questions and thoughts on the competition's forums. Additional resources specific to the YT-8M dataset and Google Cloud ML can be found here. Google Cloud Machine Learning, Competition Sponsor Google Cloud Machine Learning is a managed service that enables you to easily build machine learning models, that work on any type of data, of any size. Create your model with the powerful TensorFlow framework that powers many Google products, from GooglePhotos to Google Cloud Speech. Build models of any size with our managed scalable infrastructure. Your trained model is immediately available for use with our global prediction platform that can support thousands of users and TBs of data. The service is integrated with Google Cloud Dataflow for pre-processing, allowing you to access data from Google Cloud Storage, Google BigQuery, and others.",
        "dataset_text": "In this competition, you will predict the labels of a YouTube video. We provide you extracted frame-level and video-level features. The feature data and detailed feature information can be found on the YouTube-8M dataset webpage.  The training dataset in this competition contain videos and labels that are publicly available on YouTube, while the test data is not publicly available. The test data also has anonymized video IDs to ensure the fairness of the competition.  We have a step-by-step tutorial to get you started with Google Cloud. "
    },
    {
        "name": "Intel & MobileODT Cervical Cancer Screening",
        "url": "https://www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening",
        "overview_text": "Overview text not found",
        "description_text": "Cervical cancer is so easy to prevent if caught in its pre-cancerous stage that every woman should have access to effective, life-saving treatment no matter where they live. Today, women worldwide in low-resource settings are benefiting from programs where cancer is identified and treated in a single visit. However, due in part to lacking expertise in the field, one of the greatest challenges of these cervical cancer screen and treat programs is determining the appropriate method of treatment which can vary depending on patients\u2019 physiological differences.  Especially in rural parts of the world, many women at high risk for cervical cancer are receiving treatment that will not work for them due to the position of their cervix. This is a tragedy: health providers are able to identify high risk patients, but may not have the skills to reliably discern which treatment which will prevent cancer in these women. Even worse, applying the wrong treatment has a high cost. A treatment which works effectively for one woman may obscure future cancerous growth in another woman, greatly increasing health risks. Currently, MobileODT offers a Quality Assurance workflow to support remote supervision which helps healthcare providers make better treatment decisions in rural settings. However, their workflow would be greatly improved given the ability to make real-time determinations about patients\u2019 treatment eligibility based on cervix type. In this competition, Intel is partnering with MobileODT to challenge Kagglers to develop an algorithm which accurately identifies a woman\u2019s cervix type based on images. Doing so will prevent ineffectual treatments and allow healthcare providers to give proper referral for cases that require more advanced treatment.  MobileODT has developed and sells the Enhanced Visual Assessment (EVA) System, a digital toolkit for health care workers of every level to provide expert services to patients, anchored at the point-of-care by an FDA-approved, intelligent, mobile-phone based medical device. Combining the algorithmic power of biomedical optics with the computational capabilities and connectivity of mobile phones, MobileODT's connected, intelligent medical systems can be used everywhere, under nearly any conditions. MobileODT's first product, the FDA approved EVA System for colposcopy, is in use by health providers in 31 hospital systems across the US, and in 22 countries, to better screen and treat women for cervical cancer and to conduct forensic colposcopy.",
        "dataset_text": "Warning: This data contains graphic contents that some may find disturbing. In this competition, you will develop algorithms to correctly classify cervix types based on cervical images. These different types of cervix in our data set are all considered normal (not cancerous), but since the transformation zones aren't always visible, some of the patients require further testing while some don't. This decision is very important for the healthcare provider and critical for the patient. Identifying the transformation zones is not an easy task for the healthcare providers, therefore, an algorithm-aided decision will significantly improve the quality and efficiency of cervical cancer screening for these patients.  To understand more about the background of how these cervix types are defined, please refer to this document.    UPDATED 5/22: We revisited these images and found that we need to update 3 labels:"
    },
    {
        "name": "Mercari Price Suggestion Challenge",
        "url": "https://www.kaggle.com/competitions/mercari-price-suggestion-challenge",
        "overview_text": "Overview text not found",
        "description_text": "It can be hard to know how much something\u2019s really worth. Small details can mean big differences in pricing. For example, one of these sweaters cost $335 and the other cost $9.99. Can you guess which one\u2019s which?  Product pricing gets even harder at scale, considering just how many products are sold online. Clothing has strong seasonal pricing trends and is heavily influenced by brand names, while electronics have fluctuating prices based on product specs. Mercari, Japan\u2019s biggest community-powered shopping app, knows this problem deeply. They\u2019d like to offer pricing suggestions to sellers, but this is tough because their sellers are enabled to put just about anything, or any bundle of things, on Mercari's marketplace. In this competition, Mercari\u2019s challenging you to build an algorithm that automatically suggests the right product prices. You\u2019ll be provided user-inputted text descriptions of their products, including details like product category name, brand name, and item condition. Note that, because of the public nature of this data, this competition is a \u201cKernels Only\u201d competition. In the second stage of the challenge, files will only be available through Kernels and you will not be able to modify your approach in response to new data. Read more details in the data tab and Kernels FAQ page.",
        "dataset_text": "In this competition, you will predict the sale price of a listing based on information a user provides for this listing. This is a Kernels-only competition, the files in this Data section are downloadable just for your reference in Stage 1. Stage 2 files will only be available in Kernels and not available for download here. The files consist of a list of product listings. These files are tab-delimited. Please note that in stage 1, all the test data will be calculated on the public leaderboard. In stage 2, we will swap the test.tsv file to the complete test dataset that includes the private leaderboard data. A sample submission file in the correct format. In the second stage of the competition, we will re-run your selected Kernels. The following files will be swapped with new data: EDIT: we uploaded stage 2 files after the competition ended, 2/21/2018. They are named sample_submission_stg2.csv and test_stg2.tsv."
    },
    {
        "name": "2018 Data Science Bowl",
        "url": "https://www.kaggle.com/competitions/data-science-bowl-2018",
        "overview_text": "Overview text not found",
        "description_text": "Imagine speeding up research for almost every disease, from lung cancer and heart disease to rare disorders. The 2018 Data Science Bowl offers our most ambitious mission yet: create an algorithm to automate nucleus detection. We\u2019ve all seen people suffer from diseases like cancer, heart disease, chronic obstructive pulmonary disease, Alzheimer\u2019s, and diabetes. Many have seen their loved ones pass away. Think how many lives would be transformed if cures came faster. By automating nucleus detection, you could help unlock cures faster\u2014from rare disorders to the common cold. Want a snapshot about the 2018 Data Science Bowl? View this video. Identifying the cells\u2019 nuclei is the starting point for most analyses because most of the human body\u2019s 30 trillion cells contain a nucleus full of DNA, the genetic code that programs each cell. Identifying nuclei allows researchers to identify each individual cell in a sample, and by measuring how cells react to various treatments, the researcher can understand the underlying biological processes at work. By participating, teams will work to automate the process of identifying nuclei, which will allow for more efficient drug testing, shortening the 10 years it takes for each new drug to come to market. Check out this video overview to find out more. Teams will create a computer model that can identify a range of nuclei across varied conditions. By observing patterns, asking questions, and building a model, participants will have a chance to push state-of-the-art technology farther. Visit DataScienceBowl.com to:\n\u2022 Sign up to receive news about the competition\n\u2022 Learn about the history of the Data Science Bowl and past competitions\n\u2022 Read our latest insights on emerging analytics techniques ",
        "dataset_text": "This dataset contains a large number of segmented nuclei images. The images were acquired under a variety of conditions and vary in the cell type, magnification, and imaging modality (brightfield vs. fluorescence). The dataset is designed to challenge an algorithm's ability to generalize across these variations. Each image is represented by an associated ImageId. Files belonging to an image are contained in a folder with this ImageId. Within this folder are two subfolders: The second stage dataset will contain images from unseen experimental conditions. To deter hand labeling, it will also contain images that are ignored in scoring. The metric used to score this competition requires that your submissions are in run-length encoded format. Please see the evaluation page for details. As with any human-annotated dataset, you may find various forms of errors in the data. You may manually correct errors you find in the training set. The dataset will not be updated/re-released unless it is determined that there are a large number of systematic errors. The masks of the stage 1 test set will be released with the release of the stage 2 test set."
    },
    {
        "name": "Riiid Answer Correctness Prediction",
        "url": "https://www.kaggle.com/competitions/riiid-test-answer-prediction",
        "overview_text": "Overview text not found",
        "description_text": "  Think back to your favorite teacher. They motivated and inspired you to learn. And they knew your strengths and weaknesses. The lessons they taught were based on your ability. For example, teachers would make sure you understood algebra before advancing to calculus. Yet, many students don\u2019t have access to personalized learning. In a world full of information, data scientists like you can help. Machine learning can offer a path to success for young people around the world, and you are invited to be part of this mission.  In 2018, 260 million children weren't attending school. At the same time, more than half of these young students didn't meet minimum reading and math standards. Education was already in a tough place when COVID-19 forced most countries to temporarily close schools. This further delayed learning opportunities and intellectual development. The equity gaps in every country could grow wider. We need to re-think the current education system in terms of attendance, engagement, and individualized attention. Riiid Labs, an AI solutions provider delivering creative disruption to the education market, empowers global education players to rethink traditional ways of learning leveraging AI. With a strong belief in equal opportunity in education, Riiid launched an AI tutor based on deep-learning algorithms in 2017 that attracted more than one million South Korean students. This year, the company released EdNet, the world\u2019s largest open database for AI education containing more than 100 million student interactions. In this competition, your challenge is to create algorithms for \"Knowledge Tracing,\" the modeling of student knowledge over time. The goal is to accurately predict how students will perform on future interactions. You will pair your machine learning skills using Riiid\u2019s EdNet data. Your innovative algorithms will help tackle global challenges in education. If successful, it\u2019s possible that any student with an Internet connection can enjoy the benefits of a personalized learning experience, regardless of where they live. With your participation, we can build a better and more equitable model for education in a post-COVID-19 world. Paul Kim, Stanford Graduate School of Education\nNeil Heffernan, WPI & ASSISTments ",
        "dataset_text": "Tailoring education to a student's ability level is one of the many valuable things an AI tutor can do. Your challenge in this competition is a version of that overall task; you will predict whether students are able to answer their next questions correctly. You'll be provided with the same sorts of information a complete education app would have: that student's historic performance, the performance of other students on the same question, metadata about the question itself, and more. This is a time-series code competition, you will receive test set data and make predictions with Kaggle's time-series API. Please be sure to review the Time-series API Details section closely. train.csv questions.csv: metadata for the questions posed to users. lectures.csv: metadata for the lectures watched by users as they progress in their education. example_test_rows.csv Three sample groups of the test set data as it will be delivered by the time-series API. The format is largely the same as train.csv. There are two different columns that mirror what information the AI tutor actually has available at any given time, but with the user interactions grouped together for the sake of API performance rather than strictly showing information for a single user at a time. Some users will appear in the hidden test set that have NOT been presented in the train set, emulating the challenge of quickly adapting to modeling new arrivals to a website."
    },
    {
        "name": "Jane Street Market Prediction",
        "url": "https://www.kaggle.com/competitions/jane-street-market-prediction",
        "overview_text": "Overview text not found",
        "description_text": "\u201cBuy low, sell high.\u201d It sounds so easy\u2026. In reality, trading for profit has always been a difficult problem to solve, even more so in today\u2019s fast-moving and complex financial markets. Electronic trading allows for thousands of transactions to occur within a fraction of a second, resulting in nearly unlimited opportunities to potentially find and take advantage of price differences in real time. In a perfectly efficient market, buyers and sellers would have all the agency and information needed to make rational trading decisions. As a result, products would always remain at their \u201cfair values\u201d and never be undervalued or overpriced. However, financial markets are not perfectly efficient in the real world. Developing trading strategies to identify and take advantage of inefficiencies is challenging. Even if a strategy is profitable now, it may not be in the future, and market volatility makes it impossible to predict the profitability of any given trade with certainty. As a result, it can be hard to distinguish good luck from having made a good trading decision. In the first three months of this challenge, you will build your own quantitative trading model to maximize returns using market data from a major global stock exchange. Next, you\u2019ll test the predictiveness of your models against future market returns and receive feedback on the leaderboard. Your challenge will be to use the historical data, mathematical tools, and technological tools at your disposal to create a model that gets as close to certainty as possible. You will be presented with a number of potential trading opportunities, which your model must choose whether to accept or reject. In general, if one is able to generate a highly predictive model which selects the right trades to execute, they would also be playing an important role in sending the market signals that push prices closer to \u201cfair\u201d values. That is, a better model will mean the market will be more efficient going forward. However, developing good models will be challenging for many reasons, including a very low signal-to-noise ratio, potential redundancy, strong feature correlation, and difficulty of coming up with a proper mathematical formulation. Jane Street has spent decades developing their own trading models and machine learning solutions to identify profitable opportunities and quickly decide whether to execute trades. These models help Jane Street trade thousands of financial products each day across 200 trading venues around the world. Admittedly, this challenge far oversimplifies the depth of the quantitative problems Jane Streeters work on daily, and Jane Street is happy with the performance of its existing trading model for this particular question. However, there\u2019s nothing like a good puzzle, and this challenge will hopefully serve as a fun introduction to a type of data science problem that a Jane Streeter might tackle on a daily basis. Jane Street looks forward to seeing the new and creative approaches the Kaggle community will take to solve this trading challenge.",
        "dataset_text": "Note 2021-10-08: the dataset for this competition is no longer available. This dataset contains an anonymized set of features, feature_{0...129}, representing real stock market data. Each row in the dataset represents a trading opportunity, for which you will be predicting an action value: 1 to make the trade and 0 to pass on it. Each trade has an associated weight and resp, which together represents a return on the trade. The date column is an integer which represents the day of the trade, while ts_id represents a time ordering. In addition to anonymized feature values, you are provided with metadata about the features in features.csv. In the training set, train.csv, you are provided a resp value, as well as several other resp_{1,2,3,4} values that represent returns over different time horizons. These variables are not included in the test set. Trades with weight = 0 were intentionally included in the dataset for completeness, although such trades will not contribute towards the scoring evaluation. This is a code competition that relies on a time-series API to ensure models do not peek forward in time. To use the API, follow the instructions on the Evaluation page. When you submit your notebook, it will be rerun on an unseen test: Note that during the second (forecasting) phase of the competition, the notebook time limits will scale with the number of trades presented in the test set. Refer to the Code Requirements for details."
    },
    {
        "name": "Parkinson's Freezing of Gait Prediction",
        "url": "https://www.kaggle.com/competitions/tlvmc-parkinsons-freezing-gait-prediction",
        "overview_text": "Overview text not found",
        "description_text": "The goal of this competition is to detect freezing of gait (FOG), a debilitating symptom that afflicts many people with Parkinson\u2019s disease. You will develop a machine learning model trained on data collected from a wearable 3D lower back sensor. Your work will help researchers better understand when and why FOG episodes occur. This will improve the ability of medical professionals to optimally evaluate, monitor, and ultimately, prevent FOG events. An estimated 7 to 10 million people around the world have Parkinson\u2019s disease, many of whom suffer from freezing of gait (FOG). During a FOG episode, a patient's feet are \u201cglued\u201d to the ground, preventing them from moving forward despite their attempts. FOG has a profound negative impact on health-related quality of life\u2014people who suffer from FOG are often depressed, have an increased risk of falling, are likelier to be confined to wheelchair use, and have restricted independence. While researchers have multiple theories to explain when, why, and in whom FOG occurs, there is still no clear understanding of its causes. The ability to objectively and accurately quantify FOG is one of the keys to advancing its understanding and treatment. Collection and analysis of FOG events, such as with your data science skills, could lead to potential treatments. There are many methods of evaluating FOG, though most involve FOG-provoking protocols. People with FOG are filmed while performing certain tasks that are likely to increase its occurrence. Experts then review the video to score each frame, indicating when FOG occurred. While scoring in this manner is relatively reliable and sensitive, it is extremely time-consuming and requires specific expertise. Another method involves augmenting FOG-provoking testing with wearable devices. With more sensors, the detection of FOG becomes easier, however, compliance and usability may be reduced. Therefore, a combination of these two methods may be the best approach. When combined with machine learning methods, the accuracy of detecting FOG from a lower back accelerometer is relatively high. However, the datasets used to train and test these algorithms have been relatively small and generalizability is limited to date. Furthermore, the emphasis has been on achieving high levels of accuracy, while precision, for example, has largely been ignored. Competition host, the Center for the Study of Movement, Cognition, and Mobility (CMCM), Neurological Institute, Tel Aviv Sourasky Medical Center, aims to improve the personalized treatment of age-related movement, cognition, and mobility disorders and to alleviate the associated burden. They leverage a combination of clinical, engineering, and neuroscience expertise to: 1) Gain new understandings into the physiologic and pathophysiologic mechanisms that contribute to cognitive and motor function, the factors that influence these functions, and their changes with aging and disease (e.g., Parkinson\u2019s disease, Alzheimer\u2019s). 2) Develop new methods and tools for the early detection and tracking of cognitive and motor decline. A major focus is on using leveraging wearable devices and digital technologies; and 3) Develop and evaluate novel methods for the prevention and treatment of gait, falls, and cognitive function. Your work will help advance the evaluation, understanding and treatment of FOG, improving the lives of the many people who suffer from this debilitating Parkinson\u2019s disease symptom. The competition data was collected by three research groups: The Center for the Study of Movement, Cognition and Mobility, as indicated above, The Neurorehabilitation Research Group at Katholieke Universiteit Leuven in Belgium, and the Mobility and Falls Translational Research Center at the Hinda and Arthur Marcus Institute for Aging, affiliated with Harvard Medical School in Boston. The Michael J. Fox Foundation for Parkinson\u2019s Research generously supported the data collection and this data competition.  ",
        "dataset_text": "This competition dataset comprises lower-back 3D accelerometer data from subjects exhibiting freezing of gait episodes, a disabling symptom that is common among people with Parkinson's disease. Freezing of gait (FOG) negatively impacts walking abilities and impinges locomotion and independence. Your objective is to detect the start and stop of each freezing episode and the occurrence in these series of three types of freezing of gait events: Start Hesitation, Turn, and Walking. The data series include three datasets, collected under distinct circumstances: Trials from the tdcsfog and defog datasets were videotaped and annotated by expert reviewers documented the freezing of gait episodes. That is, the start, end and type of each episode were marked by the experts. Series in the daily dataset are unannotated. You will be detecting FOG episodes for the tdcsfog and defog series. You may wish to apply unsupervised or semi-supervised methods to the series in the daily dataset to support your detection modelling. See this page for more on these datasets as well as video examples of freezing of gait events: Additional Data Documentation. Note that the Valid and Task fields are only present in the defog dataset. They are not relevant for the tdcsfog data. Please note that this is a Code Competition, in which the actual test set is hidden. In this public version, we give some sample data drawn from the training set to help you author your solutions. When your submission is scored, this example test data will be replaced with the full test set. The test set contains about 250 data series. The series from the tdcsfog and defog are in a proportion similar to that of the training set. These series have subjects that are entirely distinct from those in the training set. This also holds true for the public / private test split. (In other words, the train / public test / private test splits were formed by grouping on subjects and stratifying on datasets.) Most, but not all, subjects in the defog test set have unannotated series in the daily dataset. All of the series in the daily dataset are in the public version of the data. There are no additional unannotated series in the hidden dataset. The tdcsfog_metadata.csv, defog_metadata.csv, and subjects.csv files in the hidden version of the data contain additional entries for the test set series. The hidden versions of these files are supersets of the public versions. The events.csv, tasks.csv, and daily_metadata.csv files are the same in both the public and hidden datasets."
    },
    {
        "name": "Google - Isolated Sign Language Recognition",
        "url": "https://www.kaggle.com/competitions/asl-signs",
        "overview_text": "Overview text not found",
        "description_text": "The goal of this competition is to classify isolated American Sign Language (ASL) signs. You will create a TensorFlow Lite model trained on labeled landmark data extracted using the MediaPipe Holistic Solution. Your work may improve the ability of PopSign* to help relatives of deaf children learn basic signs and communicate better with their loved ones.^ (opens in a new tab)\"> Around 90% of which are born to hearing parents many of which may not know American Sign Language. (kdhe.ks.gov, deafchildren.org) Without sign language, deaf babies are at risk of Language Deprivation Syndrome. This syndrome is characterized by a lack of access to naturally occurring language acquisition during their critical language-learning years. It can cause serious impacts on different aspects of their lives, such as relationships, education, and employment. Learning American Sign Language is as difficult for English speakers as learning Japanese. (jstor.org) It takes time and resources, which many parents don't have. They want to learn sign language, but it's hard when they are working long hours just to make ends meet. And even if they find the time and money for classes, the classes are often far away. PopSign is a smartphone game app that makes learning American Sign Language fun, interactive, and accessible. Players match videos of ASL signs with bubbles containing written English words to pop them.\nPopSign is designed to help parents with deaf children learn ASL, but it's open to anyone who wants to learn sign language vocabulary. By adding a sign language recognizer from this competition, PopSign players will be able to sign the type of bubble they want to shoot, providing the player with the opportunity to practice the sign themselves instead of just watching videos of other people signing. By training a sign language recognizer for PopSign, you can help make the game more interactive and improve the learning and confidence of players who want to learn sign language to communicate with their loved ones. To allow the ML model to run on device in an attempt to limit latency inside the game, PopSign doesn\u2019t send user videos to the cloud. Therefore, all inference must be done on the phone itself. PopSign is building its recognition pipeline on top of TensorFlow Lite, which runs on both Android and iOS. In order for the competition models to integrate seamlessly with PopSign, we are asking our competitors to submit their entries in the form of TensorFlow Lite models. We\u2019d like to thank the Georgia Institute of Technology, the National Technical Institute for the Deaf at Rochester Institute of Technology, and Deaf Professional Arts Network for their work to create the dataset, the PopSign game, and overall competition preparation. *PopSign is an app developed by the Georgia Institute of Technology and the National Technical Institute for the Deaf at Rochester Institute of Technology. The app is available in beta on Android and iOS.\n^We cannot guarantee the competition will benefit the competitors or the disabled community directly.",
        "dataset_text": "Deaf children are often born to hearing parents who do not know sign language. Your challenge in this competition is to help identify signs made in processed videos, which will support the development of mobile apps to help teach parents sign language so they can communicate with their Deaf children. This competition requires submissions to be made in the form of TensorFlow Lite models. You are welcome to train your model using the framework of your choice as long as you convert the model checkpoint into the tflite format prior to submission. Please see the evaluation page for details. Update: the rerun dataset has been published here. train_landmark_files/[participant_id]/[sequence_id].parquet The landmark data. The landmarks were extracted from raw videos with the MediaPipe holistic model. Not all of the frames necessarily had visible hands or hands that could be detected by the model. Landmark data should not be used to identify or re-identify an individual. Landmark data is not intended to enable any form of identity recognition or store any unique biometric identification. train.csv"
    },
    {
        "name": "Stanford Ribonanza RNA Folding",
        "url": "https://www.kaggle.com/competitions/stanford-ribonanza-rna-folding",
        "overview_text": "Without being able to understand how RNA molecules fold, we are missing a deeper understanding of how nature works, how life began, and how we can design better medicines and biological approaches to grand problems like climate change.",
        "description_text": "Ribonucleic acid (RNA) is essential for most biological functions. A better understanding of how to manipulate RNA could help usher in an age of programmable medicine, including first cures for pancreatic cancer and Alzheimer\u2019s disease as well as much-needed antibiotics and new biotechnology approaches for climate change. But first, researchers must better understand each RNA molecule's structure, an ideal problem for data science. Recent efforts to predict RNA structure have run into a number of challenges: (1) a paucity of training data, (2) lack of intellectual and computational power, and (3) difficulties in rigorously splitting training and test data. Can a Kaggle competition close these gaps? Towards sourcing a large and diverse collection of RNA molecules for data acquisition, the host Das laboratory brings together scientists and gamers to solve puzzles and invent medicine in the Eterna project. The Eterna community has previously unlocked new scientific principles, designed thermodynamically-optimized riboswitches, and revealed RNA degradation patterns for improving the shelf life of mRNA vaccines, which formed the basis of the Kaggle OpenVaccine challenge. Now the Eterna project is creating diverse sequences that are predicted to form complex structures. The data for this new Ribonanza competition are experimental measurements of the chemical reactivity at each position of an RNA molecule. These data are exquisitely sensitive to the structure \u2013 or multiple structures \u2013 that each RNA forms in the test tube. An algorithm that could perfectly predict these chemical reactivities would need to have an implicit \u2018understanding\u2019 of RNA structure to do so. Such an oracle could be then utilized to predictively model structures of novel RNA molecules. As in the OpenVaccine competition, the majority of the private leaderboard data will be collected in parallel with your prediction efforts -- no one will know the answers until the competition closes! To reiterate the potential impact of your participation, an accurate model that solves the RNA structure prediction problem could be a game changer for medical researchers who are trying to identify unique RNA-based drug targets in the many bacterial, viral, neurological, and cancer genes that remain undruggable at the protein level. In addition, accurate RNA structure prediction is needed to predictively design RNA-based medicines such as mRNA vaccines and CRISPR gene therapeutics that promise to treat nearly all human disease. More than its medical implications, RNA molecules underlie and can even dominate core biological processes for all of life, including the very first forms of life on Earth and the plants and marine organisms that fix most of the carbon on our planet now. A full understanding of life requires a full, predictive understanding of RNA. In addition to being made available as open source code for the entire science and medicine communities, winners\u2019 innovations will be featured in a keynote talk by Rhiju Das at the flagship conference in machine learning in structure biology at NeurIPS in December 2023.          ",
        "dataset_text": "In this competition, you will be predicting the reactivity of an RNA sequence to two chemical modifiers DMS and 2A3. These data can be measured efficiently through a mutational profiling (MaP) experiment read out by high-throughput sequencing and positions that are protected from chemical modification are likely to be forming base pairs or other kinds of RNA structure. Above edited on Sep. 24, 2023 to group column explanations based on column presence in each of the three files. At the beginning of the competition, Stanford scientists have data on 1,118,513 RNA sequences of lengths ranging from 115 to 206. We have split out 311,935 of these 1,118,513 sequences for a public test set to allow for continuous evaluation through the competition, on the Public Leaderboard. This set has been additionally filtered to ensure high signal-to-noise and read coverage (see note on SN_filter above). The remaining 806,578 sequences for which we have data are in train_data.csv. We note that 37,828 of the test sequences, derived from the RFAM database, are identical or near-identical to the train set; for these cases the leaderboard test set contains higher signal-to-noise measurements than the train_data and serve as a test of model ability to 'denoise' chemical mapping data. Our final and most important scoring (the Private Leaderboard) involves 1,031,888 sequences. Within this set, the majority (1,008,000 RNAs) will be experimentally synthesized and profiled after the Kaggle competition begins, to help ensure rigor. Furthermore, these test sequences will have lengths ranging from 207 to 457 bases -- we intentionally chose a different length distribution compared to the train data and Public Leaderboard, to help test the generality of your models. As those profiles are collected, they will again be filtered for acceptable signal to noise before computing your final Private Leaderboard scores."
    },
    {
        "name": "LMSYS - Chatbot Arena Human Preference Predictions",
        "url": "https://www.kaggle.com/competitions/lmsys-chatbot-arena",
        "overview_text": "This competition challenges you to predict which responses users will prefer in a head-to-head battle between chatbots powered by large language models (LLMs). You'll be given a dataset of conversations from the Chatbot Arena, where different LLMs generate answers to user prompts. By developing a winning machine learning model, you'll help improve how chatbots interact with humans and ensure they better align with human preferences.",
        "description_text": "Large language models (LLMs) are rapidly entering our lives, but ensuring their responses resonate with users is critical for successful interaction. This competition presents a unique opportunity to tackle this challenge with real-world data and help us bridge the gap between LLM capability and human preference. We utilized a large dataset collected from Chatbot Arena, where users chat with two anonymous LLMs and choose the answer they prefer. Your task in this competition is to predict which response a user will prefer in these head-to-head battles. This challenge aligns with the concept of \"reward models\" or \"preference models\" in reinforcement learning from human feedback (RLHF). Previous research has identified limitations in directly prompting an existing LLM for preference predictions. These limitations often stem from biases such as favoring responses presented first (position bias), being overly verbose (verbosity bias), or exhibiting self-promotion (self-enhancement bias). We encourage you to explore various machine-learning techniques to build a model that can effectively predict user preferences. Your work will be instrumental in developing LLMs that can tailor responses to individual user preferences, ultimately leading to more user-friendly and widely accepted AI-powered conversation systems.",
        "dataset_text": "The competition dataset consists of user interactions from the ChatBot Arena. In each user interaction a judge provides one or more prompts to two different large language models, and then indicates which of the models gave the more satisfactory response. The goal of the competition is to predict the preferences of the judges and determine the likelihood that a given prompt/response pair is selected as the winner. Please note that this is a Code Competition. When your submission is scored, this example test data will be replaced with the full test set. There are 55K rows in the training data, and you can expect roughly 25,000 rows in the test set. train.csv test.csv sample_submission.csv A submission file in the correct format. Note: the dataset for this competition contains text that may be considered profane, vulgar, or offensive."
    },
    {
        "name": "MasterCard - Data Cleansing Competition",
        "url": "https://www.kaggle.com/competitions/mastercard-data-cleansing-competition-finals",
        "overview_text": "Overview text not found",
        "description_text": "This is a private, invitation-only competition. The relevant information is provided only to contestants. The competition is closed to new entrants. Qualification for future private competitions is based solely on objective leaderboard performance in competitions.",
        "dataset_text": "Dataset description not found"
    },
    {
        "name": "Cervical Cancer Screening",
        "url": "https://www.kaggle.com/competitions/cervical-cancer-screening",
        "overview_text": "Overview text not found",
        "description_text": "This is a Masters competition. You must be a Kaggle Master to participate. Cervical cancer is the third most common cancer in women worldwide, affecting over 500,000 women and resulting in approximately 275,000 deaths every year. After reading these statistics, you may be surprised to he\nar that cervical cancer is potentially preventable and curable. Cervical cancer can be prevented through early administration of the HPV vaccine and regular pap smear screenings, which indicate the presence of precancerous cells. It is also sometimes curable by the removal of the early-stage cancerous tissue that is identified through pap smears. Screening and early treatment can lead to potential cures in about 95% of women at risk for cervical cancer. Most women in the US have access to cervical cancer screening, yet 4,000 women die every year from cervical cancer in the US and it is estimated that 30% of US women do not receive regular pap screenings. We know little about who these women are and why they are not getting screened. Prior research suggests that lower screening rates are associated with low income, low education, lack of interaction with the healthcare system, and lack of health insurance. But research also shows that even in women with access to healthcare fail to get this preventive test, indicating that barriers like lack of education and not being comfortable with the procedure are influencing their behavior (Patient Survey).  There are many patient advocacy programs on the importance of pap smears in cervical cancer prevention. However, these widespread programs may not be reaching or effectively speaking to the most vulnerable populations. If one could better identify these women, education campaigns could target them with content that speaks directly to their unique risk factors. Identifying predictors of not receiving pap smears will provide important information to stakeholders in cervical cancer prevention who run awareness programs. With this Masters competition, Genentech is asking you to join their mission to help prevent cervical cancer. Given a dataset of de-identified health records, your challenge is to predict which women will not be screened for cervical cancer on the recommended schedule. Identifying at-risk populations will make education and other intervention efforts more effective, ideally ultimately reducing the number of women who die from this disease. Founded more than 35 years ago, Genentech is a leading biotechnology company that discovers, develops, manufactures and commercializes medicines to treat patients with serious or life-threatening medical conditions. The company, a member of the Roche Group, has headquarters in South San Francisco, California. For additional information about the company, please visit http://www.gene.com. The dataset for this competition is provided by Symphony Health Solutions.   @Jotform.Show(35)",
        "dataset_text": "In this competition, you are to predict whether a patient receives regular cervical cancer screening (pap smear), given the medical records of over 3 million women in the United States. A patient is defined as a screener if the patient has had a pap smear in the last 5 years. This patient must be between the age of 25 and 65, and has not been diagnosed of cervical cancer or had hysterectomies.  Please note that because of the data source, this data provided in the competition may have some sampling bias, therefore may not reflect the prevalence of cervical cancer screening of the general population. This competition has 11 relational data tables. Relationships between the datasets are captured in the following schema:         a sample submission file in the correct format.  The claim ID is a way of identifying the information received on a claim form (CMS 1450 and CMS1500) submitted by a physician. These claim forms can contain multiple services, so the claim ID ties them all back together. A claim may have multiple diagnoses associated with it. A unique record in the diagnosis table would be a patient_id/Claim_id/Srvc_date/diagnosis code. On the procedure and surgical tables, claims may have multiple lines. A unique record is a patient_id/Claim_id/srvc_line/procedure code. Within the data, that is a key field used to appropriately link the diagnosis to the procedure, but the contestants SHOULD NOT try to link the Rx (prescription) claim ID to the diagnosis or procedure claim ID. The Rx claim ID is an entirely different claim submission from the pharmacy, so it has a separate claim ID. The above data dictionary can be downloaded here. "
    },
    {
        "name": "Western Australia Rental Prices",
        "url": "https://www.kaggle.com/competitions/deloitte-western-australia-rental-prices",
        "overview_text": "Overview text not found",
        "description_text": "This is a Masters competition. You must be a Kaggle Master to participate. Property rental prices are a key economic indicator, often signaling significant changes in things like unemployment rate or income. Accurately predicting rental prices would help organizations offering public and commercial services with the ability to better plan for and price these services. Weekly rental values for properties vary due to a broad mix of factors. Some measures are objective, like proximity to hospitals, schools, transport, and coastline. Others are more subjective, like the aesthetic value of your backyard garden. The rental market in Western Australia is unusually diverse and difficult to predict due to the region's varied landscape and small, widely spread population. Currently, automated valuation models are used for over 90% of residential property estimates in Western Australia. Using data on location, property, zoning, past sales, and more, the goal of this competition is to improve on existing models by accurately estimating the weekly market rental value for residential properties across Western Australia. @JotForm.Show(34)",
        "dataset_text": "Note: because of the manual enrollment process, we are leaving approximately one day to review paperwork before the data is posted and submissions open. Enrollment in the competition will remain open until the first submission deadline. In Western Australia, property taxes are determined based on rental rates, while rental rates are typically assessed by manual inspection. The sparse population, spread out distances, and lack of comparable pricing information makes this assessment process time-consuming and difficult. The objective of this competition is to create an algorithmic assessment of a rental based on the property and its geographic information. To accomplish this, you are provided a large relational database with data on rental properties and the land on which they reside, covering a time period from 1990 to 2015. The target variable you are predicting is REN_BASE_RENT. The test set has been picked based on stratified sampling by the post code. This was done to increase representation of rural areas, where rental prices may be harder to predict. This is real data! Real data is messy. Expect to encounter and do your best to work around imperfect joins, nonsensical values, and missing information. This is also a complicated and large data set. You may need to spend some time with the files before the structure of the data is clear. Rentals are called \"valuation entities\" and have unique VE_NUMBERs as their id column. Each table with valuation_entities in the name can be joined to a rental property. Land information is referenced by a LAN_ID. Each table with land in the name contains information on land. Use land_valuation_key.csv to join rentals to the land they inhabit. A data dictionary and entity relationship diagram is provided to give a rough guide for column meanings. You may notice minor discrepancies (such as an additional prefix on column names) due to differences in the client schema vs. the competition files. If you find a discrepancy, you should rely on what is true in the data over the auxiliary files."
    },
    {
        "name": "NFL Big Data Bowl 2021",
        "url": "https://www.kaggle.com/competitions/nfl-big-data-bowl-2021",
        "overview_text": "Overview text not found",
        "description_text": "When a quarterback takes a snap and drops back to pass, what happens next may seem like chaos. As offensive players move in various patterns, the defense works together to prevent successful pass completions and then to quickly tackle receivers that do catch the ball. In this year\u2019s Kaggle competition, your goal is to use data science to better understand the schemes and players that make for a successful defense against passing plays.  In American football, there are a plethora of defensive strategies and outcomes. The National Football League (NFL) has used previous Kaggle competitions to focus on offensive plays, but as the old proverb goes, \u201cdefense wins championships.\u201d Though metrics for analyzing quarterbacks, running backs, and wide receivers are consistently a part of public discourse, techniques for analyzing the defensive part of the game trail and lag behind. Identifying player, team, or strategic advantages on the defensive side of the ball would be a significant breakthrough for the game. This competition uses NFL\u2019s Next Gen Stats data, which includes the position and speed of every player on the field during each play. You\u2019ll employ player tracking data for all drop-back pass plays from the 2018 regular season. The goal of submissions is to identify unique and impactful approaches to measure defensive performance on these plays. There are several different directions for participants to \u2018tackle\u2019 (ha)\u2014which may require levels of football savvy, data aptitude, and creativity. As examples: What does data tell us about defending the pass play? You are about to find out. Note: Are you a university participant? Students have the option to participate in a college-only Competition, where you\u2019ll work on the identical themes above. Students can opt-in for either the Open or College Competitions, but not both.",
        "dataset_text": "The 2021 Big Data Bowl data contains player tracking, play, game, and player level information for all possible passing plays during the 2018 regular season. For purposes of this event, passing plays are considered to be ones on a pass was thrown, the quarterback was sacked, or any one of five different penalties was called (defensive pass interference, offensive pass interference, defensive holding, illegal contact, or roughing the passer). On each play, linemen (both offensive and defensive) data are not provided. The focus of this year's contest is on pass coverage. Here, you'll find a summary of each data set in the 2021 Data Bowl, a list of key variables to join on, and a description of each variable. Game data: The games.csv contains the teams playing in each game. The key variable is gameId. Player data: The players.csv file contains player-level information from players that participated in any of the tracking data files. The key variable is nflId. Play data: The plays.csv file contains play-level information from each game. The key variables are gameId and playId. Tracking data: Files week[week].csv contain player tracking data from all games in week [week]. The key variables are gameId, playId, and nflId. There are 17 weeks to a typical NFL Regular Season, and thus 17 data frames with player tracking data are provided. Each of the 17 week[week].csv files contain player tracking data from all passing plays during Week [week] of the 2018 regular season. Nearly all plays from each [gameId] are included; certain plays or games with insufficient data are dropped. Each team and player plays no more than 1 game in a given week."
    },
    {
        "name": "NFL Big Data Bowl 2022",
        "url": "https://www.kaggle.com/competitions/nfl-big-data-bowl-2022",
        "overview_text": "Overview text not found",
        "description_text": "Before National Football League (NFL) coaches celebrate a big W, they strategize ways to improve field position and score points. Both of these objectives receive significant contributions from special teams plays, which consist of punts, kickoffs, field goals and extra points. These play types take on important roles in a game\u2019s final score\u2014so much so that coaches say they're a third of the game. Yet special teams remain an understudied part of American football, with an opportunity for data science to offer better ways to understand its impact. The 2022 Big Data Bowl creates the opportunity for you (and the world!) to learn more about special teams play than ever before. We've provided the NFL's Next Gen Stats (NGS) tracking data from all 2018-2020 special teams plays. This data provides location information for each special teams player, wherever they are on the field, and includes their speed, acceleration, and direction. Additionally, and for the first time in Big Data Bowl history, participants can utilize scouting data from PFF, which supplements the tracking data with football specific metrics that coaches find critical to team success. The NFL is America's most popular sports league. Founded in 1920, the organization behind American football has developed the model for the successful modern sports league. They're committed to advancing every aspect of the game, including the lesser researched special teams. In this competition, you\u2019ll quantify what happens on special teams plays. You might create a new special teams metric, quantify team or individual strategies, rank players, or even something we haven\u2019t considered. With your creativity and analytical skills, the development of these new methods could lead to additional stats for special teams plays. If successful, your effort may even be adopted by the NFL for on air distribution, and you can watch future games knowing you had a hand in improving America's most popular sports league.",
        "dataset_text": "The 2022 Big Data Bowl data contains Next Gen Stats player tracking, play, game, player, and PFF scouting data for all 2018-2020 Special Teams plays. Here, you'll find a summary of each data set in the 2022 Data Bowl, a list of key variables to join on, and a description of each variable. Game data: The games.csv contains the teams playing in each game. The key variable is gameId.\nPlay data: The plays.csv file contains play-level information from each game. The key variables are gameId and playId.\nPlayer data: The players.csv file contains player-level information from players that participated in any of the tracking data files. The key variable is nflId.\nTracking data: Files tracking[season].csv contain player tracking data from season [season]. The key variables are gameId, playId, and nflId.\nPFF Scouting data: The PFFScoutingData.csv file contains play-level scouting information for each game. The key variables are gameId and playId. Files tracking[season].csv contains player tracking data from season [season]."
    },
    {
        "name": "NFL Big Data Bowl 2023",
        "url": "https://www.kaggle.com/competitions/nfl-big-data-bowl-2023",
        "overview_text": "Overview text not found",
        "description_text": "The National Football League (NFL) is back with another Big Data Bowl, where contestants use Next Gen Stats player tracking data to generate actionable, creative, and novel stats. Previous iterations have considered running backs, defensive backs, and special teams, and have generated metrics that have been used on television and by NFL teams. In this year\u2019s competition, you\u2019ll have more subtle performances to consider\u2014and potentially more players to measure. Quarterbacks may get the glory, but some of the most important work takes place a few feet in front of them. The offensive line protects the passer, providing precious seconds to find receivers downfield. At the same time, the opposing team\u2019s defensive line attempts to find a disruptive path. If a defender sneaks through, it can mean a sack, a blocked pass, or even a turnover. Some of the game\u2019s most important plays happen on the line and this competition examines the data behind the hardest workers in football. In this competition, you\u2019ll have access to the NFL\u2019s Next Gen Stats data, including player tracking, play, game, and player information, as well as Pro Football Focus (PFF) scouting data for 2021 passing plays (Weeks 1-8 of the NFL season). You\u2019ll create new metrics and stats for America's most popular sports league. Notebook submissions will be scored based on five components: innovation, accuracy, relevance, clarity, and data visualization. Winners will be invited to present their results to the NFL, where one competition team will receive an additional prize. The most useful new metrics or analysis could be also used by NFL teams to evaluate their offensive and defensive lines.",
        "dataset_text": "Here, you'll find a summary of each data set in the 2023 Data Bowl, a list of key variables to join on, and a description of each variable. The tracking data is provided by the NFL Next Gen Stats team. The Scouting data is provided by Pro Football Focus. The 2023 Big Data Bowl allows participants to use supplemental NFL data as long as it is free and publicly available to all participants. Examples of sources that could be used include nflverse and Pro Football Reference. Please note that the gameId and playId of the Big Data Bowl data merge with the old_game_id and play_id of nflverse's play-by-play data. Game data: The games.csv contains the teams playing in each game. The key variable is gameId. Play data: The plays.csv file contains play-level information from each game. The key variables are gameId and playId. Player data: The players.csv file contains player-level information from players that participated in any of the tracking data files. The key variable is nflId. PFF Scouting data: The pffScoutingData.csv file contains player-level scouting information for each game and play. The key variables are gameId, playId, and nflId. Tracking data: Files week[week].csv contain player tracking data from season [week]. The key variables are gameId, playId, and nflId. Files week[week].csv contains player tracking data from week [week]. "
    },
    {
        "name": "NFL Big Data Bowl 2024",
        "url": "https://www.kaggle.com/competitions/nfl-big-data-bowl-2024",
        "overview_text": "The National Football League (NFL) is back with another Big Data Bowl, where contestants use Next Gen Stats player tracking data to generate actionable, creative, and novel stats. Previous iterations have analyzed running backs, defensive backs, special teams, and pass rush plays, and have generated metrics that have been used on television and by NFL teams. This year's competition focuses on one area that we have yet to \u2026 tackle \u2026",
        "description_text": "American football is a complex sport, but once an offensive player receives a handoff or catches a pass, all 11 defenders focus on one task -- tackle that ball carrier as soon as possible. Conversely, the ball carrier's role is to advance the ball down the field to gain as much yardage as possible until he is tackled, scores, or runs out of bounds. This year's competition offers up a general goal \u2014 create metrics that assign value to elements of tackling. You can access the NFL\u2019s Next Gen Stats data as in previous competitions. This year's player tracking includes data from Weeks 1-9 of the 2022 NFL season. Data will show the location, speed, and acceleration of all 22 players on the field, along with football location. Additional PFF scouting data and NFL advanced stats such as expected points and win probability are also included. Top winners will be invited to present their results to the NFL, where one team will receive an additional prize. NFL teams may also use the most useful new metrics or analysis to evaluate both their offensive and defensive players. Your challenge is generating actionable, practical, and novel insights from player tracking data corresponding to tackling. Examples include, but are not limited to: Note that the above list is not exhaustive, and we encourage participants to be creative with their submissions.",
        "dataset_text": "Here, you'll find a summary of each data set in the 2024 Data Bowl, a list of key variables to join on, and a description of each variable. The tracking data is provided by the NFL Next Gen Stats team. The pff_missedTackles column in the tackles data is provided by Pro Football Focus.\nPlease note that in the tracking data, only a subset of frames are included based on what occurs in the play: The 2024 Big Data Bowl allows participants to use supplemental NFL data as long as it is free and publicly available to all participants. Examples of sources that could be used include nflverse and Pro Football Reference. Please note that the gameId and playId of the Big Data Bowl data merges with the old_game_id and play_id of nflverse's play-by-play data. Game data: The games.csv contains the teams playing in each game. The key variable is gameId.\nPlay data: The plays.csv file contains play-level information from each game. The key variables are gameId and playId.\nPlayer data: The players.csv file contains player-level information from players that participated in any of the tracking data files. The key variable is nflId.\nTackles data: The tackles.csv file contains player-level tackle information for each game and play. The key variables are gameId, playId, and nflId.\nTracking data: Files tracking_week_[week].csv contain player tracking data from week number [week]. The key variables are gameId, playId, and nflId. Files tracking_week_[week].csv contains player tracking data from week [week]."
    },
    {
        "name": "Google - Gemini Long Context",
        "url": "https://www.kaggle.com/competitions/gemini-long-context",
        "overview_text": "A differentiating factor for the Gemini 1.5 model is its large context window that supports context caching.",
        "description_text": "Gemini 1.5 introduced a major breakthrough in AI with its notably large context window. It can process up to 2 million tokens at once vs. the typical 32,000 - 128,000 tokens. This is equivalent to being able to remember roughly 100,000 lines of code, 10 years of text messages, or 16 average English novels. With large context windows, methods like vector databases and RAG (that were built to overcome short context windows) become less important, and more direct methods such as in-context retrieval become viable instead. Likewise, methods like many-shot prompting where models are provided with hundreds or thousands of examples of a task as either a replacement or a supplement for fine-tuning also become possible. In initial tests, the Google Deepmind team saw very promising results, with state-of-the-art performance in long-document QA, long-video QA, and long-context ASR. They shared an entire code base with Gemini 1.5 and had it successfully create documentation. They also had the model \"watch\" the film Sherlock JR from 1924, and it answered questions correctly. This competition challenges you to stress test Gemini 1.5\u2019s long context window by building public Kaggle Notebooks and YouTube Videos that demonstrate creative use cases. We\u2019re eager to see what you build!",
        "dataset_text": "This competition does not have an official dataset. For more information about the Gemini model family's ability to work with long context prompts, see here and here. For documentation about the Gemini API, see here and here. The submission_instructions.txt file is included below, for reference."
    },
    {
        "name": "NFL Big Data Bowl 2025",
        "url": "https://www.kaggle.com/competitions/nfl-big-data-bowl-2025",
        "overview_text": "The National Football League (NFL) is back with another Big Data Bowl, where contestants use Next Gen Stats player tracking data to generate actionable, creative, and novel stats. Previous iterations have analyzed running backs, defensive backs, special teams, pass rush plays, and tackling, and have generated metrics that have been used on television and by NFL teams.",
        "description_text": "NFL offenses have 40 seconds in which to run a play. That time begins with substitutions, as players run on and off the field until both teams' personnel are configured. It continues into the play call, where both the offensive and defensive units learn their formation and assignments. It ends with myriad strategic decisions by the 22 players on the field, including motion, shifts, and alignment changes, designed to both confuse the opponent and capitalize on any advantages. In all that action prior to the snap, both teams likely divulge patterns in what players will do after the snap. The goal of this year's competition aims to tell us just what those patterns are. Your challenge is generating actionable, practical, and novel insights from player tracking data corresponding to pre-snap team and player tendencies. Examples include, but are not limited to: Note that the above list is not exhaustive, and we encourage participants to be creative with their submissions.",
        "dataset_text": "Here, you'll find a summary of each data set in the 2025 Data Bowl, a list of key variables to join on, and a description of each variable. The tracking data is provided by the NFL Next Gen Stats team. The columns labeled with a prefix of \"pff_\" are provided by Pro Football Focus. The 2025 Big Data Bowl allows participants to use supplemental NFL data as long as it is free and publicly available to all participants. Examples of sources that could be used include nflverse and Pro Football Reference. Please note that the gameId and playId of the Big Data Bowl data merges with the old_game_id and play_id of nflverse's play-by-play data. Game data: The games.csv contains the teams playing in each game. The key variable is gameId.\nPlay data: The plays.csv file contains play-level information from each game. The key variables are gameId and playId.\nPlayer data: The players.csv file contains player-level information from players that participated in any of the tracking data files. The key variable is nflId.\nPlayer play data: The player_play.csv file contains player-level stats for each game and play. The key variables are gameId, playId, and nflId.\nTracking data: Files tracking_week_[week].csv contain player tracking data from week number [week]. The key variables are gameId, playId, and nflId. Files tracking_week_[week].csv contains player tracking data from week [week]."
    },
    {
        "name": "CDP - Unlocking Climate Solutions",
        "url": "https://www.kaggle.com/competitions/cdp-unlocking-climate-solutions",
        "overview_text": "Overview text not found",
        "description_text": "CDP is a\u202fglobal\u202fnon-profit that drives companies and governments to reduce their greenhouse gas emissions, safeguard water resources, and protect forests. Each year, CDP takes the information supplied in its annual reporting process and scores companies and cities based on their journey through disclosure and towards environmental leadership. CDP houses the world\u2019s largest, most comprehensive dataset on environmental action. As the data grows to include thousands more companies and cities each year, there is increasing potential for the data to be utilized in impactful ways. Because of this potential, CDP is excited to launch an analytics challenge for the Kaggle community. Data scientists will scour environmental information provided to CDP by disclosing companies and cities, searching for solutions to our most pressing problems related to climate change, water security, deforestation, and social inequity. Develop a methodology for calculating key performance indicators (KPIs) that relate to the environmental and social issues that are discussed in the CDP survey data. Leverage external data sources and thoroughly discuss the intersection between environmental issues and social issues. Mine information to create automated insight generation demonstrating whether city and corporate ambitions take these factors into account. To make a submission, complete the submission form. Only one submission will be judged per participant, so if you make multiple submissions we will only review the most recent entry. A starter notebook demonstrates how to load and work with the data. To be valid, a submission must be contained in one or more notebook, and made public on or before the submission deadline. Participants are free to use any datasets in addition to the official Kaggle dataset, but those datasets must be public and hosted on Kaggle for the submission to be valid.",
        "dataset_text": "The CDP dataset consists of publicly available responses to 3 different surveys: (1) corporate climate change disclosures; (2) corporate water security disclosures; and (3) disclosures from cities. Data is available for 2018, 2019, and 2020, along with a small collection of supplementary datasets. A starter notebook demonstrates how to load and work with the data. "
    },
    {
        "name": "Coleridge Initiative - Show US the Data",
        "url": "https://www.kaggle.com/competitions/coleridgeinitiative-show-us-the-data",
        "overview_text": "Overview text not found",
        "description_text": "This competition challenges data scientists to show how publicly funded data are used to serve science and society. Evidence through data is critical if government is to address the many threats facing society, including; pandemics, climate change, Alzheimer\u2019s disease, child hunger, increasing food production, maintaining biodiversity, and addressing many other challenges. Yet much of the information about data necessary to inform evidence and science is locked inside publications. Can natural language processing find the hidden-in-plain-sight data citations? Can machine learning find the link between the words used in research articles and the data referenced in the article? Now is the time for data scientists to help restore trust in data and evidence. In the United States, federal agencies are now mandated to show how their data are being used. The new Foundations of Evidence-based Policymaking Act requires agencies to modernize their data management. New Presidential Executive Orders are pushing government agencies to make evidence-based decisions based on the best available data and science. And the government is working to respond in an open and transparent way. This competition will build just such an open and transparent approach. The results will show how public data are being used in science and help the government make wiser, more transparent public investments. It will help move researchers and governments from using ad-hoc methods to automated ways of finding out what datasets are being used to solve problems, what measures are being generated, and which researchers are the experts. Previous competitions have shown that it is possible to develop algorithms to automate the search and discovery of references to data. Now, we want the Kaggle community to develop the best approaches to identify critical datasets used in scientific publications. In this competition, you'll use natural language processing (NLP) to automate the discovery of how scientific data are referenced in publications. Utilizing the full text of scientific publications from numerous research areas gathered from CHORUS publisher members and other sources, you'll identify data sets that the publications' authors used in their work. If successful, you'll help support evidence in government data. Automated NLP approaches will enable government agencies and researchers to quickly find the information they need. The approach will be used to develop data usage scorecards to better enable agencies to show how their data are used and bring down a critical barrier to the access and use of public data. The Coleridge Initiative is a not-for-profit that has been established to use data for social good. One way in which the organization does this is by furthering science through publicly available research. Coleridge Data Examples\nRich Search and Discovery for Research Datasets\nDemocratizing Our Data\nNSF\"Rich Context\" Video United States Department of Agriculture\nUnited States Department of Commerce\nUnited States Geological Survey\nNational Oceanic and Atmospheric Administration\nNational Science Foundation\nNational Institutes of Health\nCHORUS\nWestat\nAlfred P. Sloan Foundation\nSchmidt Futures\nOverdeck Family Foundation ",
        "dataset_text": "The objective of the competition is to identify the mention of datasets within scientific publications. Your predictions will be short excerpts from the publications that appear to note a dataset. Predictions that more accurately match the precise words used to identify the dataset within the publication will score higher. Predictions should be cleaned using the clean_text function from the Evaluation page to ensure proper matching. Publications are provided in JSON format, broken up into sections with section titles. The goal in this competition is not just to match known dataset strings but to generalize to datasets that have never been seen before using NLP and statistical techniques. A percentage of the public test set publications are drawn from the training set - not all datasets have been identified in train, so these unidentified datasets have been used as a portion of the public test labels. These should serve as guides for the difficult task of labeling the private test set. Note that the hidden test set has roughly ~8000 publications, many times the size of the public test set. Plan your compute time accordingly."
    },
    {
        "name": "The Allen AI Science Challenge",
        "url": "https://www.kaggle.com/competitions/the-allen-ai-science-challenge",
        "overview_text": "Overview text not found",
        "description_text": " The Allen Institute for Artificial Intelligence (AI2) is working to improve humanity through fundamental advances in artificial intelligence. One critical but challenging problem in AI is to demonstrate the ability to consistently understand and correctly answer general questions about the world.  The Aristo project at AI2 is focused on building such a system. One way Aristo \"learns\" is by extracting facts from various sources and processing them into a structured knowledge base. When taking an exam, questions are parsed and processed along with any accompanying diagrams to determine a strategy for answering. Aristo then uses entailment, statistical analysis, and inference methods to select a final answer. While Aristo's abilities have improved significantly in the last two years, it still doesn't have perfect, reliable methods of gathering knowledge, understanding questions, or reasoning through answers. Using a dataset of multiple choice question and answers from a standardized 8th grade science exam, AI2 is challenging you to create a model that gets to the head of the class.",
        "dataset_text": "The training data consists of 2,500 multiple choice questions from a typical US 8th grade science curriculum. Each question has four possible answers, of which exactly one is correct. Note that the questions in these datasets are private intellectual property, and by acknowledging the competition rules, you agree to not sharing or publishing any questions to parties other than yourself at any point in time, both during and after the competition. This is a two-stage competition. In the first stage, you are building models based on the training dataset, and testing your models by submitting predictions on the validation set. One week before the final deadline, you will submit your model to Kaggle. At this point, the second stage of the competition starts. Kaggle will release the final test dataset, on which you will run your predictive models. The final scores will be calculated based on this final test set. The validation set contains 8,132 questions of the same type without providing the correct answer. This set should only be used to submit automatically generated answers and should not be used for training purposes. To discourage inappropriate use, only a small proportion of these questions are real competition questions that will count for scoring. All the valid questions are used for public leaderboard, and none for private leaderboard. The final test set, to be released at stage 2 of the competition, will contain 21,298 questions of the same type (including the 8,132 from the validation set). Again, only a small proportion will be used in the scoring. All the validation set questions will be used for the public leaderboard, and all the new test set questions used for private leaderboard. EDIT: Train/Validation/Test datasets are removed as of 2/13/2016 at the end of the competition "
    },
    {
        "name": "SenNet + HOA - Hacking the Human Vasculature in 3D",
        "url": "https://www.kaggle.com/competitions/blood-vessel-segmentation",
        "overview_text": "The goal of this competition is to segment blood vessels. You will create a model trained on 3D Hierarchical Phase-Contrast Tomography (HiP-CT) data from human kidneys to help complete a picture of vasculature throughout a body.",
        "description_text": "Your body's organs and tissues depend on the interaction, spatial organization, and specialization of your cells\u2014all 37 trillion of them. Researchers make sense of cellular functions and relationships with the Vasculature Common Coordinate Framework (VCCF). The VCCF maps cells using the blood vasculature in the human body as the primary navigation system. The framework crosses all scale levels and provides a unique way to identify cellular locations using capillary structures as an address. However, the gaps in what researchers know about the vasculature lead to gaps in the VCCF. If data science could help automatically segment vasculature arrangements, researchers could use the real-world tissue data to fill in those gaps and complete their picture of the vasculature throughout the body. Currently, human expert annotators manually trace the vascular structures\u2014a slow process. Even with expert annotators, each new dataset takes 6+ months to complete. Machine learning approaches using this manual data do not generalize well to new datasets because of the variability of both human anatomy and to changes in the image quality as HiP-CT technology continues to improve and change. Competition host the Common Fund\u2019s Cellular Senescence Network (SenNet) Program was established to comprehensively identify and characterize the differences in senescent cells across the body, across various states of human health, and across the lifespan. SenNet provides publicly accessible atlases of senescent cells and develops innovative tools and technologies that build upon previous advances in single-cell analysis. SenNet is joined by the Human Organ Atlas (HOA) for this competition. HOA is a digital atlas containing 3D multi-resolution imaging datasets, created at the world\u2019s brightest synchrotron (European Synchrotron Radiation Facility), using an imaging technique called Hierarchical Phase-Contrast Tomography (HiP-CT). HiP-CT spans a previously poorly explored scale in researchers\u2019 understanding of human anatomy, from microns to entire intact organs. Your efforts could improve our understanding of the effect of vasculature on different cells in the human body. With better data, researchers could simulate the flow of blood, oxygen, or even drugs through the vessel network. They could also use the available organ images to augment their understanding of how blood vasculature changes as sex, age, and BMI change. Ultimately, you could help pave the way towards a more complete Vascular Common Coordinate Framework (VCCF) and Human Reference Atlas (HRA), which could identify how the relationships between cells affect our health.",
        "dataset_text": "This competition dataset comprises high-resolution 3D images of several kidneys together with 3D segmentation masks of their vasculature. Your task is to create segmentation masks for the kidney datasets in the test set. The kidney images were obtained through Hierarchical Phase-Contrast Tomography (HiP-CT) imaging. HiP-CT is an imaging technique that obtains high-resolution (from 1.4 micrometers - 50 micrometers resolution) 3D data from ex vivo organs. See this Nature Methods article for more information. You may find functions to encode and decode the segmentation masks in this notebook. Please note that this is a Code Competition. We give a few example images in the test/ folder to help you author your submissions. These example images are not meant to be representative of the test set with regard to scan resolution, beamline, or other such qualities. When your submission is scored, this example test data will be replaced with the full test set. The full test set contains about 1500 TIFF images. Hi All Kagglers,\nIt's been great watching the progress these last few weeks. Seeing a few things in the competition and also following the discussion carefully we decided that it makes sense to give some more information about the two hidden datasets i.e. the Public and Private test datasets: Public Test:\nContinuous 3D part of a whole human kidney imaged with HiP-CT - Originally scanned at 25.14um/voxel and binned to 50.28um/voxel (bin x2) before segmentation. Private Test:\nContinuous 3D part of a whole human kidney imaged with HiP-CT - Originally scanned at 15.77um/voxel binned to 63.08um/voxel (bin x4) before segmentation. Neither of these kidneys are different parts of ones already in the public dataset, i.e. they are physically different samples. The scanning and reconstruction procedures follow the method described in the original HiP-CT Methods paper (you should read this paper for background), and the manual annotation process is the same as in the benchmark review paper. We provided the higher resolution 5.2um/voxel VOI, to give some example data of different resolutions as we thought this might be useful."
    },
    {
        "name": "ISIC 2024 - Skin Cancer Detection with 3D-TBP",
        "url": "https://www.kaggle.com/competitions/isic-2024-challenge",
        "overview_text": "In this competition, you'll develop image-based algorithms to identify histologically confirmed skin cancer cases with single-lesion crops from 3D total body photos (TBP). The image quality resembles close-up smartphone photos, which are regularly submitted for telehealth purposes. Your binary classification algorithm could be used in settings without access to specialized care and improve triage for early skin cancer detection.",
        "description_text": "Skin cancer can be deadly if not caught early, but many populations lack specialized dermatologic care. Over the past several years, dermoscopy-based AI algorithms have been shown to benefit clinicians in diagnosing melanoma, basal cell, and squamous cell carcinoma. However, determining which individuals should see a clinician in the first place has great potential impact. Triaging applications have a significant potential to benefit underserved populations and improve early skin cancer detection, the key factor in long-term patient outcomes. Dermatoscope images reveal morphologic features not visible to the naked eye, but these images are typically only captured in dermatology clinics. Algorithms that benefit people in primary care or non-clinical settings must be adept to evaluating lower quality images. This competition leverages 3D TBP to present a novel dataset of every single lesion from thousands of patients across three continents with images resembling cell phone photos. This competition challenges you to develop AI algorithms that differentiate histologically-confirmed malignant skin lesions from benign lesions on a patient. Your work will help to improve early diagnosis and disease prognosis by extending the benefits of automated skin cancer detection to a broader population and settings.",
        "dataset_text": "What should I expect the data format to be?\nThe dataset consists of diagnostically labelled images with additional metadata. The images are JPEGs. The associated .csv file contains a binary diagnostic label (target), potential input variables (e.g. age_approx, sex, anatom_site_general, etc.), and additional attributes (e.g. image source and precise diagnosis). What am I predicting?\nIn this challenge you are differentiating benign from malignant cases. For each image (isic_id) you are assigning the probability (target) ranging [0, 1] that the case is malignant. To mimic non-dermoscopic images, this competition uses standardized cropped lesion-images of lesions from 3D Total Body Photography (TBP). Vectra WB360, a 3D TBP product from Canfield Scientific, captures the complete visible cutaneous surface area in one macro-quality resolution tomographic image. An AI-based software then identifies individual lesions on a given 3D capture. This allows for the image capture and identification of all lesions on a patient, which are exported as individual 15x15 mm field-of-view cropped photos. The dataset contains every lesion from a subset of thousands of patients seen between the years 2015 and 2024 across nine institutions and three continents. The following are examples from the training set. 'Strongly-labelled tiles' are those whose labels were derived through histopathology assessment. 'Weak-labelled tiles' are those who were not biopsied and were considered 'benign' by a doctor.  '+ D\u2019Alessandro, B. \"Methods and apparatus for identifying skin features of interest.\" US Patent #11,164,670. (2021). '+ D\u2019Alessandro, B. \"Methods and apparatus for identifying skin features of interest.\" US Patent #11,164,670. (2021).\n'++Betz-Stablein, B., et al. Reproducible naevus counts using 3D total body photography and convolutional neural networks. Dermatology. 238, 4\u201311 (2021). Please cite the SLICE-3D dataset under CC BY-NC 4.0 with the following attribution:"
    },
    {
        "name": "NFL Punt Analytics Competition",
        "url": "https://www.kaggle.com/competitions/NFL-Punt-Analytics-Competition",
        "overview_text": "Overview text not found",
        "description_text": "In this challenge you'll notice there isn't a leaderboard, and you are not required to develop a predictive model. This isn't a traditional supervised Kaggle machine learning competition. Instead, this challenge asks you to use data to propose specific rule modifications for the NFL that aim to reduce the occurrence of concussions during punt plays. For more information on this challenge format, see this forum thread. This challenge is part of NFL 1st & Future, presented by Arrow Electronics \u2013 the NFL\u2019s annual Super Bowl competition designed to spur innovation in player health, safety and performance. For the 2018 season, the NFL revised their kickoff rules in an effort to reduce the risk of injury during those plays. By examining injury reports, player position and velocity data, and game video, they were able to understand the game-play circumstances that may exacerbate the risk of injury to players. This comprehensive review showed that over the course of all games during the 2015-2017 seasons, the kickoff represented only six percent of plays but 12 percent of concussions. Players had approximately four times the risk of concussion on returned kickoffs compared to running or passing plays. The changes to the kickoff rule aim to address the components that posed the most risk, like the use of a two-man wedge. Now, the NFL is challenging Kagglers to help them perform the same examination, this time on punt play rules. They have provided data for all punt plays from the 2016 and 2017 NFL seasons that includes player rosters, on-field position data and video data, including the plays in which a player suffered a concussion. Your challenge is to propose specific rule modifications (e.g. changes to the initial formation, tackling techniques, blocking rules etc.), supported by data, that may reduce the occurrence of concussions during punt plays. More details on the entry criteria are available in Overview tab > Evaluation. The National Football League is America's most popular sports league, comprised of 32 franchises that compete each year to win the Super Bowl, the world's biggest annual sporting event. Founded in 1920, the NFL developed the model for the successful modern sports league, including national and international distribution, extensive revenue sharing, competitive excellence, and strong franchises across the country. The NFL is committed to advancing progress in the diagnosis, prevention and treatment of sports-related injuries. The NFL's ongoing health and safety efforts include support for independent medical research and engineering advancements and a commitment to look at anything and everything to protect players and make the game safer, including enhancements to medical protocols and improvements to how our game is taught and played. As more is learned, the league evaluates and changes rules to evolve the game and try to improve protections for players. Since 2002 alone, the NFL has made 50 rules changes intended to eliminate potentially dangerous tactics and reduce the risk of injuries. For more information about the NFL's health and safety efforts, please visit www.PlaySmartPlaySafe.com. Evaluation",
        "dataset_text": "The following data is provided for NFL seasons 2016 to 2017. Each dataset can be merged on the game, play or player level using the provided key variables. GameKey provides a unique identifier for a specific game which is unique across NFL seasons. PlayID identifies a unique play within a specified GameKey. GSISID provides a unique identifier for a player across all seasons.  Player punt data assigns each player their typical football position. Player Play Role data assigns each player a punt-specific role. These roles may differ by player between plays. This table also defines all players in each punt play. See the Appendix for a diagram of the Role definitions. The Video Review database contains play and player information for each identifiable play that was associated with a concussion. For each injured player, the Primary Exposure is the impact that is observed to be markedly more severe than any other exposure during that play and was considered to be the primary source of the concussion. In some cases, the injury producing play can be identified, but the \u201cPrimary\u201d event (helmet to helmet, helmet to body) cannot be identified. The \u201cPrimary Impact\u201d will be listed as Unclear if the video coverage was adequate to observe all the events experienced by the player, but the competing exposures could not be differentiated to identify a primary. For plays in which the video coverage was not sufficient to visualize the player\u2019s exposures, the primary exposure will be listed as Indeterminate. The data provided in the video review dataset will be only those for the primary impact. Within the video review database, the prefix \u201cPlayer\u201d indicates the concussed player and \u201cPartner\u201d indicates the collision partner when applicable. If both the player and partner are concussed, then each player will be listed as a player. The NGS datasets contains player position and direction data for each player during the entire course of the play. The NGS dataset is the only dataset that contains Timeas a variable. Speed can be calculated from the provided time, and X, Y position coordinates of the player. When processing NGS data, it is recommended to calculate velocity direction using the x, y position data and use those calculated velocities for any analyses. If acceleration is desired, differentiating the NGS velocity data is recommended. The NGS data is in units of yards, so the final analyses should be converted to meters. The origin for the x and y coordinates is defined as the corner of the home endzone and home sideline (Figure 1). The angles defined by orientation and direction are referenced from the y-axis of the coordinate system. Figure 1: Coordinate system and origin (bottom left) used with the NGS position data. Appendix 1: Punt Player Position (Role)\n\nPunt Coverage Appendix 2: Punt Player Position (Role)\n\nPunt Return"
    },
    {
        "name": "Draper Satellite Image Chronology",
        "url": "https://www.kaggle.com/competitions/draper-satellite-image-chronology",
        "overview_text": "Overview text not found",
        "description_text": "Imagine a world where we can use satellite images to help find better access to clean water, prevent poaching of wildlife, predict storms more efficiently, optimize traffic patterns more readily, and inform human behaviors to mitigate the spread of disease.  Thanks to a marked increase of satellites in orbit, we will be able to capture images \u2013 and the information contained within \u2013 of nearly every place on Earth, every day by 2017. However, our ability to analyze datasets of these images has not advanced as quickly. Changes from day to day in images of the same location are subtle, can be hard to detect, and are difficult to understand in terms of their significance. In this competition, Draper provides a unique dataset of images taken at the same locations over 5 days. Kagglers are challenged to predict the chronological order of the photos taken at each location. Accurately doing so could uncover approaches that have a global impact on commerce, science, and humanitarian works.",
        "dataset_text": "This dataset contains over one thousand high-resolution images of aerial photographs taken in southern California. The photographs were taken from a plane and are meant as a reasonable facsimile for satellite images. Images are grouped into sets of five, each of which have the same setId. Each image in a set was taken on a different day (but not necessarily at the same time each day). The images for each set cover approximately the same area but are not exactly aligned. Images are named according to the convention setId_day. In the training set, the images have the correct day ordering. In the test set, the images are scrambled. This competition asks you order the scrambled images in the test set. This is anticipated to be a challenging task. Some locations will provide very little evidence of changes from day to day. Other locations will have subtle changes that hint at an ordering, such as parked or moving vehicles, differences in bodies of water, or changes in shadows. On account of the difficulty, you may use both manual and computer-based analysis in this competition. For convenience, we have provided both lossless, high-quality images (train/test) as well as the same images in a compressed jpeg format (train_sm/test_sm)."
    },
    {
        "name": "NFL Big Data Bowl",
        "url": "https://www.kaggle.com/competitions/nfl-big-data-bowl-2020",
        "overview_text": "Overview text not found",
        "description_text": "\u201cThe running back takes the handoff\u2026 he breaks a tackle\u2026spins\u2026 and breaks free! One man to beat! Past the 50-yard-line! To the 40! The 30! He! Could! Go! All! The! Way!\u201d But will he?  American football is a complex sport. From the 22 players on the field to specific characteristics that ebb and flow throughout the game, it can be challenging to quantify the value of specific plays and actions within a play. Fundamentally, the goal of football is for the offense to run (rush) or throw (pass) the ball to gain yards, moving towards, then across, the opposing team\u2019s side of the field in order to score. And the goal of the defense is to prevent the offensive team from scoring. In the National Football League (NFL), roughly a third of teams\u2019 offensive yardage comes from run plays.. Ball carriers are generally assigned the most credit for these plays, but their teammates (by way of blocking), coach (by way of play call), and the opposing defense also play a critical role. Traditional metrics such as \u2018yards per carry\u2019 or \u2018total rushing yards\u2019 can be flawed; in this competition, the NFL aims to provide better context into what contributes to a successful run play. As an \u201carmchair quarterback\u201d watching the game, you may think you can predict the result of a play when a ball carrier takes the handoff - but what does the data say? In this competition, you will develop a model to predict how many yards a team will gain on given rushing plays as they happen. You'll be provided game, play, and player-level data, including the position and speed of players as provided in the NFL\u2019s Next Gen Stats data. And the best part - you can see how your model performs from your living room, as the leaderboard will be updated week after week on the current season\u2019s game data as it plays out. Deeper insight into rushing plays will help teams, media, and fans better understand the skill of players and the strategies of coaches. It will also assist the NFL and its teams evaluate the ball carrier, his teammates, his coach, and the opposing defense, in order to make adjustments as necessary. Additionally, the winning model will be provided to the NFL\u2019s Next Gen Stats group to potentially share with teams. You could help the NFL Network generate models to use during games, or for pre-game/post-game breakdowns.",
        "dataset_text": "This dataset contains Next Gen Stats tracking data for running plays. You must use features known at the time when the ball is handed off (TimeHandoff) to forecast the yardage gained on that play (PlayId). Because this is a time-series code competition that will be evaluated on future data, you will receive data and make predictions with a time-series API. This API provides plays in the time order in which they occurred in a game. Refer to the starter notebook here for an example of how to complete a submission. Note: Before the evaluation period begins, we will be updating the train.csv file to include current season games. Before Stage 2 begins, Kaggle will update the train.csv file to include current-season games through Stage 1. Please take note should you want to retraining to be a part of your model submission. To deter cheating by looking ahead in time, the API has been compiled and the test data encrypted on disk. While it may be possible, you should not decompile or attempt to read the test set outside of the API, as the encryption keys will change during the live scoring portion of the competition. During stage one, we ask that you respect the spirit of the competition and do not submit predictions that incorporate future information or the ground truth. Each row in the file corresponds to a single player's involvement in a single play. The dataset was intentionally joined (i.e. denormalized) to make the API simple. All the columns are contained in one large dataframe which is grouped and provided by PlayId. "
    },
    {
        "name": "NFL 1st and Future - Impact Detection",
        "url": "https://www.kaggle.com/competitions/nfl-impact-detection",
        "overview_text": "Overview text not found",
        "description_text": " The National Football League (NFL) has teamed up with Amazon Web Services (AWS) to develop the \u201cDigital Athlete,\u201d a virtual representation of a composite NFL player that the NFL can use to model game scenarios to try to better predict and prevent player injury. The NFL is actively addressing the need for a computer vision system to detect on-field helmet impacts as part of the \u201cDigital Athlete\u201d platform, and the league is calling on Kagglers to help. In this competition, you\u2019ll develop a computer vision model that automatically detects helmet impacts that occur on the field. Kick off with a dataset of more than one thousand definitive head impacts from thousands of game images, labeled video from the sidelines and end zones, and player tracking data. This information is sourced from the NFL\u2019s Next Gen Stats (NGS) system, which documents the position, speed, acceleration, and orientation for every player on the field during NFL games. This competition is part of the NFL\u2019s annual 1st and Future competition, which is designed to spur innovation in athlete safety and performance. For the first time this year, 1st and Future will be broadcast in primetime during Super Bowl LV week on NFL Network, and winning Kagglers may have the opportunity to present their computer vision systems as part of this exciting event. If successful, you could support the NFL\u2019s research programs in a big way: improving athletes' safety. Backed by this research, the NFL may implement rule changes and helmet design improvements to try to better protect the athletes who play the game millions watch each week. The National Football League is America's most popular sports league. Founded in 1920, the NFL developed the model for the successful modern sports league and is committed to advancing progress in the diagnosis, prevention, and treatment of sports-related injuries. Health and safety efforts include support for independent medical research and engineering advancements as well as a commitment to work to better protect players and make the game safer, including enhancements to medical protocols and improvements to how our game is taught and played. For more information about the NFL's health and safety efforts, please visit NFL.com/PlayerHealthandSafety.",
        "dataset_text": "In this competition, you are tasked with identifying helmet collisions in video files. Each play has two associated videos, showing a sideline and endzone view, and the videos are aligned so that frames correspond between the videos. The training set videos are in train with corresponding labels in train_labels.csv, while the videos for which you must predict are in the test folder. To aid with helmet detection, you are also provided an ancillary dataset of images showing helmets with labeled bounding boxes. These files are located in images and the bounding boxes in image_labels.csv. This is a code competition. When you submit, your model will be rerun on a set of 15 unseen plays located in the same test location. The publicly provided test videos are simply a set of mock plays (copied from the training set) which are not used in scoring. Note: the dataset provided for this competition has been carefully designed for the purposes of training computer vision models and therefore contains plays that have much higher incidence of helmet impacts than is normal. This dataset should not be used to make inferences about the incidence of helmet impact rates during football games, as it is not a representative sample of those rates. [train/test] mp4 videos of each play. Each play has two copies, one shot from the endzone and the other shot from the sideline. The video pairs are matched frame for frame in time, but different players may be visible in each view. You only need to make predictions for the view that a player is actually visible in. train_labels.csv Helmet tracking and collision labels for the training set. Note: The Sideline and Endzone views have been time-synced such that the snap occurs 10 frames into the video. This time alignment should be considered to be accurate to within +/- 3 frames or 0.05 seconds (video data is recorded at approximately 59.94 frames per second). For the purposes of evaluation, definitive helmet impacts are defined as meeting three criteria: Those labels with confidence = 1 document cases in which human labelers asserted it was possible that a helmet impact occurred, but it was not clear that the helmet impact altered the trajectory of the helmet. Those labels with visibility = 0 indicate that although there is reason to believe that an impact occurred to that helmet at that time, the impact itself was not visible from the view. sample_submission.csv A valid sample submission file. images Still photo equivalents of the train/test videos for use making a helmet detector. image_labels.csv contains the bounding boxes corresponding to the images. [train/test]_player_tracking.csv Each player wears a sensor that allows us to precisely locate them on the field; that information is reported in these two files. "
    },
    {
        "name": "Sartorius - Cell Instance Segmentation",
        "url": "https://www.kaggle.com/competitions/sartorius-cell-instance-segmentation",
        "overview_text": "Overview text not found",
        "description_text": "Neurological disorders, including neurodegenerative diseases such as Alzheimer's and brain tumors, are a leading cause of death and disability across the globe. However, it is hard to quantify how well these deadly disorders respond to treatment. One accepted method is to review neuronal cells via light microscopy, which is both accessible and non-invasive. Unfortunately, segmenting individual neuronal cells in microscopic images can be challenging and time-intensive. Accurate instance segmentation of these cells\u2014with the help of computer vision\u2014could lead to new and effective drug discoveries to treat the millions of people with these disorders.  Current solutions have limited accuracy for neuronal cells in particular. In internal studies to develop cell instance segmentation models, the neuroblastoma cell line SH-SY5Y consistently exhibits the lowest precision scores out of eight different cancer cell types tested. This could be because neuronal cells have a very unique, irregular and concave morphology associated with them, making them challenging to segment with commonly used mask heads. Sartorius is a partner of the life science research and the biopharmaceutical industry. They empower scientists and engineers to simplify and accelerate progress in life science and bioprocessing, enabling the development of new and better therapies and more affordable medicine. They're a magnet and dynamic platform for pioneers and leading experts in the field. They bring creative minds together for a common goal: technological breakthroughs that lead to better health for more people. In this competition, you\u2019ll detect and delineate distinct objects of interest in biological images depicting neuronal cell types commonly used in the study of neurological disorders. More specifically, you'll use phase contrast microscopy images to train and test your model for instance segmentation of neuronal cells. Successful models will do this with a high level of accuracy. If successful, you'll help further research in neurobiology thanks to the collection of robust quantitative data. Researchers may be able to use this to more easily measure the effects of disease and treatment conditions on neuronal cells. As a result, new drugs could be discovered to treat the millions of people with these leading causes of death and disability. ",
        "dataset_text": "In this competition we are segmenting neuronal cells in images. The training annotations are provided as run length encoded masks, and the images are in PNG format. The number of images is small, but the number of annotated objects is quite high. The hidden test set is roughly 240 images. Note: while predictions are not allowed to overlap, the training labels are provided in full (with overlapping portions included). This is to ensure that models are provided the full data for each object. Removing overlap in predictions is a task for the competitor. train.csv - IDs and masks for all training objects. None of this metadata is provided for the test set. sample_submission.csv - a sample submission file in the correct format train - train images in PNG format test - test images in PNG format. Only a few test set images are available for download; the remainder can only be accessed by your notebooks when you submit. train_semi_supervised - unlabeled images offered in case you want to use additional data for a semi-supervised approach. LIVECell_dataset_2021 - A mirror of the data from the LIVECell dataset. LIVECell is the predecessor dataset to this competition. You will find extra data for the SH-SHY5Y cell line, plus several other cell lines not covered in the competition dataset that may be of interest for transfer learning."
    },
    {
        "name": "CZII - CryoET Object Identification",
        "url": "https://www.kaggle.com/competitions/czii-cryo-et-object-identification",
        "overview_text": "In this competition, you'll develop machine learning (ML) algorithms to annotate diverse protein complexes (biological particles with well-defined structures) in 3D cellular images, accelerating discoveries in biomedical science and advancing disease treatment.",
        "description_text": "Protein complexes (such as oxygen-carrying hemoglobin, or keratin in hair, and thousands of others) are essential for cell function, and understanding their interactions is essential for our health and finding new disease treatments. Cryo-electron tomography (cryoET) creates 3D images\u2014called tomograms\u2014at near-atomic detail, showing proteins in their very complex and crowded natural environment. Therefore, cryoET has immense potential to unlock the mysteries of the cell. There is a wealth of cryoET tomograms that is yet to be fully mined. A large and growing portion of this published corpus exists in a standardized format in the cryoET data portal (cryoetdataportal.czscience.com). Mining this data requires automatic identification of each protein molecule within these images. This problem has not been solved even for proteins that are identifiable by the human eye. A generalizable solution will reveal the \u201cdark matter\u201d of the cell, and will enable thousands of discoveries contributing to human health. This competition challenges you to create ML algorithms that automatically annotate five classes of protein complexes within a curated real-world cryoET dataset.",
        "dataset_text": "This competition is focused on identifying points corresponding to particle centers in 3D tomograms. For testing you will have a list of 3D arrays to process named \"experiments\", and you will need to submit a CSV with particle locations of 5 particle types in all experiments. The data contains 6 particle types with different difficulty levels of prediction. Note that beta-amylase is not scored because it is considered to be too difficult for accurate evaluation. It is included in the data, but whether or not it is predicted has no bearing on the score. The test dataset on this page is example data copied over from the training dataset. The rerun test dataset contains ~500 tomographs. The tomograms are 3D arrays, and are provided as multiscale 3D OME-NGFF Zarr arrays. Tomograms can be opened with the zarr, ome-zarr, and copick libraries. In the training data 4 filtered versions of the tomograms are provided. (Only the denoised option is included in the test data.) These zarrs can be found in the competition directory, and details about their methods can be found in the preprint mentioned in the pinned welcome post. Note that these are multiscale zarr arrays, and 0 corresponds to the highest resolution. You can use copick to traverse the zarrs and maintain associations with labels, see the pinned notebooks. Particle locations are stored in json files according to the copick specification. You can use copick to read from the json files and perform utility functions like converting points into segmentation masks, see the pinned notebooks. If you intend to use copick to get tomograms, points, and labels, then see the example notebooks which provide examples for access to tomograms and making valid predictions of particle locations."
    },
    {
        "name": "NFL 1st and Future - Analytics",
        "url": "https://www.kaggle.com/competitions/nfl-playing-surface-analytics",
        "overview_text": "Overview text not found",
        "description_text": "In this challenge, you're tasked to investigate the relationship between the playing surface and the injury and performance of National Football League (NFL) athletes and to examine factors that may contribute to lower extremity injuries. You'll also notice there isn't a leaderboard, and you are not required to develop a predictive model. This isn't a traditional supervised Kaggle machine learning competition. For more information on this challenge format, see this forum thread. This challenge is part of NFL 1st & Future, the NFL\u2019s annual Super Bowl competition designed to spur innovation in player health, safety and performance. In the NFL, 12 stadiums have fields with synthetic turf. Recent investigations of lower limb injuries among football athletes have indicated significantly higher injury rates on synthetic turf compared with natural turf (Mack et al., 2018; Loughran et al., 2019). In conjunction with the epidemiologic investigations, biomechanical studies of football cleat-surface interactions have shown that synthetic turf surfaces do not release cleats as readily as natural turf and may contribute to the incidence of non-contact lower limb injuries (Kent et al., 2015). Given these differences in cleat-turf interactions, it has yet to be determined whether player movement patterns and other measures of player performance differ across playing surfaces and how these may contribute to the incidence of lower limb injury. Now, the NFL is challenging Kagglers to help them examine the effects that playing on synthetic turf versus natural turf can have on player movements and the factors that may contribute to lower extremity injuries. NFL player tracking, also known as Next Gen Stats, is the capture of real time location data, speed and acceleration for every player, every play on every inch of the field. As part of this challenge, the NFL has provided full player tracking of on-field position for 250 players over two regular season schedules. One hundred of the athletes in the study data set sustained one or more injuries during the study period that were identified as a non-contact injury of a type that may have turf interaction as a contributing factor to injury. The remaining 150 athletes serve as a representative sample of the larger NFL population that did not sustain a non-contact lower-limb injury during the study period. Details of the surface type and environmental parameters that may influence performance and outcome are also provided. Your challenge is to characterize any differences in player movement between the playing surfaces and identify specific scenarios (e.g., field surface, weather, position, play type, etc.) that interact with player movement to present an elevated risk of injury. More details on the entry criteria are available in Evaluation Tab. The National Football League is America's most popular sports league, comprised of 32 franchises that compete each year to win the Super Bowl, the world's biggest annual sporting event. Founded in 1920, the NFL developed the model for the successful modern sports league, including national and international distribution, extensive revenue sharing, competitive excellence, and strong franchises across the country. The NFL is committed to advancing progress in the diagnosis, prevention and treatment of sports-related injuries. The NFL's ongoing health and safety efforts include support for independent medical research and engineering advancements and a commitment to work to better protect players and make the game safer, including enhancements to medical protocols and improvements to how our game is taught and played. As more is learned, the league evaluates and changes rules to evolve the game and try to improve protections for players. Since 2002 alone, the NFL has made 50 rules changes intended to eliminate potentially dangerous tactics and reduce the risk of injuries. For more information about the NFL's health and safety efforts, please visit www.PlaySmartPlaySafe.com Evaluation",
        "dataset_text": "This page describes the datasets and variables provided to examine the effects that playing on synthetic turf versus natural turf can have on player movements and the factors that may contribute to lower extremity injuries. The data provided for analysis are 250 complete player in-game histories from two subsequent NFL regular seasons. Three different files in .csv format are provided, documenting injuries, player-plays, and player movement during plays. This manual describes the specifics of each variable contained within the datasets as well as guidelines on the best approach to processing the information. There are three files provided in the dataset, as described below:  The following provides a description of each field contained within the datasets and their corresponding formats and descriptions. Key Variables are designated in bold.  Note that there is not a PlayKey available for every injury. This indicates that the game in which the injury occurred is known, but the specific play in which the injury occurred was not noted at the time of injury. The play file contains information about each player-play in the dataset, to include the player\u2019s assigned roster position, stadium type, field type, weather, play type, position for the play, and position group  *Important Note: The GameID field is a unique identifier of player games but does not strictly reflect the order in which the games were played. The PlayerDay is an integer sequence that provides an accurate timeline for player game participation. In order to generate an accurate timeline of an individual player\u2019s game participation, the PlayerDay variable should be used. The interval between days in the PlayerDay field for an individual player accurately reflects the interval in days between that player\u2019s participation in games. Every player has a PlayerDay = 1 (note that this date is not the same for all players). Some players may have negative values for PlayerDay, which simply indicates participation in a game that occurred before their individually assigned PlayerDay = 1. The player track file in .csv format includes player position, direction, and orientation data for each player during the entire course of the play collected using the Next Gen Stats (NGS) system. This data is indexed by PlayKey (which includes information about the player and game), with the time variable providing a temporal index within an individual play. When processing the player track data, it is recommended to calculate velocity using the x, y position data and use those calculated velocities for any analysis (although we have provided the speed variable reported by the NGS system). The origin for the x and y coordinates is defined as the corner of the home endzone and home sideline (see Figure 1). The angles defined by orientation and direction are referenced from the y-axis of the coordinate system. Note that the orientation variable should not be considered to be a reliable indicator of the actual direction a player is facing. The records for this study come from multiple seasons of the NFL during which different systems were used to calculate and record a player\u2019s orientation. Within a play, and across plays in a game, an individual player\u2019s orientation will be recorded consistently, but the \u201cgeography\u201d (i.e. the actual direction the player is facing) of the reported orientation may not match the \u201cgeography\u201d of the direction variable in the same play. For those players that participated in multiple seasons, the geography used to record the orientation variable is not consistent across seasons. The orientation variable can be used to characterize how much a player is turning or pivoting during the course of a play. The \u201cgeography\u201d of the direction variable does remain consistent across the study horizon.  "
    },
    {
        "name": "As the World Churns",
        "url": "https://www.kaggle.com/competitions/deloitte-churn-prediction",
        "overview_text": "Overview text not found",
        "description_text": "Understanding customer loyalty is an important part of any business. The ability to predict ahead of time when a customer is likely to churn can enable early intervention processes to be put in place, and ultimately a reduction in customer churn.  This competition seeks a solution for predicting which current customers of an insurance company will leave in 12 months time, and when.",
        "dataset_text": "Please review the additional competition details described in AdditionalCompetitionInformation.pdf before downloading the data sets."
    },
    {
        "name": "Home Credit Default Risk",
        "url": "https://www.kaggle.com/competitions/home-credit-default-risk",
        "overview_text": "Overview text not found",
        "description_text": "Many people struggle to get loans due to insufficient or non-existent credit histories. And, unfortunately, this population is often taken advantage of by untrustworthy lenders.  Home Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities. While Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful.",
        "dataset_text": ""
    },
    {
        "name": "2023 Kaggle AI Report",
        "url": "https://www.kaggle.com/competitions/2023-kaggle-ai-report",
        "overview_text": "Overview text not found",
        "description_text": "More than a hundred AI papers are published every day, making it exceedingly hard to keep up with current innovations. The goal of this competition is to tap into the diverse expertise of the Kaggle community to centralize and summarize the rapid advancements in AI from the past two years. The Kaggle community has a breadth and depth of AI experience which extends beyond the reach of any single individual or research group. We aim to share your collective perspective with the broader research community. In this analytics competition, participants will write an essay on one of the following seven topics, with a prompt to describe what the community has learned over the past 2 years of working and experimenting with: Kaggle has provided a collection of writeups and a collection of scholarly articles in the official competition dataset, but participants are free to use any resources. Participants will also take part in a peer feedback process that involves reviewing three peer essays, and your submission will be assessed by an expert grading panel of seven Kaggle Grandmasters. Winning essays will be aggregated in a post-competition publication and given authorship credit. In response to demand and feedback from prior analytics competitions, this competition also pilots a format in which all eligible participants will be ranked on the leaderboard and awarded points based on their performance. Results will show on your profile, just as in a standard competition. See the submission instructions and evaluation pages for more detail.",
        "dataset_text": "Participants are encouraged to reference resources such as winning solution writeups and scholarly articles. We have included a collection of writeups and scholarly articles for your convenience, but participants are free to use additional resources as well. There is no requirement to refer to any of these files using code."
    },
    {
        "name": "State Farm Distracted Driver Detection",
        "url": "https://www.kaggle.com/competitions/state-farm-distracted-driver-detection",
        "overview_text": "Overview text not found",
        "description_text": "We've all been there: a light turns green and the car in front of you doesn't budge. Or, a previously unremarkable vehicle suddenly slows and starts swerving from side-to-side. When you pass the offending driver, what do you expect to see? You certainly aren't surprised when you spot a driver who is texting, seemingly enraptured by social media, or in a lively hand-held conversation on their phone.  According to the CDC motor vehicle safety division, one in five car accidents is caused by a distracted driver. Sadly, this translates to 425,000 people injured and 3,000 people killed by distracted driving every year. State Farm hopes to improve these alarming statistics, and better insure their customers, by testing whether dashboard cameras can automatically detect drivers engaging in distracted behaviors. Given a dataset of 2D dashboard camera images, State Farm is challenging Kagglers to classify each driver's behavior. Are they driving attentively, wearing their seatbelt, or taking a selfie with their friends in the backseat?",
        "dataset_text": "In this competition you are given driver images, each taken in a car with a driver doing something in the car (texting, eating, talking on the phone, makeup, reaching behind, etc). Your goal is to predict the likelihood of what the driver is doing in each picture.   The 10 classes to predict are: To ensure that this is a computer vision problem, we have removed metadata such as creation dates. The train and test data are split on the drivers, such that one driver can only appear on either train or test set.  To discourage hand labeling, we have supplemented the test dataset with some images that are resized. These processed images are ignored and don't count towards your score. Disclaimer: State Farm set up these experiments in a controlled environment - a truck dragging the car around on the streets - so these \"drivers\" weren't really driving. "
    },
    {
        "name": "Santander Customer Transaction Prediction",
        "url": "https://www.kaggle.com/competitions/santander-customer-transaction-prediction",
        "overview_text": "Overview text not found",
        "description_text": " At Santander our mission is to help people and businesses prosper. We are always looking for ways to help our customers understand their financial health and identify which products and services might help them achieve their monetary goals. Our data science team is continually challenging our machine learning algorithms, working with the global data science community to make sure we can more accurately identify new ways to solve our most common challenge, binary classification problems such as: is a customer satisfied? Will a customer buy this product? Can a customer pay this loan? In this challenge, we invite Kagglers to help us identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data we have available to solve this problem.",
        "dataset_text": "You are provided with an anonymized dataset containing numeric feature variables, the binary target column, and a string ID_code column. The task is to predict the value of target column in the test set."
    },
    {
        "name": "Jigsaw Unintended Bias in Toxicity Classification",
        "url": "https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification",
        "overview_text": "Overview text not found",
        "description_text": "Can you help detect toxic comments \u2015 and minimize unintended model bias? That's your challenge in this competition. The Conversation AI team, a research initiative founded by Jigsaw and Google (both part of Alphabet), builds technology to protect voices in conversation. A main area of focus is machine learning models that can identify toxicity in online conversations, where toxicity is defined as anything rude, disrespectful or otherwise likely to make someone leave a discussion. Last year, in the Toxic Comment Classification Challenge, you built multi-headed models to recognize toxicity and several subtypes of toxicity. This year's competition is a related challenge: building toxicity models that operate fairly across a diverse range of conversations. Here\u2019s the background: When the Conversation AI team first built toxicity models, they found that the models incorrectly learned to associate the names of frequently attacked identities with toxicity. Models predicted a high likelihood of toxicity for comments containing those identities (e.g. \"gay\"), even when those comments were not actually toxic (such as \"I am a gay woman\"). This happens because training data was pulled from available sources where unfortunately, certain identities are overwhelmingly referred to in offensive ways. Training a model from data with these imbalances risks simply mirroring those biases back to users. In this competition, you're challenged to build a model that recognizes toxicity and minimizes this type of unintended bias with respect to mentions of identities. You'll be using a dataset labeled for identity mentions and optimizing a metric designed to measure unintended bias. Develop strategies to reduce unintended bias in machine learning models, and you'll help the Conversation AI team, and the entire industry, build models that work well for a wide range of conversations. Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive. The Conversation AI team would like to thank Civil Comments for making this dataset available publicly and the Online Hate Index Research Project at D-Lab, University of California, Berkeley, whose labeling survey/instrument informed the dataset labeling. We'd also like to thank everyone who has contributed to Conversation AI's research, especially those who took part in our last competition, the success of which led to the creation of this challenge.",
        "dataset_text": "Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive. UPDATE (Nov 18, 2019): The following files have been added post-competition close to facilitate ongoing research. See the File Description section for details. At the end of 2017 the Civil Comments platform shut down and chose make their ~2m public comments from their platform available in a lasting open archive so that researchers could understand and improve civility in online conversations for years to come. Jigsaw sponsored this effort and extended annotation of this data by human raters for various toxic conversational attributes. In the data supplied for this competition, the text of the individual comment is found in the comment_text column. Each comment in Train has a toxicity label (target), and models should predict the target toxicity for the Test data. This attribute (and all others) are fractional values which represent the fraction of human raters who believed the attribute applied to the given comment. For evaluation, test set examples with target >= 0.5 will be considered to be in the positive class (toxic). The data also has several additional toxicity subtype attributes. Models do not need to predict these attributes for the competition, they are included as an additional avenue for research. Subtype attributes are: Additionally, a subset of comments have been labelled with a variety of identity attributes, representing the identities that are mentioned in the comment. The columns corresponding to identity attributes are listed below. Only identities with more than 500 examples in the test set (combined public and private) will be included in the evaluation calculation. These identities are shown in bold. Note that the data contains different comments that can have the exact same text. Different comments that have the same text may have been labeled with different targets or subgroups. Here are a few examples of comments and their associated toxicity and identity labels. Label values range from 0.0 - 1.0 represented the fraction of raters who believed the label fit the comment. Comment: i'm a white woman in my late 60's and believe me, they are not too crazy about me either!! Comment: Why would you assume that the nurses in this story were women? Comment: Continue to stand strong LGBT community. Yes, indeed, you'll overcome and you have. In addition to the labels described above, the dataset also provides metadata from Jigsaw's annotation: toxicity_annotator_count and identity_annotator_count, and metadata from Civil Comments: created_date, publication_id, parent_id, article_id, rating, funny, wow, sad, likes, disagree. Civil Comments' label rating is the civility rating Civil Comments users gave the comment. To obtain the toxicity labels, each comment was shown to up to 10 annotators*. Annotators were asked to: \"Rate the toxicity of this comment\" These ratings were then aggregated with the target value representing the fraction of annotations that annotations fell within the former two categories. To collect the identity labels, annotators were asked to indicate all identities that were mentioned in the comment. An example question that was asked as part of this annotation effort was: \"What genders are mentioned in the comment?\" Again, these were aggregated into fractional values representing the fraction of raters who said the identity was mentioned in the comment. The distributions of labels and subgroup between Train and Test can be assumed to be similar, but not exact. *Note: Some comments were seen by many more than 10 annotators (up to thousands), due to sampling and strategies used to enforce rater accuracy. Toxicity and identity labeling was done in 2018 on the Figure Eight crowd rating platform, which has since been purchased by Appen. Annotators came from all over the world, and all were proficient in English. Raters were compensated 1.5 cents per judgment, a rate that was set based on two factors: what rates were competitive on the platform and targeting an hourly wage appropriate for raters' locales. Figure Eight offers raters the option to select from multiple tasks with different pay, so rates need to be competitive to attract enough raters. At a rate of 1.5 cents per judgment, enough raters participated to complete this annotation in a few weeks. Following task completion, raters were given a satisfaction survey where the average score for \"Pay\" was 3.8 out of 5, which aligned with Figure Eight's recommendation to target pay to a satisfaction score greater than 3.5. Hourly rates for workers depend on how fast judgements were completed. Most raters will have earned between $0.90/hour (at one comment per minute) to $5.40/hour (at 6 comments per minute), which aligns with typical hourly pay in the geographic regions where most raters are located. Since this dataset was annotated in 2018, more tools have become available to help set rates. Consequently, typical pay for data annotation is increasing globally. The following files were added post-competition close, to use for additional research. Learn more here. This dataset is released under CC0, as is the underlying comment text."
    },
    {
        "name": "JPX Tokyo Stock Exchange Prediction",
        "url": "https://www.kaggle.com/competitions/jpx-tokyo-stock-exchange-prediction",
        "overview_text": "Overview text not found",
        "description_text": "Success in any financial market requires one to identify solid investments. When a stock or derivative is undervalued, it makes sense to buy. If it's overvalued, perhaps it's time to sell. While these finance decisions were historically made manually by professionals, technology has ushered in new opportunities for retail investors. Data scientists, specifically, may be interested to explore quantitative trading, where decisions are executed programmatically based on predictions from trained models. There are plenty of existing quantitative trading efforts used to analyze financial markets and formulate investment strategies. To create and execute such a strategy requires both historical and real-time data, which is difficult to obtain especially for retail investors. This competition will provide financial data for the Japanese market, allowing retail investors to analyze the market to the fullest extent. Japan Exchange Group, Inc. (JPX) is a holding company operating one of the largest stock exchanges in the world, Tokyo Stock Exchange (TSE), and derivatives exchanges Osaka Exchange (OSE) and Tokyo Commodity Exchange (TOCOM). JPX is hosting this competition and is supported by AI technology company AlpacaJapan Co.,Ltd. This competition will compare your models against real future returns after the training phase is complete. The competition will involve building portfolios from the stocks eligible for predictions (around 2,000 stocks). Specifically, each participant ranks the stocks from highest to lowest expected returns and is evaluated on the difference in returns between the top and bottom 200 stocks. You'll have access to financial data from the Japanese market, such as stock information and historical stock prices to train and test your model. All winning models will be made public so that other participants can learn from the outstanding models. Excellent models also may increase the interest in the market among retail investors, including those who want to practice quantitative trading. At the same time, you'll gain your own insights into programmatic investment methods and portfolio analysis\u2015and you may even discover you have an affinity for the Japanese market. ",
        "dataset_text": "This dataset contains historic data for a variety of Japanese stocks and options. Your challenge is to predict the future returns of the stocks. As historic stock prices are not confidential this will be a forecasting competition using the time series API. The data for the public leaderboard period is included as part of the competition dataset. Expect to see many people submitting perfect submissions for fun. Accordingly, the active phase public leaderboard for this competition is intended as a convenience for anyone who wants to test their code. The forecasting phase leaderboard will be determined using real market data gathered after the submission period closes. stock_prices.csv The core file of interest. Includes the daily closing price for each stock and the target column. options.csv Data on the status of a variety of options based on the broader market. Many options include implicit predictions of the future price of the stock market and so may be of interest even though the options are not scored directly. secondary_stock_prices.csv The core dataset contains on the 2,000 most commonly traded equities but many less liquid securities are also traded on the Tokyo market. This file contains data for those securities, which aren't scored but may be of interest for assessing the market as a whole. trades.csv Aggregated summary of trading volumes from the previous business week. financials.csv Results from quarterly earnings reports. stock_list.csv - Mapping between the SecuritiesCode and company names, plus general information about which industry the company is in. data_specifications/ - Definitions for individual columns. jpx_tokyo_market_prediction/ Files that enable the API. Expect the API to deliver all rows in under five minutes and to reserve less than 0.5 GB of memory. Copies of data files exist in multiple folders that cover different time windows and serve different purposes. train_files/ Data folder covering the main training period. supplemental_files/ Data folder containing a dynamic window of supplemental training data. This will be updated with new data during the main phase of the competition in early May, early June, and roughly a week before the submissions are locked. The supplemental data will also be updated once at the very beginning of the forecasting phase so that the test set will start with the trading day after the last trading day in the supplemental data. example_test_files/ Data folder covering the public test period. Intended to facilitate offline testing. Includes the same columns delivered by the API (ie no Target column). You can calculate the Target column from the Close column; it's the return from buying a stock the next day and selling the day after that. This folder also includes an example of the sample submission file that will be delivered by the API."
    },
    {
        "name": "Santander Customer Satisfaction",
        "url": "https://www.kaggle.com/competitions/santander-customer-satisfaction",
        "overview_text": "Overview text not found",
        "description_text": "From frontline support teams to C-suites, customer satisfaction is a key measure of success. Unhappy customers don't stick around. What's more, unhappy customers rarely voice their dissatisfaction before leaving. Santander Bank is asking Kagglers to help them identify dissatisfied customers early in their relationship. Doing so would allow Santander to take proactive steps to improve a customer's happiness before it's too late. In this competition, you'll work with hundreds of anonymized features to predict if a customer is satisfied or dissatisfied with their banking experience.",
        "dataset_text": "You are provided with an anonymized dataset containing a large number of numeric variables. The \"TARGET\" column is the variable to predict. It equals one for unsatisfied customers and 0 for satisfied customers. The task is to predict the probability that each customer in the test set is an unsatisfied customer."
    },
    {
        "name": "Santander Product Recommendation",
        "url": "https://www.kaggle.com/competitions/santander-product-recommendation",
        "overview_text": "Overview text not found",
        "description_text": "Ready to make a downpayment on your first house? Or looking to leverage the equity in the home you have? To support needs for a range of financial decisions, Santander Bank offers a lending hand to their customers through personalized product recommendations.  Under their current system, a small number of Santander\u2019s customers receive many recommendations while many others rarely see any resulting in an uneven customer experience. In their second competition, Santander is challenging Kagglers to predict which products their existing customers will use in the next month based on their past behavior and that of similar customers. With a more effective recommendation system in place, Santander can better meet the individual needs of all customers and ensure their satisfaction no matter where they are in life. Disclaimer: This data set does not include any real Santander Spain's customer, and thus it is not representative of Spain's customer base. ",
        "dataset_text": "In this competition, you are provided with 1.5 years of customers behavior data from Santander bank to predict what new products customers will purchase. The data starts at 2015-01-28 and has monthly records of products a customer has, such as \"credit card\", \"savings account\", etc. You will predict what additional products a customer will get in the last month, 2016-06-28, in addition to what they already have at 2016-05-28. These products are the columns named: ind_(xyz)_ult1, which are the columns #25 - #48 in the training data. You will predict what a customer will buy in addition to what they already had at 2016-05-28.  The test and train sets are split by time, and public and private leaderboard sets are split randomly. Please note: This sample does not include any real Santander Spain customers, and thus it is not representative of Spain's customer base. "
    },
    {
        "name": "Planet: Understanding the Amazon from Space",
        "url": "https://www.kaggle.com/competitions/planet-understanding-the-amazon-from-space",
        "overview_text": "Overview text not found",
        "description_text": " Every minute, the world loses an area of forest the size of 48 football fields. And deforestation in the Amazon Basin accounts for the largest share, contributing to reduced biodiversity, habitat loss, climate change, and other devastating effects. But better data about the location of deforestation and human encroachment on forests can help governments and local stakeholders respond more quickly and effectively. Planet, designer and builder of the world\u2019s largest constellation of Earth-imaging satellites, will soon be collecting daily imagery of the entire land surface of the earth at 3-5 meter resolution. While considerable research has been devoted to tracking changes in forests, it typically depends on coarse-resolution imagery from Landsat (30 meter pixels) or MODIS (250 meter pixels). This limits its effectiveness in areas where small-scale deforestation or forest degradation dominate. Furthermore, these existing methods generally cannot differentiate between human causes of forest loss and natural causes. Higher resolution imagery has already been shown to be exceptionally good at this, but robust methods have not yet been developed for Planet imagery. In this competition, Planet and its Brazilian partner SCCON are challenging Kagglers to label satellite image chips with atmospheric conditions and various classes of land cover/land use. Resulting algorithms will help the global community better understand where, how, and why deforestation happens all over the world - and ultimately how to respond. To dig into/explore more Planet data, sign up for a free account. And if you're interested in building applications on Planet data, check out our Application Developer Program.",
        "dataset_text": "For all the tar.7z files, you can extract them with: 7za x train-jpg.tar.7z tar xf train-jpg.tar The chips for this competition were derived from Planet's full-frame analytic scene products using our 4-band satellites in sun-synchronous orbit (SSO) and International Space Station (ISS) orbit. The set of chips for this competition use the GeoTiff format and each contain four bands of data: red, green, blue, and near infrared. The specific spectral response of the satellites can be found in the Planet documentation. Each of these channels is in 16-bit digital number format, and meets the specification of the Planet four band analytic ortho scene product. For purposes of the competition we have stripped out all of the geotiff information regarding the chip footprint and ground control points (GCPs). The imagery has a ground-sample distance (GSD) of 3.7m and an orthorectified pixel size of 3m. The data comes from Planet's Flock 2 satellites in both sun-synchronous and ISS orbits and was collected between January 1, 2016 and February 1, 2017. All of the scenes come from the Amazon basin which includes Brazil, Peru, Uruguay, Colombia, Venezuela, Guyana, Bolivia, and Ecuador (see map below). We have also included a set of JPG chips for reference and practice. These chips were processed using the Planet visual product processor and then saved as jpg chips. These chips are provided as a reference to the scene content, but we expect that the additional information in the tif chips will be more fruitful for the competition. Above: A map of the Amazon basin.  To assemble this data set we set out with an initial specification of the phenomena we wished to find and include in the final data set. From that initial specification we created a \"wish list\" of scenes where we included a ballpark number of scenes required to get a sufficient number of chips to demonstrate the phenomena. This initial set of scenes was painstakingly collected by our Berlin team using Planet Explorer. All told this initial set of scenes numbered approximately 1600 and covered a land area of thirty million hectares. This initial set of scenes was then processed using a custom product processor to create the jpg and 4-band tif chips. Any chip that did not have a full and complete four band product was omitted. This initial set of over 150,000 chips was then divided into two sets, a \"hard\" and an \"easy\" set. The easy set contained scenes that the Berlin team identified as having easier-to-identify labels like primary rainforest, agriculture, habitation, roads, water, and cloud conditions. The harder set of data was derived from scenes where the Berlin team had selected for shifting cultivation, slash and burn agriculture, blow down, mining, and other phenomenon. The chips were labeled using the Crowd Flower platform and a mixture of crowd-sourced labor and our Berlin and San Francisco teams. While the utmost care was taken to get a large and well-labeled dataset, we are aware that not all of the labels in our dataset are correct. Governments around the world retain a large number of highly trained analysts to review images and even they can't always agree on what is present in a given satellite image. Moreover, the commonly prescribed approach for labeling data in the GIS community is to use actual ground truth data to label scenes, which is both costly and time consuming. With this in mind we do believe our data has a reasonably high signal to noise ratio and is sufficient for training. Given the ease and expediency of crowd labeling, we believe that a large, relatively inexpensive and rapidly labeled dataset is better than a small, more definitive but less diverse dataset. We are interested to see how competitors handle any inaccuracies. The class labels for this task were chosen in collaboration with Planet's Impact team and represent a reasonable subset of phenomena of interest in the Amazon basin. The labels can broadly be broken into three groups: atmospheric conditions, common land cover/land use phenomena, and rare land cover/land use phenomena. Each chip will have one and potentially more than one atmospheric label and zero or more common and rare labels. Chips that are labeled as cloudy should have no other labels, but there may be labeling errors. Above: Sample chips and their labels. As discussed in the data collection portion of this document, the chip labels are inherently noisy due to the labeling process and ambiguity of features, and scenes may either omit class labels or have incorrect class labels. Part of the challenge of this competition is to figure out how to work with noisy data. Clouds are a major challenge for passive satellite imaging, and daily cloud cover and rain showers in the Amazon basin can significantly complicate monitoring in the area. For this reason we have chosen to include a cloud cover label for each chip. These labels closely mirror what one would see in a local weather forecast: clear, partly cloudy, cloudy, and haze. For our purposes haze is defined as any chip where atmospheric clouds are visible but they are not so opaque as to obscure the ground. Clear scenes show no evidence of clouds, and partly cloudy scenes can show opaque cloud cover over any portion of the image. Cloudy images have 90% of the chip obscured by opaque cloud cover.    The common labels in this data set are rainforest, agriculture, rivers, towns/cities, and roads. Examples of each class are given below. The overwhelming majority of the data set is labeled as \"primary\", which is shorthand for primary rainforest, or what is known colloquially as virgin forest. Generally speaking, the \"primary\" label was used for any area that exhibited dense tree cover.This Mongobay article gives a concise description of the difference between primary and secondary rainforest, but distinguishing between the two is difficult solely using satellite imagery. This is particularly true in older \"secondary\" forests. Above: Approximately 25,000 acres of untouched primary rainforest. Rivers, reservoirs, and oxbow lakes are important features of the Amazon basin, and we used the water tag as a catch-all term for these features. Rivers in the Amazon basin often change course and serve as highways deep into the forest. The changing course of these rivers creates new habitat but can also strand endangered Amazon River Dolphins. Above: A larger and slower river with significant sand bars. The brown color comes from significant silt deposits. Above: A small tributary joins a larger river system. The deep brown color of the river is noticeable near the bright sand bars. The habitation class label was used for chips that appeared to contain human homes or buildings. This includes anything from dense urban centers to rural villages along the banks of rivers. Small, single-dwelling habitations are often difficult to spot but usually appear as clumps of a few pixels that are bright white. Above: A larger city in the Amazon basin. Above: A large city. Commercial agriculture, while an important industry, is also a major driver of deforestation in the Amazon. For the purposes of this dataset, agriculture is considered to be any land cleared of trees that is being used for agriculture or range land. More reading on agriculture in the Amazon:  Above: An agricultural area that showing the end state of \"fishbone\" deforestation. Above: A newer agricultural area showing \"fishbone\" deforestation. Roads are important for transportation in the Amazon but they also serve as drivers of deforestation. In particular, \"fishbone\" deforestation often follows new road construction, while smaller logging roads drive selective logging operations. For our data, all types of roads are labeled with a single \"road\" label. Some rivers look very similar to smaller logging roads, and consequently there may be some noise in this label. Analysis of the image using the near infrared band may prove useful in disambiguating the two classes. More information:  Above: classic \"Fishbone\" deforestation following a road. Above: roads snake out of a small town in the Amazon. Shifting cultivation is a subset of agriculture that is very easy to see from space, and occurs in rural areas where individuals and families maintain farm plots for subsistence. This article by MongaBay by MongaBay gives a detailed overview of the practice. This type of agriculture is often found near smaller villages along major rivers, and at the outskirts of agricultural areas. It typically relies on non-mechanized labor, and covers relatively small areas. Above: A zoomed-in area showing cultivation (right side of river) Above: A zoomed-in area showing cultivation and some selective logging. Dark areas indicate recent slash/burn activity Bare ground is a catch-all term used for naturally occuring tree free areas that aren't the result of human activity. Some of these areas occur naturally in the Amazon, while others may be the result from the source scenes containing small regions of biome much similar to the pantanal or cerrado. Above: a naturally occuring bare area in the cerrado. Above: a naturally occuring bare area in the cerrado. Slash-and-burn agriculture can be considered to be a subset of the shifting cultivation label and is used for areas that demonstrate recent burn events. This is to say that the shifting cultivation patches appear to have dark brown or black areas consistent with recent burning.This NASA Earth Observatory article gives a good primer on the practice as does this wikipedia article.\n\nAbove: ground view of slash and burn agriculture. By Alzenir Ferreira de Souza Above: A zoomed-in view of an area with shifting cultivation with evidence of a recent fire. Above: A zoomed-in view of an area with shifting cultivation and evidence of a recent fire. The selective logging label is used to cover the practice of selectively removing high value tree species from the rainforest (such as teak and mahogany). From space this appears as winding dirt roads adjacent to bare brown patches in otherwise primary rain forest. This Mongabay Article covers the details of this process. Global Forest Watch is another great resource for learning about deforestation and logging. Above: The brown lines on the right of this scene are a logging road. Note the small brown dots in the area around the road. Above: A zoomed image of logging roads and selective logging. Above: A zoomed image of logging roads and selective logging. Blooming is a natural phenomenon found in the Amazon where particular species of flowering trees bloom, fruit, and flower at the same time to maximize the chances of cross pollination. These trees are quite large and these events can be seen from space. Planet recently captured a similar event in Panama. Above: a zoomed and contrast enhanced of a bloom event in the Amazon basin. The red arrows point to a few specific trees. The canopies of these trees can be over 30m across (~100ft). There are a number of large conventional mines in the Amazon basin and the number is steadily growning. This label is used to classify large-scale legal mining operations.  Above: A conventional mine in the Amazon. Artisinal mining is a catch-all term for small scale mining operations. Throughout the Amazon, especially at the foothills of the Andes, gold deposits lace the deep, clay soils. Artisanal miners, sometimes working illegally in land designated for conservation, slash through the forest and excavate deep pits near rivers. They pump a mud-water slurry into the river banks, blasting them away so that they can be processed further with mercury - which is used to separate out the gold. The denuded moonscape left behind takes centuries to recover. Above: A zoomed image of an artisanal mine in Peru. Above: A zoomed image of an artisanal mine in Peru. Blow down, also called windthrow, is a naturally occurring phenomenon in the Amazon. Briefly, blow down events occur during microbursts where cold dry air from the Andes settles on top of warm moist air in the rainforest. The colder air punches a hole in the moist warm layer, and sinks down with incredible force and high speed (in excess of 100MPH). These high winds topple the larger rainforest trees, and the resulting open areas are visible from space. The open areas do not stay visible for along as plants in the understory rush in to take advantage of the sunlight. Above: A recent blow down event in the Amazon circled in red. Note the light green of the forest understory and the pattern of tree loss."
    },
    {
        "name": "Santander Value Prediction Challenge",
        "url": "https://www.kaggle.com/competitions/santander-value-prediction-challenge",
        "overview_text": "Overview text not found",
        "description_text": "According to Epsilon research, 80% of customers are more likely to do business with you if you provide personalized service. Banking is no exception. The digitalization of everyday lives means that customers expect services to be delivered in a personalized and timely manner\u2026 and often before they\u00b4ve even realized they need the service. In their 3rd Kaggle competition, Santander Group aims to go a step beyond recognizing that there is a need to provide a customer a financial service and intends to determine the amount or value of the customer's transaction. This means anticipating customer needs in a more concrete, but also simple and personal way. With so many choices for financial services, this need is greater now than ever before. In this competition, Santander Group is asking Kagglers to help them identify the value of transactions for each potential customer. This is a first step that Santander needs to nail in order to personalize their services at scale.",
        "dataset_text": "You are provided with an anonymized dataset containing numeric feature variables, the numeric target column, and a string ID column. The task is to predict the value of target column in the test set."
    },
    {
        "name": "Airbus Ship Detection Challenge",
        "url": "https://www.kaggle.com/competitions/airbus-ship-detection",
        "overview_text": "Overview text not found",
        "description_text": " Airbus is excited to challenge Kagglers to build a model that detects all ships in satellite images as quickly as possible. Can you find them even in imagery with clouds or haze? Here\u2019s the backstory: Shipping traffic is growing fast. More ships increase the chances of infractions at sea like environmentally devastating ship accidents, piracy, illegal fishing, drug trafficking, and illegal cargo movement. This has compelled many organizations, from environmental protection agencies to insurance companies and national government authorities, to have a closer watch over the open seas.  Airbus offers comprehensive maritime monitoring services by building a meaningful solution for wide coverage, fine details, intensive monitoring, premium reactivity and interpretation response. Combining its proprietary-data with highly-trained analysts, they help to support the maritime industry to increase knowledge, anticipate threats, trigger alerts, and improve efficiency at sea. A lot of work has been done over the last 10 years to automatically extract objects from satellite images with significative results but no effective operational effects. Now Airbus is turning to Kagglers to increase the accuracy and speed of automatic ship detection. Algorithm Speed Prize: After the Kaggle challenge is complete, competitors may submit their model via a private Kaggle kernel for a speed evaluation based upon the inference time on over 40.000 images chips (typical size of a full satellite image) to win a special algorithm speed prize.  ",
        "dataset_text": "In this competition, you are required to locate ships in images, and put an aligned bounding box segment around the ships you locate. Many images do not contain ships, and those that do may contain multiple ships. Ships within and across images may differ in size (sometimes significantly) and be located in open sea, at docks, marinas, etc. For this metric, object segments cannot overlap. There were a small percentage of images in both the Train and Test set that had slight overlap of object segments when ships were directly next to each other. Any segments overlaps were removed by setting them to background (i.e., non-ship) encoding. Therefore, some images have a ground truth may be an aligned bounding box with some pixels removed from an edge of the segment. These small adjustments will have a minimal impact on scoring, since the scoring evaluates over increasing overlap thresholds. The train_ship_segmentations.csv file provides the ground truth (in run-length encoding format) for the training images. The sample_submission files contains the images in the test images. Please click on each file / folder in the Data Sources section to get more information about the files."
    },
    {
        "name": "CommonLit Readability Prize",
        "url": "https://www.kaggle.com/competitions/commonlitreadabilityprize",
        "overview_text": "Overview text not found",
        "description_text": "Can machine learning identify the appropriate reading level of a passage of text, and help inspire learning? Reading is an essential skill for academic success. When students have access to engaging passages offering the right level of challenge, they naturally develop reading skills. Currently, most educational texts are matched to readers using traditional readability methods or commercially available formulas. However, each has its issues. Tools like Flesch-Kincaid Grade Level are based on weak proxies of text decoding (i.e., characters or syllables per word) and syntactic complexity (i.e., number or words per sentence). As a result, they lack construct and theoretical validity. At the same time, commercially available formulas, such as Lexile, can be cost-prohibitive, lack suitable validation studies, and suffer from transparency issues when the formula's features aren't publicly available. CommonLit, Inc., is a nonprofit education technology organization serving over 20 million teachers and students with free digital reading and writing lessons for grades 3-12. Together with Georgia State University, an R1 public research university in Atlanta, they are challenging Kagglers to improve readability rating methods. In this competition, you\u2019ll build algorithms to rate the complexity of reading passages for grade 3-12 classroom use. To accomplish this, you'll pair your machine learning skills with a dataset that includes readers from a wide variety of age groups and a large collection of texts taken from various domains. Winning models will be sure to incorporate text cohesion and semantics. If successful, you'll aid administrators, teachers, and students. Literacy curriculum developers and teachers who choose passages will be able to quickly and accurately evaluate works for their classrooms. Plus, these formulas will become more accessible for all. Perhaps most importantly, students will benefit from feedback on the complexity and readability of their work, making it far easier to improve essential reading skills. CommonLit would like to extend a special thanks to Professor Scott Crossley's research team at the Georgia State University Departments of Applied Linguistics and Learning Sciences for their partnership on this project. The organizers would like to thank Schmidt Futures for their advice and support for making this work possible.          ",
        "dataset_text": "In this competition, we're predicting the reading ease of excerpts from literature. We've provided excerpts from several time periods and a wide range of reading ease scores. Note that the test set includes a slightly larger proportion of modern texts (the type of texts we want to generalize to) than the training set. Also note that while licensing information is provided for the public test set (because the associated excerpts are available for display / use), the hidden private test set includes only blank license / legal information. This dataset, the CLEAR Corpus, has now been released in full. You may obtain it from either of the following locations: The full corpus contains an expanded set of fields as well as six readability predictions on each except resulting from this competition. You may read more about the CLEAR Corpus from the following publications:"
    },
    {
        "name": "HuBMAP + HPA - Hacking the Human Body",
        "url": "https://www.kaggle.com/competitions/hubmap-organ-segmentation",
        "overview_text": "Overview text not found",
        "description_text": "When you think of \u201clife hacks,\u201d normally you\u2019d imagine productivity techniques. But how about the kind that helps you understand your body at a molecular level? It may be possible! Researchers must first determine the function and relationships among the 37 trillion cells that make up the human body. A better understanding of our cellular composition could help people live healthier, longer lives. A previous Kaggle competition aimed to annotate cell population neighborhoods that perform an organ\u2019s main physiologic function, also called functional tissue units (FTUs). Manually annotating FTUs (e.g., glomeruli in kidney or alveoli in the lung) is a time-consuming process. In the average kidney, there are over 1 million glomeruli FTUs. While there are existing cell and FTU segmentation methods, we want to push the boundaries by building algorithms that generalize across different organs and are robust across different dataset differences. The Human BioMolecular Atlas Program (HuBMAP) is working to create a Human Reference Atlas at the cellular level. Sponsored by the National Institutes of Health (NIH), HuBMAP and Indiana University\u2019s Cyberinfrastructure for Network Science Center (CNS) have partnered with institutions across the globe for this endeavor. A major partner is the Human Protein Atlas (HPA), a Swedish research program aiming to map the protein expression in human cells, tissues, and organs, funded by the Knut and Alice Wallenberg Foundation. In this competition, you\u2019ll identify and segment functional tissue units (FTUs) across five human organs. You'll build your model using a dataset of tissue section images, with the best submissions segmenting FTUs as accurately as possible. If successful, you'll help accelerate the world\u2019s understanding of the relationships between cell and tissue organization. With a better idea of the relationship of cells, researchers will have more insight into the function of cells that impact human health. Further, the Human Reference Atlas constructed by HuBMAP will be freely available for use by researchers and pharmaceutical companies alike, potentially improving and prolonging human life.",
        "dataset_text": "The goal of this competition is to identify the locations of each functional tissue unit (FTU) in biopsy slides from several different organs. The underlying data includes imagery from different sources prepared with different protocols at a variety of resolutions, reflecting typical challenges for working with medical data. This competition uses data from two different consortia, the Human Protein Atlas (HPA) and Human BioMolecular Atlas Program (HuBMAP). The training dataset consists of data from public HPA data, the public test set is a combination of private HPA data and HuBMAP data, and the private test set contains only HuBMAP data. Adapting models to function properly when presented with data that was prepared using a different protocol will be one of the core challenges of this competition. While this is expected to make the problem more difficult, developing models that generalize is a key goal of this endeavor. This competition uses a hidden test. When your submitted notebook is scored the actual test data (including a full length sample submission) will be made available to your notebook. [train/test].csv Metadata for the train/test set. Only the first few rows of the test set are available for download. sample_submission.csv [train/test]_images/ The images. Expect roughly 550 images in the hidden test set. All HPA images are 3000 x 3000 pixels with a tissue area within the image around 2500 x 2500 pixels. The HuBMAP images range in size from 4500x4500 down to 160x160 pixels. HPA samples were stained with antibodies visualized with 3,3'-diaminobenzidine (DAB) and counterstained with hematoxylin. HuBMAP images were prepared using Periodic acid-Schiff (PAS)/hematoxylin and eosin (H&E) stains. All images used have at least one FTU. All tissue data used in this competition is from healthy donors that pathologists identified as pathologically unremarkable tissue. train_annotations/ The annotations provided in the format of points that define the boundaries of the polygon masks of the FTUs."
    },
    {
        "name": "AMP\u00ae-Parkinson's Disease Progression Prediction",
        "url": "https://www.kaggle.com/competitions/amp-parkinsons-disease-progression-prediction",
        "overview_text": "Overview text not found",
        "description_text": "The goal of this competition is to predict MDS-UPDR scores, which measure progression in patients with Parkinson's disease. The Movement Disorder Society-Sponsored Revision of the Unified Parkinson's Disease Rating Scale (MDS-UPDRS) is a comprehensive assessment of both motor and non-motor symptoms associated with Parkinson's. You will develop a model trained on data of protein and peptide levels over time in subjects with Parkinson\u2019s disease versus normal age-matched control subjects. Your work could help provide important breakthrough information about which molecules change as Parkinson\u2019s disease progresses. Parkinson\u2019s disease (PD) is a disabling brain disorder that affects movements, cognition, sleep, and other normal functions. Unfortunately, there is no current cure\u2014and the disease worsens over time. It's estimated that by 2037, 1.6 million people in the U.S. will have Parkinson\u2019s disease, at an economic cost approaching $80 billion. Research indicates that protein or peptide abnormalities play a key role in the onset and worsening of this disease. Gaining a better understanding of this\u2014with the help of data science\u2014could provide important clues for the development of new pharmacotherapies to slow the progression or cure Parkinson\u2019s disease. Current efforts have resulted in complex clinical and neurobiological data on over 10,000 subjects for broad sharing with the research community. A number of important findings have been published using this data, but clear biomarkers or cures are still lacking. Competition host, the Accelerating Medicines Partnership\u00ae Parkinson\u2019s Disease (AMP\u00aePD), is a public-private partnership between government, industry, and nonprofits that is managed through the Foundation of the National Institutes of Health (FNIH). The Partnership created the AMP PD Knowledge Platform, which includes a deep molecular characterization and longitudinal clinical profiling of Parkinson\u2019s disease patients, with the goal of identifying and validating diagnostic, prognostic, and/or disease progression biomarkers for Parkinson\u2019s disease. Your work could help in the search for a cure for Parkinson\u2019s disease, which would alleviate the substantial suffering and medical care costs of patients with this disease. ",
        "dataset_text": "The goal of this competition is to predict the course of Parkinson's disease (PD) using protein abundance data. The complete set of proteins involved in PD remains an open research question and any proteins that have predictive value are likely worth investigating further. The core of the dataset consists of protein abundance values derived from mass spectrometry readings of cerebrospinal fluid (CSF) samples gathered from several hundred patients. Each patient contributed several samples over the course of multiple years while they also took assessments of PD severity. This is a time-series code competition: you will receive test set data and make predictions with Kaggle's time-series API. train_peptides.csv Mass spectrometry data at the peptide level. Peptides are the component subunits of proteins. train_proteins.csv Protein expression frequencies aggregated from the peptide level data. train_clinical_data.csv supplemental_clinical_data.csv Clinical records without any associated CSF samples. This data is intended to provide additional context about the typical progression of Parkinsons. Uses the same columns as train_clinical_data.csv. example_test_files/ Data intended to illustrate how the API functions. Includes the same columns delivered by the API (ie no updrs columns). amp_pd_peptide/ Files that enable the API. Expect the API to deliver all of the data (less than 1,000 additional patients) in under five minutes and to reserve less than 0.5 GB of memory. A brief demonstration of what the API delivers is available here. public_timeseries_testing_util.py An optional file intended to make it easier to run custom offline API tests. See the script's docstring for details."
    },
    {
        "name": "GoDaddy - Microbusiness Density Forecasting",
        "url": "https://www.kaggle.com/competitions/godaddy-microbusiness-density-forecasting",
        "overview_text": "Overview text not found",
        "description_text": "The goal of this competition is to predict monthly microbusiness density in a given area. You will develop an accurate model trained on U.S. county-level data. Your work will help policymakers gain visibility into microbusinesses, a growing trend of very small entities. Additional information will enable new policies and programs to improve the success and impact of these smallest of businesses. American policy leaders strive to develop economies that are more inclusive and resilient to downturns. They're also aware that with advances in technology, entrepreneurship has never been more accessible than it is today. Whether to create a more appropriate work/life balance, to follow a passion, or due to loss of employment, studies have demonstrated that Americans increasingly choose to create businesses of their own to meet their financial goals. The challenge is that these \"microbusinesses\" are often too small or too new to show up in traditional economic data sources, making it nearly impossible for policymakers to study them. But data science could help fill in the gaps and provide insights into the factors associated these businesses. Over the past few years the Venture Forward team at GoDaddy has worked hard to produce data assets about the tens of millions of microbusinesses in the United States. Microbusinesses are generally defined as businesses with an online presence and ten or fewer employees. GoDaddy has visibility into more than 20 million of them, owned by more than 10 million entrepreneurs. We've surveyed this universe of microbusiness owners for several years and have collected a great deal of information on them that you can access via our survey data here. Current models leverage available internal and census data, use econometric approaches, and focus on understanding primary determinants. While these methods are adequate, there's potential to include additional data and using more advanced approaches to improve predictions and to better inform decision-making. Competition host GoDaddy is the world\u2019s largest services platform for entrepreneurs around the globe. They're on a mission to empower their worldwide community of 20+ million customers\u2014and entrepreneurs everywhere\u2014by giving them all the help and tools they need to grow online. Your work will help better inform policymakers as they strive to make the world a better place for microbusiness entrepreneurs. This will have a real and substantial impact on communities across the country and will help our broader economy adapt to a constantly evolving world.",
        "dataset_text": "Your challenge in this competition is to forecast microbusiness activity across the United States, as measured by the density of microbusinesses in US counties. Microbusinesses are often too small or too new to show up in traditional economic data sources, but microbusiness activity may be correlated with other economic indicators of general interest. As historic economic data are widely available, this is a forecasting competition. The forecasting phase public leaderboard and final private leaderboard will be determined using data gathered after the submission period closes. You will make static forecasts that can only incorporate information available before the end of the submission period. This means that while we will rescore submissions during the forecasting period we will not rerun any notebooks. A great deal of data is publicly available about counties and we have not attempted to gather it all here. You are strongly encouraged to use external data sources for features. train.csv sample_submission.csv A valid sample submission. This file will remain unchanged throughout the competition. test.csv Metadata for the submission rows. This file will remain unchanged throughout the competition. revealed_test.csv During the submission period, only the most recent month of data will be used for the public leaderboard. Any test set data older than that will be published in revealed_test.csv, closely following the usual data release cycle for the microbusiness report. We expect to publish one copy of revealed_test.csv in mid February. This file's schema will match train.csv. census_starter.csv Examples of useful columns from the Census Bureau's American Community Survey (ACS) at data.census.gov. The percentage fields were derived from the raw counts provided by the ACS. All fields have a two year lag to match what information was avaiable at the time a given microbusiness data update was published."
    },
    {
        "name": "ICR - Identifying Age-Related Conditions",
        "url": "https://www.kaggle.com/competitions/icr-identify-age-related-conditions",
        "overview_text": "Overview text not found",
        "description_text": "The goal of this competition is to predict if a person has any of three medical conditions. You are being asked to predict if the person has one or more of any of the three medical conditions (Class 1), or none of the three medical conditions (Class 0). You will create a model trained on measurements of health characteristics. To determine if someone has these medical conditions requires a long and intrusive process to collect information from patients. With predictive models, we can shorten this process and keep patient details private by collecting key characteristics relative to the conditions, then encoding these characteristics. Your work will help researchers discover the relationship between measurements of certain characteristics and potential patient conditions. They say age is just a number but a whole host of health issues come with aging. From heart disease and dementia to hearing loss and arthritis, aging is a risk factor for numerous diseases and complications. The growing field of bioinformatics includes research into interventions that can help slow and reverse biological aging and prevent major age-related ailments. Data science could have a role to play in developing new methods to solve problems with diverse data, even if the number of samples is small. Currently, models like XGBoost and random forest are used to predict medical conditions yet the models' performance is not good enough. Dealing with critical problems where lives are on the line, models need to make correct predictions reliably and consistently between different cases. Founded in 2015, competition host InVitro Cell Research, LLC (ICR) is a privately funded company focused on regenerative and preventive personalized medicine. Their offices and labs in the greater New York City area offer state-of-the-art research space. InVitro Cell Research's Scientists are what set them apart, helping guide and defining their mission of researching how to repair aging people fast. In this competition, you\u2019ll work with measurements of health characteristic data to solve critical problems in bioinformatics. Based on minimal training, you\u2019ll create a model to predict if a person has any of three medical conditions, with an aim to improve on existing methods. You could help advance the growing field of bioinformatics and explore new methods to solve complex problems with diverse data.",
        "dataset_text": "The competition data comprises over fifty anonymized health characteristics linked to three age-related conditions. Your goal is to predict whether a subject has or has not been diagnosed with one of these conditions -- a binary classification problem. Note that this is a Code Competition, in which the actual test set is hidden. In this version, we give some sample data in the correct format to help you author your solutions. When your submission is scored, this example test data will be replaced with the full test set. There are about 400 rows in the full test set."
    },
    {
        "name": "CommonLit - Evaluate Student Summaries",
        "url": "https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries",
        "overview_text": "Overview text not found",
        "description_text": "The goal of this competition is to assess the quality of summaries written by students in grades 3-12. You'll build a model that evaluates how well a student represents the main idea and details of a source text, as well as the clarity, precision, and fluency of the language used in the summary. You'll have access to a collection of real student summaries to train your model. Your work will assist teachers in evaluating the quality of student work and also help learning platforms provide immediate feedback to students. Summary writing is an important skill for learners of all ages. Summarization enhances reading comprehension, particularly among second language learners and students with learning disabilities. Summary writing also promotes critical thinking, and it\u2019s one of the most effective ways to improve writing abilities. However, students rarely have enough opportunities to practice this skill, as evaluating and providing feedback on summaries can be a time-intensive process for teachers. Innovative technology like large language models (LLMs) could help change this, as teachers could employ these solutions to assess summaries quickly. There have been advancements in the automated evaluation of student writing, including automated scoring for argumentative or narrative writing. However, these existing techniques don't translate well to summary writing. Evaluating summaries introduces an added layer of complexity, where models must consider both the student writing and a single, longer source text. Although there are a handful of current techniques for summary evaluation, these models have often focused on assessing automatically-generated summaries rather than real student writing, as there has historically been a lack of these types of datasets. Competition host CommonLit is a nonprofit education technology organization. CommonLit is dedicated to ensuring that all students, especially students in Title I schools, graduate with the reading, writing, communication, and problem-solving skills they need to be successful in college and beyond. The Learning Agency Lab, Vanderbilt University, and Georgia State University join CommonLit in this mission. As a result of your help to develop summary scoring algorithms, teachers and students alike will gain a valuable tool that promotes this fundamental skill. Students will have more opportunities to practice summarization, while simultaneously improving their reading comprehension, critical thinking, and writing abilities. CommonLit, the Learning Agency Lab, Vanderbilt University, and Georgia State University would like to thank the Walton Family Foundation and Schmidt Futures for their support in making this work possible. ",
        "dataset_text": "Update You may read more about Commonlit Summary Data from the following publication: The dataset comprises about 24,000 summaries written by students in grades 3-12 of passages on a variety of topics and genres. These summaries have been assigned scores for both content and wording. The goal of the competition is to predict content and wording scores for summaries on unseen topics. Please note that this is a Code Competition. To help you author your submissions, we provide a some example data in summaries_test.csv and prompts_test.csv in the correct format. When your submission is scored, this example test data will be replaced with the full test set. The full test set comprises about 17,000 summaries from a large number of prompts."
    },
    {
        "name": "The Learning Agency Lab - PII Data Detection",
        "url": "https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data",
        "overview_text": "The goal of this competition is to develop a model that detects personally identifiable information (PII) in student writing. Your efforts to automate the detection and removal of PII from educational data will lower the cost of releasing educational datasets. This will support learning science research and the development of educational tools.",
        "description_text": "In today\u2019s era of abundant educational data from sources such as ed tech, online learning, and research, widespread PII is a key challenge. PII\u2019s presence is a barrier to analyze and create open datasets that advance education because releasing the data publicly puts students at risk. To reduce these risks, it\u2019s crucial to screen and cleanse educational data for PII before public release, which data science could streamline.\nManually reviewing the entire dataset for PII is currently the most reliable screening method, but this results in significant costs and restricts the scalability of educational datasets. While techniques for automatic PII detection that rely on named entity recognition (NER) exist, these work best for PII that share common formatting such as emails and phone numbers. PII detection systems struggle to correctly label names and distinguish between names that are sensitive (e.g., a student's name) and those that are not (e.g., a cited author).\nCompetition host Vanderbilt University is a private research university in Nashville, Tennessee. It offers 70 undergraduate majors and a full range of graduate and professional degrees across 10 schools and colleges, all on a beautiful campus with state-of-the-art laboratories. Vanderbilt is optimized to inspire and nurture cross-disciplinary research that fosters groundbreaking discoveries.\nFor this competition, Vanderbilt has partnered with The Learning Agency Lab, an Arizona-based independent nonprofit focused on developing the science of learning-based tools and programs for the social good.\nYour work in creating reliable automated techniques to detect PII will lead to more high-quality public educational datasets. Researchers can then tap into the potential of this previously unavailable data to develop effective tools and interventions that benefit both teachers and students.",
        "dataset_text": "Update You may read more about PII Data from the following publication: The competition dataset comprises approximately 22,000 essays written by students enrolled in a massively open online course. All of the essays were written in response to a single assignment prompt, which asked students to apply course material to a real-world problem. The goal of the competition is to annotate personally identifiable information (PII) found within the essays. In order to protect student privacy, the original PII in the dataset has been replaced by surrogate identifiers of the same type using a partially automated process. A majority of the essays are reserved for the test set (70%), so competitors are encouraged to use external datasets that are publicly available to bolster the training data. The competition asks competitors to assign labels to the following seven types of PII: The data is presented in JSON format, which includes a document identifier, the full text of the essay, a list of tokens, information about whitespace, and token annotations. The documents were tokenized using the SpaCy English tokenizer. Token labels are presented in BIO (Beginning, Inner, Outer) format. The PII type is prefixed with \u201cB-\u201d when it is the beginning of an entity. If the token is a continuation of an entity, it is prefixed with \u201cI-\u201d. Tokens that are not PII are labeled \u201cO\u201d."
    },
    {
        "name": "Child Mind Institute \u2014 Problematic Internet Use",
        "url": "https://www.kaggle.com/competitions/child-mind-institute-problematic-internet-use",
        "overview_text": "Can you predict the level of problematic internet usage exhibited by children and adolescents, based on their physical activity? The goal of this competition is to develop a predictive model that analyzes children's physical activity and fitness data to identify early signs of problematic internet use. Identifying these patterns can help trigger interventions to encourage healthier digital habits.",
        "description_text": "In today\u2019s digital age, problematic internet use among children and adolescents is a growing concern. Better understanding this issue is crucial for addressing mental health problems such as depression and anxiety. Current methods for measuring problematic internet use in children and adolescents are often complex and require professional assessments. This creates access, cultural, and linguistic barriers for many families. Due to these limitations, problematic internet use is often not measured directly, but is instead associated with issues such as depression and anxiety in youth. Conversely, physical & fitness measures are extremely accessible and widely available with minimal intervention or clinical expertise. Changes in physical habits, such as poorer posture, irregular diet, and reduced physical activity, are common in excessive technology users. We propose using these easily obtainable physical fitness indicators as proxies for identifying problematic internet use, especially in contexts lacking clinical expertise or suitable assessment tools. This competition challenges you to develop a predictive model capable of analyzing children's physical activity data to detect early indicators of problematic internet and technology use. This will enable prompt interventions aimed at promoting healthier digital habits. Your work will contribute to a healthier, happier future where children are better equipped to navigate the digital landscape responsibly. The data used for this competition was provided by the Healthy Brain Network, a landmark mental health study based in New York City that will help children around the world. In the Healthy Brain Network, families, community leaders, and supporters are partnering with the Child Mind Institute to unlock the secrets of the developing brain. In addition to the generous support provided by the Kaggle team, financial support has been provided by the California Department of Health Care Services (DHCS) as part of the Children and Youth Behavioral Health Initiative (CYBHI).  Dell Technologies and NVIDIA are thrilled to partner with the Child Mind Institute, recognizing the profound impact this collaboration will have on advancing mental health support for children and adolescents. This partnership aligns perfectly with our commitment to leveraging technology for social good and fostering a healthier, more inclusive future. Dell Technologies AI solutions from desktop to datacenter to cloud. NVIDIA pioneered accelerated computing to tackle challenges no one else can solve. Our work in AI and digital twins is transforming the world's largest industries and profoundly impacting society.",
        "dataset_text": "The Healthy Brain Network (HBN) dataset is a clinical sample of about five-thousand 5-22 year-olds who have undergone both clinical and research screenings. The objective of the HBN study is to find biological markers that will improve the diagnosis and treatment of mental health and learning disorders from an objective biological perspective. Two elements of this study are being used for this competition: physical activity data (wrist-worn accelerometer data, fitness assessments and questionnaires) and internet usage behavior data. The goal of this competition is to predict from this data a participant's Severity Impairment Index (sii), a standard measure of problematic internet use. Note that this is a Code Competition, in which the actual test set is hidden. In this public version, we give some sample data in the correct format to help you author your solutions. The full test set comprises about 3800 instances. The competition data is compiled into two sources, parquet files containing the accelerometer (actigraphy) series and csv files containing the remaining tabular data. The majority of measures are missing for most participants. In particular, the target sii is missing for a portion of the participants in the training set. You may wish to apply non-supervised learning techniques to this data. The sii value is present for all instances in the test set. The tabular data in train.csv and test.csv comprises measurements from a variety of instruments. The fields within each instrument are described in data_dictionary.csv. These instruments are: Note in particular the field PCIAT-PCIAT_Total. The target sii for this competition is derived from this field as described in the data dictionary: 0 for None, 1 for Mild, 2 for Moderate, and 3 for Severe. Additionally, each participant has been assigned a unique identifier id. During their participation in the HBN study, some participants were given an accelerometer to wear for up to 30 days continually while at home and going about their regular daily lives."
    },
    {
        "name": "HuBMAP - Hacking the Kidney",
        "url": "https://www.kaggle.com/competitions/hubmap-kidney-segmentation",
        "overview_text": "Overview text not found",
        "description_text": "Our best estimates show there are over 7 billion people on the planet and 300 billion stars in the Milky Way galaxy. By comparison, the adult human body contains 37 trillion cells. To determine the function and relationship among these cells is a monumental undertaking. Many areas of human health would be impacted if we better understand cellular activity. A problem with this much data is a great match for the Kaggle community. Just as the Human Genome Project mapped the entirety of human DNA, the Human BioMolecular Atlas Program (HuBMAP) is a major endeavor. Sponsored by the National Institutes of Health (NIH), HuBMAP is working to catalyze the development of a framework for mapping the human body at a level of glomeruli functional tissue units for the first time in history. Hoping to become one of the world\u2019s largest collaborative biological projects, HuBMAP aims to be an open map of the human body at the cellular level. This competition, \u201cHacking the Kidney,\" starts by mapping the human kidney at single cell resolution. Your challenge is to detect functional tissue units (FTUs) across different tissue preparation pipelines. An FTU is defined as a \u201cthree-dimensional block of cells centered around a capillary, such that each cell in this block is within diffusion distance from any other cell in the same block\u201d (de Bono, 2013). The goal of this competition is the implementation of a successful and robust glomeruli FTU detector. You will also have the opportunity to present your findings to a panel of judges for additional consideration. Successful submissions will construct the tools, resources, and cell atlases needed to determine how the relationships between cells can affect the health of an individual. Advancements in HuBMAP will accelerate the world\u2019s understanding of the relationships between cell and tissue organization and function and human health. These datasets and insights can be used by researchers in cell and tissue anatomy, pharmaceutical companies to develop therapies, or even parents to show their children the magnitude of the human body.",
        "dataset_text": "The HuBMAP data used in this hackathon includes 11 fresh frozen and 9 Formalin Fixed Paraffin Embedded (FFPE) PAS kidney images. Glomeruli FTU annotations exist for all 20 tissue samples; some of these will be shared for training, and others will be used to judge submissions. There are over 600,000 glomeruli in each human kidney (Nyengaard, 1992). Normal glomeruli typically range from 100-350\u03bcm in diameter with a roughly spherical shape (Kannan, 2019). Teams are invited to develop segmentation algorithms that identify glomeruli in the PAS stained microscopy data. They are welcome to use other external data and/or pre-trained machine learning models in support of FTU segmentation. All data and all code used must be released under Attribution 4.0 International (CC BY 4.0). The dataset is comprised of very large (>500MB - 5GB) TIFF files. The training set has 8, and the public test set has 5. The private test set is larger than the public test set. The training set includes annotations in both RLE-encoded and unencoded (JSON) forms. The annotations denote segmentations of glomeruli. Both the training and public test sets also include anatomical structure segmentations. They are intended to help you identify the various parts of the tissue. The JSON files are structured as follows, with each feature having: Note that the objects themselves do NOT have unique IDs. The expected prediction for a given image is an RLE-encoded mask containing ALL objects in the image. The mask, as mentioned in the Evaluation page, should be binary when encoded - with 0 indicating the lack of a masked pixel, and 1 indicating a masked pixel. train.csv contains the unique IDs for each image, as well as an RLE-encoded representation of the mask for the objects in the image. See the evaluation tab for details of the RLE encoding scheme. HuBMAP-20-dataset_information.csv contains additional information (including anonymized patient data) about each image."
    },
    {
        "name": "OSIC Pulmonary Fibrosis Progression",
        "url": "https://www.kaggle.com/competitions/osic-pulmonary-fibrosis-progression",
        "overview_text": "Overview text not found",
        "description_text": " Imagine one day, your breathing became consistently labored and shallow. Months later you were finally diagnosed with pulmonary fibrosis, a disorder with no known cause and no known cure, created by scarring of the lungs. If that happened to you, you would want to know your prognosis. That\u2019s where a troubling disease becomes frightening for the patient: outcomes can range from long-term stability to rapid deterioration, but doctors aren\u2019t easily able to tell where an individual may fall on that spectrum. Your help, and data science, may be able to aid in this prediction, which would dramatically help both patients and clinicians.   Current methods make fibrotic lung diseases difficult to treat, even with access to a chest CT scan. In addition, the wide range of varied prognoses create issues organizing clinical trials. Finally, patients suffer extreme anxiety\u2014in addition to fibrosis-related symptoms\u2014from the disease\u2019s opaque path of progression.  Open Source Imaging Consortium (OSIC) is a not-for-profit, co-operative effort between academia, industry and philanthropy. The group enables rapid advances in the fight against Idiopathic Pulmonary Fibrosis (IPF), fibrosing interstitial lung diseases (ILDs), and other respiratory diseases, including emphysematous conditions. Its mission is to bring together radiologists, clinicians and computational scientists from around the world to improve imaging-based treatments.  In this competition, you\u2019ll predict a patient\u2019s severity of decline in lung function based on a CT scan of their lungs. You\u2019ll determine lung function based on output from a spirometer, which measures the volume of air inhaled and exhaled. The challenge is to use machine learning techniques to make a prediction with the image, metadata, and baseline FVC as input.  If successful, patients and their families would better understand their prognosis when they are first diagnosed with this incurable lung disease. Improved severity detection would also positively impact treatment trial design and accelerate the clinical development of novel treatments. ",
        "dataset_text": "The aim of this competition is to predict a patient\u2019s severity of decline in lung function based on a CT scan of their lungs. Lung function is assessed based on output from a spirometer, which measures the forced vital capacity (FVC), i.e. the volume of air exhaled. In the dataset, you are provided with a baseline chest CT scan and associated clinical information for a set of patients. A patient has an image acquired at time Week = 0 and has numerous follow up visits over the course of approximately 1-2 years, at which time their FVC is measured. There are around 200 cases in the public & private test sets, combined. This is split roughly 15-85 between public-private. Since this is real medical data, you will notice the relative timing of FVC measurements varies widely. The timing of the initial measurement relative to the CT scan and the duration to the forecasted time points may be different for each patient. This is considered part of the challenge of the competition. To avoid potential leakage in the timing of follow up visits, you are asked to predict every patient's FVC measurement for every possible week. Those weeks which are not in the final three visits are ignored in scoring. This is a synchronous rerun code competition. The provided test set is a small representative set of files (copied from the training set) to demonstrate the format of the private test set. When you submit your notebook, Kaggle will rerun your code on the test set, which contains unseen images."
    },
    {
        "name": "Feedback Prize - Predicting Effective Arguments",
        "url": "https://www.kaggle.com/competitions/feedback-prize-effectiveness",
        "overview_text": "Overview text not found",
        "description_text": "The goal of this competition is to classify argumentative elements in student writing as \"effective,\" \"adequate,\" or \"ineffective.\" You will create a model trained on data that is representative of the 6th-12th grade population in the United States in order to minimize bias. Models derived from this competition will help pave the way for students to receive enhanced feedback on their argumentative writing. With automated guidance, students can complete more assignments and ultimately become more confident, proficient writers. This competition will comprise two tracks. The first track will be a traditional track in which accuracy of classification will be the only metric used for success. Success on this track will be updated on the Kaggle leaderboard. Prize money for the accuracy-only, \u201cLeaderboard Prize\u201d track will be $25,000. The second track will measure computational efficiency in which efficiency is determined using a combination of accuracy and the speed at which models are able to generate these predictions. We are hosting this track because highly accurate models are often computationally heavy. Such models have a stronger carbon footprint and frequently prove difficult to utilize in real-world educational contexts, since most educational organizations have limited computational capabilities. Weekly updates on models based on computational efficiency will be posted in the discussion forum. Prize money for the computational, \u201cEfficiency Prize\u201d track will be $30,000. You can find more details about the Efficiency Prize Evaluation via the side tab. Writing is crucial for success. In particular, argumentative writing fosters critical thinking and civic engagement skills, and can be strengthened by practice. However, only 13 percent of eighth-grade teachers ask their students to write persuasively each week. Additionally, resource constraints disproportionately impact Black and Hispanic students, so they are more likely to write at the \u201cbelow basic\u201d level as compared to their white peers. An automated feedback tool is one way to make it easier for teachers to grade writing tasks assigned to their students that will also improve their writing skills. There are numerous automated writing feedback tools currently available, but they all have limitations, especially with argumentative writing. Existing tools often fail to evaluate the quality of argumentative elements, such as organization, evidence, and idea development. Most importantly, many of these writing tools are inaccessible to educators due to their cost, which most impacts already underserved schools. Georgia State University (GSU) is an undergraduate and graduate urban public research institution in Atlanta. U.S. News & World Report ranked GSU as one of the most innovative universities in the nation. GSU awards more bachelor\u2019s degrees to African-Americans than any other non-profit college or university in the country. GSU and The Learning Agency Lab, an independent nonprofit based in Arizona, are focused on developing science of learning-based tools and programs for social good. To best prepare all students, GSU and The Learning Agency Lab have joined forces to encourage data scientists to improve automated writing assessments. This public effort could also encourage higher quality and more accessible automated writing tools. If successful, students will receive more feedback on the argumentative elements of their writing and will apply the skill across many disciplines. Georgia State University and the Learning Agency Lab would like to thank the Bill & Melinda Gates Foundation, Schmidt Futures, and Chan Zuckerberg Initiative for their support in making this work possible.                           ",
        "dataset_text": "Update You may read more about Feedback Prize 2.0 data from the following publication: The dataset presented here contains argumentative essays written by U.S students in grades 6-12. These essays were annotated by expert raters for discourse elements commonly found in argumentative writing: Your task is to predict the quality rating of each discourse element. Human readers rated each rhetorical or argumentative element, in order of increasing quality, as one of: For more information on the annotation scheme and scoring rubric, please see: Argumentation Annotation Scheme and Descriptions. Note that this is a Code Competition, in which you will submit code that will be run against an unseen test set. The unseen test set comprises about 3,000 essays. A small public test sample has been provided for testing your submission notebooks. This dataset is a subset of the dataset from the Feedback Prize - Evaluating Student Writing competition. You are welcome to make use of this earlier dataset, if you like. The training set consist of a .csv file containing the annotated discourse elements each essay, including the quality ratings, together with .txt files containing the full text of each essay. It is important to note that some parts of the essays will be unannotated (i.e., they do not fit into one of the classifications above) and they will lack a quality rating. We do not include the unannotated parts in train.csv. To help you author submission code, we include a few example instances selected from the test set. When you submit your notebook for scoring, this example data will be replaced by the actual test data, including the sample_submission.csv file."
    },
    {
        "name": "Feedback Prize - English Language Learning",
        "url": "https://www.kaggle.com/competitions/feedback-prize-english-language-learning",
        "overview_text": "Overview text not found",
        "description_text": "The goal of this competition is to assess the language proficiency of 8th-12th grade English Language Learners (ELLs). Utilizing a dataset of essays written by ELLs will help to develop proficiency models that better supports all students. Your work will help ELLs receive more accurate feedback on their language development and expedite the grading cycle for teachers. These outcomes could enable ELLs to receive more appropriate learning tasks that will help them improve their English language proficiency. Writing is a foundational skill. Sadly, it's one few students are able to hone, often because writing tasks are infrequently assigned in school. A rapidly growing student population, students learning English as a second language, known as English Language Learners (ELLs), are especially affected by the lack of practice. While automated feedback tools make it easier for teachers to assign more writing tasks, they are not designed with ELLs in mind. Existing tools are unable to provide feedback based on the language proficiency of the student, resulting in a final evaluation that may be skewed against the learner. Data science may be able to improve automated feedback tools to better support the unique needs of these learners. Competition host Vanderbilt University is a private research university in Nashville, Tennessee. It offers 70 undergraduate majors and a full range of graduate and professional degrees across 10 schools and colleges, all on a beautiful campus\u2014an accredited arboretum\u2014complete with athletic facilities and state-of-the-art laboratories. Vanderbilt is optimized to inspire and nurture cross-disciplinary research that fosters discoveries that have global impact. Vanderbilt and co-host, The Learning Agency Lab, an independent nonprofit based in Arizona, are focused on developing science of learning-based tools and programs for social good. Vanderbilt and The Learning Agency Lab have partnered together to offer data scientists the opportunity to support ELLs using data science skills in machine learning, natural language processing, and educational data analytics. You can improve automated feedback tools for ELLs by sensitizing them to language proficiency. The resulting tools could serve teachers by alleviating the grading burden and support ELLs by ensuring their work is evaluated within the context of their current language level. Vanderbilt University and the Learning Agency Lab would like to thank the Bill & Melinda Gates Foundation, Schmidt Futures, and Chan Zuckerberg Initiative for their support in making this work possible.                           ",
        "dataset_text": "Update You may read more about Feedback Prize 3.0 data from the following publication: The dataset presented here (the ELLIPSE corpus) comprises argumentative essays written by 8th-12th grade English Language Learners (ELLs). The essays have been scored according to six analytic measures: cohesion, syntax, vocabulary, phraseology, grammar, and conventions. Each measure represents a component of proficiency in essay writing, with greater scores corresponding to greater proficiency in that measure. The scores range from 1.0 to 5.0 in increments of 0.5. Your task is to predict the score of each of the six measures for the essays given in the test set. Some of these essays have appeared in the datasets for the Feedback Prize - Evaluating Student Writing and Feedback Prize - Predicting Effective Arguments competitions. You are welcome to make use of these earlier datasets in this competition. Please note that this is a Code Competition. We give a few sample essays in test.csv to help you author your solutions. When your submission is scored, this example test data will be replaced with the full test set. The full test set comprises about 2700 essays."
    },
    {
        "name": "Learning Equality - Curriculum Recommendations",
        "url": "https://www.kaggle.com/competitions/learning-equality-curriculum-recommendations",
        "overview_text": "Overview text not found",
        "description_text": "The goal of this competition is to streamline the process of matching educational content to specific topics in a curriculum. You will develop an accurate and efficient model trained on a library of K-12 educational materials that have been organized into a variety of topic taxonomies. These materials are in diverse languages, and cover a wide range of topics, particularly in STEM (Science, Technology, Engineering, and Mathematics). Your work will enable students and educators to more readily access relevant educational content to support and supplement learning. (opens in a new tab)\"> Every country in the world has its own educational structure and learning objectives. Most materials are categorized against a single national system or are not organized in a way that facilitates discovery. The process of curriculum alignment, the organization of educational resources to fit standards, is challenging as it varies between country contexts. Current efforts to align digital materials to national curricula are manual and require time, resources, and curricular expertise, and the process needs to be made more efficient in order to be scalable and sustainable. As new materials become available, they require additional efforts to be realigned, resulting in a never-ending process. There are no current algorithms or other AI interventions that address the resource constraints associated with improving the process of curriculum alignment. Competition host Learning Equality is committed to enabling every person in the world to realize their right to a quality education, by supporting the creation, adaptation, and distribution of open educational resources, and creating supportive tools for innovative pedagogy. Their core product is Kolibri, an adaptable set of open solutions and tools specially designed to support offline-first teaching and learning for the 37% of the world without Internet access. Their close partner UNHCR has consistently highlighted the strong need and innovation required to create automated alignment tools to ensure refugee learners and teachers are provided with relevant digital learning resources. They have been jointly exploring this challenge in depth for the past few years, engaging with curriculum designers, teachers, and machine learning experts. In addition, Learning Equality is partnering with The Learning Agency Lab, an independent nonprofit focused on developing science of learning-based tools and programs for social good, along with UNHCR to engage you in this important process. You have the opportunity to use your skills in machine learning to support educators and students around the world in accessing aligned learning materials that are relevant for their particular context. Better curriculum alignment processes are especially impactful during the onset of new emergencies or crises, where rapid support is needed, such as for refugee learners, and during school closures as took place during COVID-19. Learning Equality and the Learning Agency Lab would like to thank Schmidt Futures and UNHCR for making this work possible. They also extend their gratitude to UNHCR for the ongoing collaboration in this effort to automate the process of curriculum alignment.                           ",
        "dataset_text": "The dataset presented here is drawn from the Kolibri Studio curricular alignment tool, in which users can create their own channel, then build out a topic tree that represents a curriculum taxonomy or other hierarchical structure, and finally organize content items into these topics, by uploading their own content and/or importing existing materials from the Kolibri Content Library of Open Educational Resources. An example of a branch of a topic tree is: Secondary Education >> Ordinary Level >> Mathematics >> Further Learning >> Activities >> Trigonometry. The leaf topic in this branch might then contain (be correlated with) a content item such as a video entitled Polar Coordinates. You are challenged to predict which content items are best aligned to a given topic in a topic tree, with the goal of matching the selections made by curricular experts and other users of the Kolibri Studio platform. In other words, your goal is to recommend content items to curators for potential inclusion in a topic, to reduce the time they spend searching for and discovering relevant content to consider including in each topic. Please note that this is a Code Competition, in which the actual test set is hidden. In this public version, we give some sample data drawn from the training set to help you author your solutions. When your submission is scored, this example test data will be replaced with the full test set. The full test set includes an additional 10,000 topics (none present in the training set) and a large number of additional content items. The additional content items are only correlated to test set topics. The training set consists of a corpus of topic trees from within the Kolibri Content Library, along with additional non-public aligned channels, and supplementary channels with less granular or lower-quality alignment."
    },
    {
        "name": "Lux AI Season 2",
        "url": "https://www.kaggle.com/competitions/lux-ai-season-2",
        "overview_text": "Overview text not found",
        "description_text": "As the sun set on the world an array of lights dotted the once dark horizon. With the help of a brigade of toads, Lux had made it past the terrors in the night to see the dawn of a new age. Seeking new challenges, plans were made to send a forward force with one mission: terraform Mars! Welcome to the Lux AI Challenge Season 2! The Lux AI Challenge is a competition where competitors design agents to tackle a multi-variable optimization, resource gathering, and allocation problem in a 1v1 scenario against other competitors. In addition to optimization, successful agents must be capable of analyzing their opponents and developing appropriate policies to get the upper hand. All code can be found at our Github, make sure to give it a star while you are there! Make sure to join our community discord to chat, strategize, and learn with other competitors! We will be posting announcements on the Kaggle Forums and on the discord.",
        "dataset_text": "This is the folder for the Python kit. Please make sure to read the instructions as they are important regarding how you will write a bot and submit it to the competition."
    },
    {
        "name": "Predict Student Performance from Game Play",
        "url": "https://www.kaggle.com/competitions/predict-student-performance-from-game-play",
        "overview_text": "Overview text not found",
        "description_text": "The goal of this competition is to predict student performance during game-based learning in real-time. You'll develop a model trained on one of the largest open datasets of game logs.\n\nYour work will help advance research into knowledge-tracing methods for game-based learning. You'll be supporting developers of educational games to create more effective learning experiences for students. Learning is meant to be fun, which is where game-based learning comes in. This educational approach allows students to engage with educational content inside a game framework, making it enjoyable and dynamic. Although game-based learning is being used in a growing number of educational settings, there are still a limited number of open datasets available to apply data science and learning analytic principles to improve game-based learning. Most game-based learning platforms do not sufficiently make use of knowledge tracing to support individual students. Knowledge tracing methods have been developed and studied in the context of online learning environments and intelligent tutoring systems. But there has been less focus on knowledge tracing in educational games. Competition host Field Day Lab is a publicly-funded research lab at the Wisconsin Center for Educational Research. They design games for many subjects and age groups that bring contemporary research to the public, making use of the game data to understand how people learn. Field Day Lab's commitment to accessibility ensures all of its games are free and available to anyone. The lab also partners with nonprofits like The Learning Agency Lab, which is focused on developing science of learning-based tools and programs for the social good. If successful, you'll enable game developers to improve educational games and further support the educators who use these games with dashboards and analytic tools. In turn, we might see broader support for game-based learning platforms. Field Day Lab and the Learning Agency Lab would like to thank the Walton Family Foundation and Schmidt Futures for making this work possible. ",
        "dataset_text": "This competition uses the Kaggle's time series API. Test data will be delivered in groupings that do not allow access to future data. The objective of this competition is to use time series data generated by an online educational game to determine whether players will answer questions correctly. There are three question checkpoints (level 4, level 12, and level 22), each with a number of questions. At each checkpoint, you will have access to all previous test data for that section. You have access to the training data and labels. There are 18 questions for each sessions - you are not given the answers, but are simply told whether the user for a particular session answered each question correctly. When you are ready to predict, use the sample notebook to iterate over the test data, which is split as described above and served up as Pandas dataframes. Make your predictions for each group of questions - at the end of this process a submission.csv file will have been created for you. Simply submit your notebook. The training columns are as listed below. The label rows are identified with <session_id>_<question #>. Each session will have 18 rows, representing 18 questions. For each <session_id>_<question #>, you are predicting the correct column, identifying whether you believe the user for this particular session will answer this question correctly, using only the previous information for the session. The timeseries API presents the questions and data to you in order of levels - level segments 0-4, 5-12, and 13-22 are each provided in sequence, and you will be predicting the correctness of each segment's questions as they are presented. Note that the hidden test set is roughly as large as the training set; you should expect it will take much longer to run on than the three test samples provided. This competition also includes an efficiency prize, which will be assessed at intervals throughout the competition. Check out the Efficiency Prize Evaluation page for more details. Note: this competition is aimed at producing models that are small and lightweight. We have introduced compute constraints to match - your VMs will have only 2 CPUs, 8GB of RAM, and no GPU available. You will still have a maximum of 9 hours to complete the task, but between the constraints and the efficiency prize there will be some interesting sub-problems to solve. Good luck!"
    },
    {
        "name": "Linking Writing Processes to Writing Quality",
        "url": "https://www.kaggle.com/competitions/linking-writing-processes-to-writing-quality",
        "overview_text": "The goal of this competition is to predict overall writing quality. Does typing behavior affect the outcome of an essay? You will develop a model trained on a large dataset of keystroke logs that have captured writing process features.",
        "description_text": "It\u2019s difficult to summarize the complex set of behavioral actions and cognitive activities in the writing process. Writers may use different techniques to plan and revise their work, demonstrate distinct pause patterns, or allocate time strategically throughout the writing process. Many of these small actions may influence writing quality. Even so, most writing assessments focus on only the final product. Data science may be able to uncover key aspects of the writing process.\nPast research explored a number of process features related to behaviors such as pausing, additions or deletions, and revisions. However, previous studies have used relatively small datasets. Additionally, only a small number of process features have been studied.\nCompetition host Vanderbilt University is a private research university in Nashville, Tennessee. It offers 70 undergraduate majors and a full range of graduate and professional degrees across 10 schools and colleges, all on a beautiful campus\u2014an accredited arboretum\u2014complete with athletic facilities and state-of-the-art laboratories. Together with The Learning Agency Lab, an independent nonprofit based in Arizona, Vanderbilt is optimized to inspire and nurture cross-disciplinary research that fosters discoveries that have global impact.\nYour work in this competition will use process features from keystroke log data to predict overall writing quality. These efforts may identify relationships between learners\u2019 writing behaviors and writing performance. Additionally, given that most current writing assessment tools mainly focus on the final written products, this may help direct learners\u2019 attention to their text production process and boost their autonomy, metacognitive awareness, and self-regulation in writing. Vanderbilt University and the Learning Agency Lab would like to thank the Bill & Melinda Gates Foundation, Schmidt Futures, and Chan Zuckerberg Initiative for their support in making this work possible.\n                         ",
        "dataset_text": "The competition dataset comprises about 5000 logs of user inputs, such as keystrokes and mouse clicks, taken during the composition of an essay. Each essay was scored on a scale of 0 to 6. Your goal is to predict the score an essay received from its log of user inputs. You can find more information about the competition dataset on the page documenting the Data Collection Procedure. Note that there may be events in the test set that do not occur in the training set. Your solution should be robust to unseen events. Note: Key_down and key_up events may not necessarily occur in the same order as they are presented in the dataset. To illustrate, a writer may press down \"a\" and then press down \"b\" before he/she even releases \"a\". However, all the keystroke information about \"a\" comes before \"b\" in the dataframe. Please note that this is a Code Competition. We give a few example logs in test_logs.csv to help you author your solutions. When your submission is scored, this example test data will be replaced with the full test set. There are logs for about 2500 essays in the test set."
    },
    {
        "name": "Eedi - Mining Misconceptions in Mathematics",
        "url": "https://www.kaggle.com/competitions/eedi-mining-misconceptions-in-mathematics",
        "overview_text": "In this competition, you\u2019ll develop an NLP model driven by ML to accurately predict the affinity between misconceptions and incorrect answers (distractors) in multiple-choice questions. This solution will suggest candidate misconceptions for distractors, making it easier for expert human teachers to tag distractors with misconceptions.",
        "description_text": "A Diagnostic Question is a multiple-choice question with four options: one correct answer and three distractors (incorrect answers). Each distractor is carefully crafted to capture a specific misconception. For example:  If a student selects the distractor \"13,\" they may have the misconception \"Carries out operations from left to right regardless of priority order.\" Tagging distractors with appropriate misconceptions is essential but time-consuming, and it is difficult to maintain consistency across multiple human labellers. Misconceptions vary significantly in terms of description granularity, and new misconceptions are often discovered as human labellers tag distractors in new topic areas. Initial efforts to use pre-trained language models have not been successful, likely due to the complexity of the mathematical content in the questions. Therefore, a more efficient and consistent approach is needed to streamline the tagging process and enhance the overall quality. This competition challenges you to develop a Natural Language Processing (NLP) model driven by Machine Learning (ML) that predicts the affinity between misconceptions and distractors. The goal is to create a model that not only aligns with known misconceptions but also generalizes to new, emerging misconceptions. Such a model would assist human labelers in accurately selecting suitable misconceptions from both existing and newly identified options. Your work could help improve the understanding and management of misconceptions, enhancing the educational experience for both students and teachers. Eedi, alongside Vanderbilt University, and together with The Learning Agency Lab, an independent nonprofit based in Arizona, have collaborated with Kaggle on this competition. Eedi, Vanderbilt University and the Learning Agency Lab would like to thank the Bill & Melinda Gates Foundation, Schmidt Futures, and Chan Zuckerberg Initiative for their support in making this work possible.   ",
        "dataset_text": "On Eedi, students answer Diagnostic Questions (DQs), which are multiple-choice questions featuring one correct answer and three incorrect answers, known as distractors. Each question targets a specific construct (also referred to as a skill), representing the most granular level of knowledge relevant to the question. Each distractor is designed to correspond with a potential misconception. Below is an example of a DQ:  In this example, the options for the question are labeled with misconceptions as follows: The Diagnostic Questions were originally presented in image format, and the text, including mathematical content, has been extracted using a human-in-the-loop OCR process. Please note that this is a Code Competition. The hidden test set has approximately 1,000 questions."
    },
    {
        "name": "Bengali.AI Speech Recognition",
        "url": "https://www.kaggle.com/competitions/bengaliai-speech",
        "overview_text": "Overview text not found",
        "description_text": "The goal of this competition is to recognize Bengali speech from out-of-distribution audio recordings. You will build a model trained on the first Massively Crowdsourced (MaCro) Bengali speech dataset with 1,200 hours of data from ~24,000 people from India and Bangladesh. The test set contains samples from 17 different domains that are not present in training. Your efforts could improve Bengali speech recognition using the first Bengali out-of-distribution speech recognition dataset. In addition, your submission will be among the first open-source speech recognition methods for Bengali. Bengali is one of the most spoken languages in the world, with approximately 340 million native and second-language speakers globally. With that comes diversity in dialects and prosodic features (combinations of sounds). For example, Muslim religious sermons in Bengali are often delivered with a pace and tonality that is significantly different from regular speech. Such \u2018shifts\u2019 can be challenging even for commercially available speech recognition methods (the Google Speech API for Bengali has a Word Error Rate of 74% for Bengali religious sermons).  There are no robust open-source speech recognition models for Bengali currently, though your data science skills could certainly help change that. In particular, out-of-distribution generalization is a common machine learning problem. When test and training data are similar, they\u2019re in-distribution. To account for Bengali\u2019s diversity, this competition\u2019s data is intentionally out-of-distribution, with the challenge to improve results.. Competition host Bengali.AI is a non-profit community initiative working to accelerate language technology research for Bengali (known locally as Bangla). Bengali.AI crowdsources large-scale datasets through community-driven collection campaigns and crowdsource solutions for their datasets through research competitions. All the outcomes from Bengali.AI's two-pronged approach, including datasets and trained models, are open-sourced for public use. Your work in this competition could have an impact beyond speech recognition improvements for one of the world's most popular, yet low-resource languages. You could also provide a much-needed push towards solving one of speech recognition's major challenges, out-of-distribution generalization. We specially thank our collaborators from Aspire to Innovate (a2i) program by the Govt. Bangladesh, Bangladesh University of Engineering and Technology (BUET), and Shahjalal University of Science and Technology (SUST).",
        "dataset_text": "The competition dataset comprises about 1200 hours of recordings of Bengali speech. Your goal is to transcribe recordings of speech that is out-of-distribution with respect to the training set. Note that this is a Code Competition, in which the actual test set is hidden. In this public version, we give some sample data in the correct format to help you author your solutions. The full test set contains about 20 hours of speech in almost 8000 MP3 audio files. All of the files in the test set are encoded at a sample rate of 32k, a bit rate of 48k, in one channel. The test set annotations were normalized with the bnUnicodeNormalizer. Details on the dataset are available in the dataset paper: https://arxiv.org/abs/2305.09688 Citation:"
    },
    {
        "name": "LANL Earthquake Prediction",
        "url": "https://www.kaggle.com/competitions/LANL-Earthquake-Prediction",
        "overview_text": "Overview text not found",
        "description_text": "Forecasting earthquakes is one of the most important problems in Earth science because of their devastating consequences. Current scientific studies related to earthquake forecasting focus on three key points: when the event will occur, where it will occur, and how large it will be. In this competition, you will address when the earthquake will take place. Specifically, you\u2019ll predict the time remaining before laboratory earthquakes occur from real-time seismic data. If this challenge is solved and the physics are ultimately shown to scale from the laboratory to the field, researchers will have the potential to improve earthquake hazard assessments that could save lives and billions of dollars in infrastructure. This challenge is hosted by Los Alamos National Laboratory which enhances national security by ensuring the safety of the U.S. nuclear stockpile, developing technologies to reduce threats from weapons of mass destruction, and solving problems related to energy, environment, infrastructure, health, and global security concerns. Acknowledgments:",
        "dataset_text": "The goal of this competition is to use seismic signals to predict the timing of laboratory earthquakes. The data comes from a well-known experimental set-up used to study earthquake physics. The acoustic_data input signal is used to predict the time remaining before the next laboratory earthquake (time_to_failure). The training data is a single, continuous segment of experimental data. The test data consists of a folder containing many small segments. The data within each test file is continuous, but the test files do not represent a continuous segment of the experiment; thus, the predictions cannot be assumed to follow the same regular pattern seen in the training file. For each seg_id in the test folder, you should predict a single time_to_failure corresponding to the time between the last row of the segment and the next laboratory earthquake."
    },
    {
        "name": "Google Universal Image Embedding",
        "url": "https://www.kaggle.com/competitions/google-universal-image-embedding",
        "overview_text": "Overview text not found",
        "description_text": "Welcome to the Universal Image Embedding competition! After hosting challenges in the domain of landmarks for the past four years, this year we introduce the first competition in image representations that should work across many object types. Image representations are a critical building block of computer vision applications. Traditionally, research on image embedding learning has been conducted with a focus on per-domain models. Generally, papers propose generic embedding learning techniques which are applied to different domains separately, rather than developing generic embedding models which could be applied to all domains combined. In this competition, the developed models are expected to retrieve relevant database images to a given query image (ie, the model should retrieve database images containing the same object as the query). The images in our dataset comprise a variety of object types, such as apparel, artwork, landmarks, furniture, packaged goods, among others. This year's competition is structured in a representation learning format: you will create a model that extracts a feature embedding for the images and submit the model via Kaggle Notebooks. Kaggle will run your model on a held-out test set, perform a k-nearest-neighbors lookup, and score the resulting embedding quality. Both Tensorflow and PyTorch models are supported. Cover image credits: Chris Schrier, CC-BY; Petri Krohn, GNU Free Documentation License; Drazen Nesic, CC0; Marco Verch Professional Photographer, CCBY; Grendelkhan, CCBY; Bobby Mikul, CC0; Vincent Van Gogh, CC0; pxhere.com, CC0; Smart Home Perfected, CC-BY.",
        "dataset_text": "In this competition, you are asked to develop models that can efficiently retrieve images from a large database. For each query image, the model is expected to retrieve the most similar images from an index set. We do not provide a training set. In accordance with the rules, any training data may be used, as long as it is disclosed in the forum by the relevant deadline. There exist many public datasets for different object types (e.g., artworks, landmarks, products, etc), and we encourage participants to experiment with them as needed. Our evaluation dataset, which is kept private, contains images of the following types of object: apparel & accessories, packaged goods, landmarks, furniture & home decor, storefronts, dishes, artwork, toys, memes, illustrations and cars. The header image above shows examples of images in these domains, which are similar to the ones in the evaluation set. Labels are defined generally at the instance level, for example: the same T-shirt, the same building, the same painting, the same dish. The following plot shows the distribution of object types in the dataset:  Please see the Evaluation page for details on how the model should be formatted and submitted."
    },
    {
        "name": "IceCube - Neutrinos in Deep Ice",
        "url": "https://www.kaggle.com/competitions/icecube-neutrinos-in-deep-ice",
        "overview_text": "Overview text not found",
        "description_text": "The goal of this competition is to predict a neutrino particle\u2019s direction. You will develop a model based on data from the \"IceCube\" detector, which observes the cosmos from deep within the South Pole ice. Your work could help scientists better understand exploding stars, gamma-ray bursts, and cataclysmic phenomena involving black holes, neutron stars and the fundamental properties of the neutrino itself. One of the most abundant particles in the universe is the neutrino. While similar to an electron, the nearly massless and electrically neutral neutrinos have fundamental properties that make them difficult to detect. Yet, to gather enough information to probe the most violent astrophysical sources, scientists must estimate the direction of neutrino events. If algorithms could be made considerably faster and more accurate, it would allow for more neutrino events to be analyzed, possibly even in real-time and dramatically increase the chance to identify cosmic neutrino sources. Rapid detection could enable networks of telescopes worldwide to search for more transient phenomena. Researchers have developed multiple approaches over the past ten years to reconstruct neutrino events. However, problems arise as existing solutions are far from perfect. They're either fast but inaccurate or more accurate at the price of huge computational costs. The IceCube Neutrino Observatory is the first detector of its kind, encompassing a cubic kilometer of ice and designed to search for the nearly massless neutrinos. An international group of scientists is responsible for the scientific research that makes up the IceCube Collaboration. By making the process faster and more precise, you'll help improve the reconstruction of neutrinos. As a result, we could gain a clearer image of our universe.  ",
        "dataset_text": "The goal of this competition is to identify which direction neutrinos detected by the IceCube neutrino observatory came from. When detection events can be localized quickly enough, traditional telescopes are recruited to investigate short-lived neutrino sources such as supernovae or gamma ray bursts. Because the sky is huge better localization will not only associate neutrinos with sources but also to help partner observatories limit their search space. With an average of three thousand events per second to process, it's difficult to keep up with the stream of data using traditional methods. Your challenge in this competition is to quickly and accurately process a large number of events. This competition uses a hidden test set. When your submitted notebook is scored the actual test data (including a full length sample submission) will be made available to your notebook. Expect to see roughly one million events in the hidden test set, split between multiple batches. [train/test]_meta.parquet [train/test]/batch_[n].parquet Each batch contains tens of thousands of events. Each event may contain thousands of pulses, each of which is the digitized output from a photomultiplier tube and occupies one row. sample_submission.parquet An example submission with the correct columns and properly ordered event IDs. The sample submission is provided in the parquet format so it can be read quickly but your final submission must be a csv. sensor_geometry.csv The x, y, and z positions for each of the 5160 IceCube sensors. The row index corresponds to the sensor_idx feature of pulses. The x, y, and z coordinates are in units of meters, with the origin at the center of the IceCube detector. The coordinate system is right-handed, and the z-axis points upwards when standing at the South Pole. You can convert from these coordinates to azimuth and zenith with the following formulas (here the vector (x,y,z) is normalized): The following image shows a visual representation of the features of an IceCube event in the dataset. The colorful dots represent sensors that logged at least one pulse in the event. The size of the dots corresponds to the total charge of all pulses while the color indicates the time of the first pulse.\nThe left panel shows only pulses with auxiliary==False, and the right panel with auxiliary==True. The small, gray points indicate the positions of all 5160 IceCube sensors. The red arrow shows the true neutrino direction of that event, i.e. the regression target. "
    },
    {
        "name": "CAFA 5 Protein Function Prediction",
        "url": "https://www.kaggle.com/competitions/cafa-5-protein-function-prediction",
        "overview_text": "Overview text not found",
        "description_text": "The goal of this competition is to predict the function of a set of proteins. You will develop a model trained on the amino-acid sequences of the proteins and on other data. Your work will help researchers better understand the function of proteins, which is important for discovering how cells, tissues, and organs work. This may also aid in the development of new drugs and therapies for various diseases. Proteins are responsible for many activities in our tissues, organs, and bodies and they also play a central role in the structure and function of cells. Proteins are large molecules composed of 20 types of building-blocks known as amino acids. The human body makes tens of thousands of different proteins, and each protein is composed of dozens or hundreds of amino acids that are linked sequentially. This amino-acid sequence determines the 3D structure and conformational dynamics of the protein, and that, in turn, determines its biological function. Due to ongoing genome sequencing projects, we are inundated with large amounts of genomic sequence data from thousands of species, which informs us of the amino-acid sequence data of proteins for which these genes code. The accurate assignment of biological function to the protein is key to understanding life at the molecular level. However, assigning function to any specific protein can be made difficult due to the multiple functions many proteins have, along with their ability to interact with multiple partners. More knowledge of the functions assigned to proteins\u2014potentially aided by data science\u2014could lead to curing diseases and improving human and animal health and wellness in areas as varied as medicine and agriculture. Research groups have developed many ways to determine the function of proteins, including numerous methods based on comparing unsolved sequences with databases of proteins whose functions are known. Other efforts aim to mine the scientific literature associated with some of these proteins, while even more methods combine sophisticated machine-learning algorithms with an understanding of biological processes to decipher what these proteins do. However, there are still many challenges in this field, which are driven by ambiguity, complexity, and data integration. The Function Community of Special Interest (Function-COSI) brings together computational biologists, experimental biologists, and biocurators who are dealing with the important problem of gene and gene product function prediction, to share ideas and create collaborations. The Function-COSI holds annual meetings at the Intelligent Systems for Molecular Biology (ISMB) conference and conducts the multi-year Critical Assessment of protein Function Annotation (CAFA) experiment, an ongoing, global, community-driven effort to evaluate and improve the computational annotation of protein function. CAFA is co-chaired by Iddo Friedberg (Iowa State University) and Predrag Radivojac (Northeastern University). Additional academic co-organizers of this Kaggle competition include M. Clara De Paolis Kaluza (Northeastern University), Parnal Joshi (Iowa State University), UniProt (European Bioinformatics Institute), and Damiano Piovesan (University of Padova). We gratefully acknowledge the support of Iowa State University who is hosting this competition. We also acknowledge the support of Northeastern University, University of Padova, UniProt, and the International Society for Computational Biology    ",
        "dataset_text": "The Gene Ontology (GO) is a concept hierarchy that describes the biological function of genes and gene products at different levels of abstraction (Ashburner et al., 2000). It is a good model to describe the multi-faceted nature of protein function. GO is a directed acyclic graph. The nodes in this graph are functional descriptors (terms or classes) connected by relational ties between them (is_a, part_of, etc.). For example, terms 'protein binding activity' and 'binding activity' are related by an is_a relationship; however, the edge in the graph is often reversed to point from binding towards protein binding. This graph contains three subgraphs (subontologies): Molecular Function (MF), Biological Process (BP), and Cellular Component (CC), defined by their root nodes. Biologically, each subgraph represent a different aspect of the protein's function: what it does on a molecular level (MF), which biological processes it participates in (BP) and where in the cell it is located (CC). See the Gene Ontology Overview for more details. The protein's function is therefore represented by a subset of one or more of the subontologies.\nThese annotations are supported by evidence codes, which can be broadly divided into experimental (e.g., as documented in a paper published by a research team of biologists) and non-experimental. Non-experimental terms are usually inferred by computational means. We recommend you read more about the different types of GO evidence codes. We will use experimentally determined term-protein assignments as class labels for each protein. That is, if a protein is labeled with a term, it means that this protein has this function validated by experimental evidence. By processing these annotated terms, we can generate a dataset of proteins and their ground truth labels for each term. The absence of a term annotation does not necessarily mean a protein does not have this function, only that this annotation does not exist (yet) in the GO. A protein may be annotated by one or more terms from the same subontology, and by terms from more than one subontology. Ashburner M, et al. Gene ontology: tool for the unification of biology. The Gene Ontology Consortium. Nat Genet (2000) 25(1):25-29. For the training set, we include all proteins with annotated terms that have been validated by experimental or high-throughput evidence, traceable author statement (evidence code TAS), or inferred by curator (IC). More information about evidence codes can be found here. We use annotations from the UniProtKB release of 2022-11-17. The participants are not required to use these data and are also welcome to use any other data available to them. The test superset is a set of protein sequences on which the participants are asked to predict GO terms. The test set is unknown at the beginning of the competition. It will contain protein sequences (and their functions) from the test superset that gained experimental annotations between the submission deadline and the time of evaluation. Gene Ontology: The ontology data is in the file go-basic.obo. This structure is the 2023-01-01 release of the GO graph. This file is in OBO format, for which there exist many parsing libraries. For example, the obonet package is available for Python. The nodes in this graph are indexed by the term name, for example the roots of the three onotlogies are: Training sequences: train_sequences.fasta contains the protein sequences for the training dataset.\nThis files are in FASTA format, a standard format for describing protein sequences. The proteins were all retrieved from the UniProt data set curated at the European Bioinformatics Institute.\nThe header contains the protein's UniProt accession ID and additional information about the protein. Most protein sequences were extracted from the Swiss-Prot database, but a subset of proteins that are not represented in Swiss-Prot were extracted from the TrEMBL database. In both cases, the sequences come from the 2022_05 release from 14-Dec-2022. More information can be found here. The train_sequences.fasta file will indicate from which database the sequence originate. For example, sp|P9WHI7|RECN_MYCT in the FASTA header indicates the protein with UniProt ID P9WHI7 and gene name RECN_MYCT was taken from Swiss-Prot (sp). Any sequences taken from TrEMBL will have tr in the header instead of sp. Swiss-Prot and TrEMBL are both parts of UniProtKB. This file contains only sequences for proteins with annotations in the dataset (labeled proteins). To obtain the full set of protein sequences for unlabeled proteins, the Swiss-Prot and TrEMBL databases can be found here. Labels: train_terms.tsv contains the list of annotated terms (ground truth) for the proteins in train_sequences.fasta. The first column indicates the protein's UniProt accession ID, the second is the GO term ID, and the third indicates in which ontology the term appears. Taxonomy: train_taxonomy.tsv contains the list of proteins and the species to which they belong, represented by a \"taxonomic identifier\" (taxon ID) number. The first column is the protein UniProt accession ID and the second is the taxon ID. More information about taxonomies can he found here. Information accretion: IA.txt contains the information accretion (weights) for each GO term. These weights are used to compute weighted precision and recall, as described in the Evaluation section. The values of this file were computed using the following code repo. Test sequences: testsuperset.fasta contains protein sequences on which the participants are asked to submit predictions. The header for each sequence in testsuperset.fasta contains the protein's UniProt accession ID and the Taxon ID of the species this protein belongs to. Only a small subset of those sequences will accumulate functional annotations and will constitute the test set. The file testsuperset-taxon-list.tsv is a set of taxon IDs for the proteins in the test superset."
    },
    {
        "name": "BirdCLEF 2023",
        "url": "https://www.kaggle.com/competitions/birdclef-2023",
        "overview_text": "Overview text not found",
        "description_text": "Birds are excellent indicators of biodiversity change since they are highly mobile and have diverse habitat requirements. Changes in species assemblage and the number of birds can thus indicate the success or failure of a restoration project. However, frequently conducting traditional observer-based bird biodiversity surveys over large areas is expensive and logistically challenging. In comparison, passive acoustic monitoring (PAM) combined with new analytical tools based on machine learning allows conservationists to sample much greater spatial scales with higher temporal resolution and explore the relationship between restoration interventions and biodiversity in depth.  For this competition, you'll use your machine-learning skills to identify Eastern African bird species by sound. Specifically, you'll develop computational solutions to process continuous audio data and recognize the species by their calls. The best entries will be able to train reliable classifiers with limited training data. If successful, you'll help advance ongoing efforts to protect avian biodiversity in Africa, including those led by the Kenyan conservation organization NATURAL STATE. NATURAL STATE is working in pilot areas around Northern Mount Kenya to test the effect of various management regimes and states of degradation on bird biodiversity in rangeland systems. By using the machine learning algorithms developed within the scope of this competition, NATURAL STATE will be able to demonstrate the efficacy of this approach in measuring the success of restoration projects and the cost-effectiveness of the method. In addition, the ability to cost-effectively monitor the impact of restoration efforts on biodiversity will allow NATURAL STATE to test and build some of the first biodiversity-focused financial mechanisms to channel much-needed investment into the restoration and protection of this landscape upon which so many people depend. These tools are necessary to scale this cost-effectively beyond the project area and achieve our vision of restoring and protecting the planet at scale. Thanks to your innovations, it will be easier for researchers and conservation practitioners to survey avian population trends accurately. As a result, they'll be able to evaluate threats and adjust their conservation actions regularly and more effectively. This competition is collaboratively organized by (alphabetic order) the Chemnitz University of Technology, Google Research, K. Lisa Yang Center for Conservation Bioacoustics at the Cornell Lab of Ornithology, LifeCLEF, NATURAL STATE, OekoFor GbR, and Xeno-canto. ",
        "dataset_text": "Your challenge in this competition is to identify which birds are calling in long recordings made in Kenya. This is an important task for scientists who monitor bird populations for conservation purposes. More accurate solutions could enable more comprehensive monitoring. This year, your notebook must also complete inference in a more constrained time frame. This will make it easier to deploy winning models for on the ground conservation efforts where efficiency is at a premium. This competition uses a hidden test. When your submitted notebook is scored, the actual test data (including a sample submission) will be made available to your notebook. train_audio/ The training data consists of short recordings of individual bird calls generously uploaded by users of xenocanto.org. These files have been downsampled to 32 kHz where applicable to match the test set audio and converted to the ogg format. The training data should have nearly all relevant files; we expect there is no benefit to looking for more on xenocanto.org. test_soundscapes/ When you submit a notebook, the test_soundscapes directory will be populated with approximately 200 recordings to be used for scoring. They are 10 minutes long and in ogg audio format. The file names are randomized. It should take your submission notebook approximately five minutes to load all of the test soundscapes. train_metadata.csv A wide range of metadata is provided for the training data. The most directly relevant fields are: sample_submission.csv A valid sample submission. eBird_Taxonomy_v2021.csv - Data on the relationships between different species."
    },
    {
        "name": "Image Matching Challenge 2023",
        "url": "https://www.kaggle.com/competitions/image-matching-challenge-2023",
        "overview_text": "Overview text not found",
        "description_text": "2024 Update: you may want to check Image Matching Challenge 2024 The goal of this competition is to reconstruct accurate 3D maps. Last year's Image Matching Challenge focused on two-view matching. This year you will take one step further: your task will be to reconstruct the 3D scene from many different views. Your work could be the key to unlocking mapping the world from assorted and noisy data sources, such as images uploaded by users to services like Google Maps. Your best camera may just be the phone in your pocket. You might take a snap of a landmark, then share it with friends. By itself, that photo is two-dimensional and only includes the perspective of your shooting location. Of course, many people may have taken photos of that same landmark. If we were able to combine all of our photos, we may be able to create a more complete, three-dimensional view of any given thing. Perhaps machine learning could help better capture the richness of the world using the vast amounts of unstructured image collections freely available on the internet. The process of reconstructing a 3D model of an environment from a collection of images is called Structure from Motion (SfM). These images are often captured by trained operators or with additional sensor data, such as the cars used by Google Maps. This ensures homogeneous, high-quality data. It is much more difficult to build 3D models from assorted images, given a wide variety of viewpoints, along with lighting, weather, and other changes. Competition host Google employs SfM techniques across many Google Maps services, such as the 3D models created from StreetView and aerial imagery. In order to accelerate research into this topic and better leverage the volume of data already publicly available, Google presents this competition in collaboration with Haiper and Kaggle. Your work in helping to build accurate 3D models may have applications to photography, cultural heritage preservation, and many services across Google.  Banner photo by Salvador Altamirano on Unsplash. Other photos courtesy of the Archaelogical Park \"Valle dei Templi\" of Agrigento. Eduard Trulls (Google), Dmytro Mishkin (Czech Technical University in Prague/HOVER Inc), Jiri Matas (Czech Technical University in Prague), Fabio Bellavia (University of Palermo), Luca Morelli (University of Trento/Bruno Kessler Foundation), Fabio Remondino (Bruno Kessler Foundation), Weiwei Sun (University of British Columbia), Kwang Moo Yi (University of British Columbia/Haiper).",
        "dataset_text": "Building a 3D model of a scene given an unstructured collection of images taken around it is a longstanding problem in computer vision research. Your challenge in this competition is to generate 3D reconstructions from image sets showing different types of scenes and accurately pose those images. This competition uses a hidden test. When your submitted notebook is scored, the actual test data (including a sample submission) will be made available to your notebook. Expect to find roughly 1,100 images in the hidden test set. The number of images in a scene may vary from <10 to ~250. Parts of the dataset (the Haiper subset) were created with the Captur3 app and the Haiper Research team from Haiper AI. sample_submission.csv A valid, randomly-generated sample submission with the following fields: [train/test]/*/*/images A batch of images all taken near the same location. Some of training datasets may also contain a folder named images_full with additional images. train/*/*/sfm A 3D reconstruction for this batch of images, which can be opened with colmap, the 3D structure-from-motion library bundled with this competition. train/*/*/LICENSE.txt The license for this dataset. train/train_labels.csv A list of images in these datasets, with ground truth."
    },
    {
        "name": "Google Research - Identify Contrails to Reduce Global Warming",
        "url": "https://www.kaggle.com/competitions/google-research-identify-contrails-reduce-global-warming",
        "overview_text": "Overview text not found",
        "description_text": "Contrails are clouds of ice crystals that form in aircraft engine exhaust. They can contribute to global warming by trapping heat in the atmosphere. Researchers have developed models to predict when contrails will form and how much warming they will cause. However, they need to validate these models with satellite imagery.\nYour work will help researchers improve the accuracy of their contrail models. This will help airlines avoid creating contrails and reduce their impact on climate change. Contrail avoidance is potentially one of the most scalable, cost-effective sustainability solutions available to airlines today. Contrails, short for \u2018condensation trails\u2019, are line-shaped clouds of ice crystals that form in aircraft engine exhaust, and are created by airplanes flying through super humid regions in the atmosphere. Persistent contrails contribute as much to global warming as the fuel they burn for flights. This phenomenon was first figured out 75 years ago by military airplanes with the goal to avoid leaving a visible trail. About 30 years ago, climate scientists in Europe began to understand that the contrails blocked heat that normally is released from the earth overnight. They have built powerful models, based on weather data, to identify when contrails will form and how warming they will be. Their research has been validated by other labs as well and it is now well accepted that contrails contribute approximately 1% of all human caused global warming. The motivation behind the use of satellite imagery is to empirically confirm the predictions from these models. With reliable verification, pilots can have confidence in the models and the airline industry can have a trusted way to measure successful contrail avoidance.\n\nImage courtesy of Imperial College\nYour work will quantifiably improve the confidence in prediction of contrail forming regions and the techniques to avoid creating them.\nGoogle Research applies ML to opportunities to mitigate climate change and adapt to the changes we already see. We have run research projects in fusion energy plasma modeling, wildfire early detection, optimal car routing, and forecasts for climate disasters. MIT Laboratory for Aviation and the Environment led by MIT Professor Steven Barrett\nSatellite images are from NOAA GOES-16, see more in https://www.goes-r.gov/.",
        "dataset_text": "In this competition you will be using geostationary satellite images to identify aviation contrails. The original satellite images were obtained from the GOES-16 Advanced Baseline Imager (ABI), which is publicly available on Google Cloud Storage. The original full-disk images were reprojected using bilinear resampling to generate a local scene image. Because contrails are easier to identify with temporal context, a sequence of images at 10-minute intervals are provided. Each example (record_id) contains exactly one labeled frame. Learn more about the dataset from the preprint: OpenContrails: Benchmarking Contrail Detection on GOES-16 ABI. Labeling instructions can be found at in this supplementary material. Some key labeling guidance: Ground truth was determined by (generally) 4+ different labelers annotating each image. Pixels were considered a contrail when >50% of the labelers annotated it as such. Individual annotations (human_individual_masks.npy) as well as the aggregated ground truth annotations (human_pixel_masks.npy) are included in the training data. The validation data only includes the aggregated ground truth annotations. This is an example of labeled contrails. Code to produce images like this can be found in this notebook. "
    },
    {
        "name": "HuBMAP - Hacking the Human Vasculature",
        "url": "https://www.kaggle.com/competitions/hubmap-hacking-the-human-vasculature",
        "overview_text": "Overview text not found",
        "description_text": "The goal of this competition is to segment instances of microvascular structures, including capillaries, arterioles, and venules. You'll create a model trained on 2D PAS-stained histology images from healthy human kidney tissue slides. Your help in automating the segmentation of microvasculature structures will improve researchers' understanding of how the blood vessels are arranged in human tissues. The proper functioning of your body's organs and tissues depends on the interaction, spatial organization, and specialization of your cells\u2014all 37 trillion of them. With so many cells, determining their functions and relationships is a monumental undertaking. Current efforts to map cells involve the Vasculature Common Coordinate Framework (VCCF), which uses the blood vasculature in the human body as the primary navigation system. The VCCF crosses all scale levels--from the whole body to the single cell level--and provides a unique way to identify cellular locations using capillary structures as an address. However, the gaps in what researchers know about microvasculature lead to gaps in the VCCF. If we could automatically segment microvasculature arrangements, researchers could use the real-world tissue data to begin to fill in those gaps and map out the vasculature. Competition host Human BioMolecular Atlas Program (HuBMAP) hopes to develop an open and global platform to map healthy cells in the human body. Using the latest molecular and cellular biology technologies, HuBMAP researchers are studying the connections that cells have with each other throughout the body. There are still many unknowns regarding microvasculature, but your Machine Learning insights could enable researchers to use the available tissue data to augment their understanding of how these small vessels are arranged throughout the body. Ultimately, you'll be helping to pave the way towards building a Vascular Common Coordinate Framework (VCCF) and a Human Reference Atlas (HRA), which will identify how the relationships between cells can affect our health.",
        "dataset_text": "Your goal in this competition is to locate microvasculature structures (blood vessels) within human kidney histology slides. The competition data comprises tiles extracted from five Whole Slide Images (WSI) split into two datasets. Tiles from Dataset 1 have annotations that have been expert reviewed. Dataset 2 comprises the remaining tiles from these same WSIs and contain sparse annotations that have not been expert reviewed. We also include, as Dataset 3, tiles extracted from an additional nine WSIs. These tiles have not been annotated. You may wish to apply semi- or self-supervised learning techniques on this data to support your predictions. Note that this is a Code Competition, in which the actual test set is hidden. In this version, we give some sample data drawn from the public test set to help you author your solutions. When your submission is scored, this example test data will be replaced with the full test set. There are about 650 tiles in the full test set. You may find resources from the previous HuBMAP competitions useful as well:"
    },
    {
        "name": "Google - Fast or Slow? Predict AI Model Runtime",
        "url": "https://www.kaggle.com/competitions/predict-ai-model-runtime",
        "overview_text": "Alice is an AI model developer, but some of the models her team developed run very slow. She recently discovered compiler's configurations that change the way the compiler compiles and optimizes the models, and hence make the models run faster (or slower)! Your task is to help Alice find the best configuration for each model.",
        "description_text": "A bit of technical background on an AI compiler will help you get started! An AI model can be represented as a graph, where a node is a tensor operation (e.g. matrix multiplication, convolution, etc), and an edge represents a tensor. A compilation configuration controls how the compiler transforms the graph for a specific optimization pass. In particular, Alice can control two types of configurations/optimizations:   Being able to predict an optimal configuration for a given graph will not only help Alice's team but also improve the compiler's heuristic to select the best configuration without human's intervention. This will make AI models run more efficiently, consuming less time and resources overall! In this competition, your aim is to train a machine learning model based on the runtime data provided to you in the training dataset and further predict the runtime of graphs and configurations in the test dataset. Here is a handy link to Kaggle's competition documentation, which includes, among other things, instructions on submitting predictions. Our dataset, called TpuGraphs, is the performance prediction dataset on XLA HLO graphs running on Tensor Processing Units (TPUs) v3. There are 5 data collections in total:layout:xla:random, layout:xla:default, layout:nlp:random, layout:nlp:default, and tile:xla. The final score will be the average across all collections. To download the entire dataset and view more information, you may navigate to the Data tab. We provide baseline models along with a training set up readily for you to get started at https://github.com/google-research-datasets/tpu_graphs. Please refer to our dataset paper on the details of the baseline models.",
        "dataset_text": "You have two options to download the dataset: After either above options, your final data path should have ~/data/tpugraphs/npz/layout and ~/data/tpugraphs/npz/tile. By default, the trainer scripts (see GitHub repo) read from ~/data/tpugraphs, but you may override flag --data_root in the trainer of tile or layout collections. In addition, you may download sample_submission.csv which contains the example submission file in the expected format. Once you have downloaded the dataset (e.g., to ~/data/tpugraphs, which is the default path for training code) and unzip it, then you will get the following directory structure. Suppose a .npz file stores a graph (representing a kernel) with n nodes and m edges. In addition, suppose we compile the graph with c different configurations, and run each on a TPU. Crucially, the configuration is at the graph-level. Then, the .npz file stores the following dictionary (can be loaded with d = dict(np.load(\"npz/tile/xla/train/<pick_1>.npz\"))): Finally, for the tile collection, your job is to predict the indices of the best configurations (i.e., ones leading to the smallest d[\"config_runtime\"] / d[\"config_runtime_normalizers\"]). Suppose a .npz file stores a graph (representing the entire program) with n nodes and m edges. In addition, suppose we compile the graph with c different configurations, and run each on a TPU. Crucially, the configuration is at the node-level. Suppose that nc of the n nodes are configurable. Then, the .npz file stores the following dictionary (can be loaded with, e.g., d = dict(np.load(\"npz/layout/xla/random/train/<pick_1>.npz\"))): Finally, for the layout collections, your job is to predict the order of the indices from best-to-worse configurations (i.e., ones leading to the smallest d[\"config_runtime\"]). We do not have to use runtime normalizers for this task because the runtime variation at the entire program level is very small. Optionally, you may access key \"node_splits\", which is a variable-length list of node IDs that are the starting of HLO computations in the graph (similar to functions in a program). Essentially, nodes d[\"node_splits\"][i] to d[\"node_splits\"][i+1] - 1 belongs to the same computation. If you want to partition the graph into multiple segments, this information may be useful, e.g., putting nodes from the same computation in the same partition. However, you may compute your own partitioning (e.g., using METIS) as well. To extract a node feature vector, we either copy values from various fields in an XLA\u2019s HLO instruction (a node in an HLO graph) as they are, or convert categorical values using one-hot encoding. To convert an unbounded list of numbers (e.g. tensor shape) to a fixed-size vector, we truncate the list to six elements and include the summation and/or product of all elements in the list (e.g., the product of dimension sizes represents the volume of the tensor). In our dataset, none of the tensors has more than six dimensions. The following describe each element at a particular index in the node feature vector. Suffix _i, where i is an integer, indicates the information for the tensor dimension i. If a tensor has N dimensions, feature values of _i are set to 0 if i >= N (0 padding). Suffix _sum is the summation of the feature values across all dimensions. Suffix _product is the product of the feature values across all dimensions. The source code of the node features extractor can be found here, which extracts features/attributes from HloProto defined here. The following describe each element at a particular index in the tile config feature vector. Note that input_bounds are usually set to 0 because they can be inferred by the compiler from output_bounds (and kernel_bounds). If a tensor has N dimensions, feature values of _i are set to 0 if i >= N (0 padding). The following describe each element at a particular index in the per-node layout config feature vector. If a tensor has N dimensions, feature values of _i are set to -1 if i >= N (-1 padding). A layout determines the order of minor-to-major tensor dimensions. For example, the layout of {1, 0, 2, -1, -1, -1} of a 3D tensor indicates that dimension 1 is the most minor (elements of the most minor dimension are consecutive in the physical space), and dimension 2 is the most major. We also include a tensor layout of {-1, -1, -1, -1, -1, -1} to indicate the compiler to apply its default strategy of selecting the layout for that tensor. If you want to extract node features differently from the pre-extracted features provided in an .npz file, you can extract the features from a raw graph in an .pb file following this instruction. The directory structure and filenames of .pb files match those of .npz files. In particular, the raw graph in npz/.../xxx.npz can be found in pb/.../xxx.pb. The raw graphs are only available for the layout collections."
    },
    {
        "name": "HMS - Harmful Brain Activity Classification",
        "url": "https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification",
        "overview_text": "The goal of this competition is to detect and classify seizures and other types of harmful brain activity. You will develop a model trained on electroencephalography (EEG) signals recorded from critically ill hospital patients.",
        "description_text": "From stethoscopes to tongue depressors, doctors rely on many tools to treat their patients. Physicians use electroencephalography with critically ill patients to detect seizures and other types of brain activity that can cause brain damage. You can learn about how doctors interpret these EEG signals in these videos:\nEEG Talk - ACNS Critical Care EEG Terminology 2021 (Part 1) (Part 2) (Part 3) (Part 4) (Part 5) Currently, EEG monitoring relies solely on manual analysis by specialized neurologists. While invaluable, this labor-intensive process is a major bottleneck. Not only can it be time-consuming, but manual review of EEG recordings is also expensive, prone to fatigue-related errors, and suffers from reliability issues between different reviewers, even when those reviewers are experts. Competition host Sunstella Foundation was created in 2021 during the COVID pandemic to help minority graduate students in technology overcome challenges and celebrate their achievements. These students are vital to America's technology leadership and diversity. Through workshops, forums, and competitions, the Sunstella Foundation provides mentorship and career advice to support their success. Sunstella Foundation is joined by Persyst, Jazz Pharmaceuticals, and the Clinical Data Animation Center (CDAC), whose research aims to help people preserve and enhance brain health. Your work in automating EEG analysis will help doctors and brain researchers detect seizures and other types of brain activity that can cause brain damage, so that they can give treatments more quickly and accurately. The algorithms developed in this contest may also help researchers who are working to develop drugs to treat and prevent seizures. There are six patterns of interest for this competition: seizure (SZ), generalized periodic discharges (GPD), lateralized periodic discharges (LPD), lateralized rhythmic delta activity (LRDA), generalized rhythmic delta activity (GRDA), or \u201cother\u201d. Detailed explanations of these patterns are available here. The EEG segments used in this competition have been annotated, or classified, by a group of experts. In some cases experts completely agree about the correct label. On other cases the experts disagree. We call segments where there are high levels of agreement \u201cidealized\u201d patterns. Cases where ~1/2 of experts give a label as \u201cother\u201d and ~1/2 give one of the remaining five labels, we call \u201cproto patterns\u201d. Cases where experts are approximately split between 2 of the 5 named patterns, we call \u201cedge cases\u201d. Examples of EEG Patterns with Different Levels of Expert Agreement:\n\nPlease refer to Data tab for full screen PDF page of each subfigure. This figure shows selected examples of EEG patterns with different level of agreement. Rows are structured with the 1st row seizure, 2nd row LPDs, 3rd row GPDs, 4th row LRDA, and 5th row GRDA. Column-wise, examples of idealized forms of patterns are in the 1st column (A). These are patterns with uniform expert agreement. The 2nd column (B) are proto or partially formed patterns. About half of raters labeled these as one IIIC pattern and the other half labeled \u201cOther\u201d. The 3rd and 4th columns (C, D) are edge cases (about half of raters labeled these one IIIC pattern and half labeled them as another IIIC pattern). For B-1 there is rhythmic delta activity with some admixed sharp discharges within the 10 second raw EEG, and the spectrogram shows that this segment may belong to the tail end of a seizure, thus disagreement between SZ and \u201cOther\u201d makes sense. B-2 shows frontal lateralized sharp transients at ~1Hz, but they have a reversed polarity, suggesting they may be coming from a non-cerebral source, thus the split between LPD and \u201cOther\u201d (artifact) makes sense. B-3 has diffused semi-rhythmic delta background with poorly formed low amplitude generalized periodic discharges with s shifting morphology making it a proto-GPD type pattern. B-4 shows semi-rhythmic delta activity with unstable morphology over the right hemisphere, a proto-LRDA pattern. B-5 shows a few waves of rhythmic delta activity with an unstable morphology and is poorly sustained, a proto-GRDA. C-1 shows 2Hz LPDs showing an evolution with increasing amplitude evolving underlying rhythmic activity, a pattern between LPDs and the beginning of a seizure, an edge-case. D-1 shows abundant GPDs on top of a suppressed background with frequency of 1-2Hz. The average over the 10-seconds is close to 1.5Hz, suggesting a seizure, another edge case. C-2 is split between LPDs and GPDs. The amplitude of the periodic discharges is higher over the right, but a reflection is also seen on the left. D-2 is tied between LPDs and LRDA. It shares some features of both; in the temporal derivations it looks more rhythmic whereas in the parasagittal derivations it looks periodic. C-3 is split between GPDs and LRDA. The ascending limb of the delta waves have a sharp morphology, and these periodic discharges are seen on both sides. The rhythmic delta appears to be of higher amplitude over the left, but there is some reflection of the activity on the left. D-3 is split between GPDs and GRDA. The ascending limb of the delta wave has a sharp morphology and there is asymmetry in slope between ascending and descending limbs making it an edge case. C-4 is split between LRDA and seizure. It shows 2Hz LRDA on the left, and the spectrogram shows that this segment may belong to the tail end of a seizure, an edge-case. D-4 is split between LRDA and GRDA. The rhythmic delta appears to be of higher amplitude over the left, but there is some reflection of the activity on the right. C-5 is split between GRDA and seizure. It shows potentially evolving rhythmic delta activity with poorly formed embedded epileptiform discharges, a pattern between GRDA and seizure, an edge-case. D-5 is split between GRDA and LPDs. There is generalized rhythmic delta activity, while the activity on the right is somewhat higher amplitude and contains poorly formed epileptiform discharges suggestive of LPDs, an edge-case. Note: Recording regions of the EEG electrodes are abbreviated as LL = left lateral; RL = right lateral; LP = left parasagittal; RP = right parasagittal.",
        "dataset_text": "The goal of this competition is to detect and classify seizures and other types of harmful brain activity in electroencephalography (EEG) data. Even experts find this to be a challenging task and often disagree about the correct labels. This is a code competition. Only a few examples from the test set are available for download. When your submission is scored the test folders will be replaced with versions containing the complete test set. train.csv Metadata for the train set. The expert annotators reviewed 50 second long EEG samples plus matched spectrograms covering 10 a minute window centered at the same time and labeled the central 10 seconds. Many of these samples overlapped and have been consolidated. train.csv provides the metadata that allows you to extract the original subsets that the raters annotated. test.csv Metadata for the test set. As there are no overlapping samples in the test set, many columns in the train metadata don't apply. sample_submission.csv train_eegs/ EEG data from one or more overlapping samples. Use the metadata in train.csv to select specific annotated subsets. The column names are the names of the individual electrode locations for EEG leads, with one exception. The EKG column is for an electrocardiogram lead that records data from the heart. All of the EEG data (for both train and test) was collected at a frequency of 200 samples per second. test_eegs/ Exactly 50 seconds of EEG data. train_spectrograms/ Spectrograms assembled EEG data. Use the metadata in train.csv to select specific annotated subsets. The column names indicate the frequency in hertz and the recording regions of the EEG electrodes. The latter are abbreviated as LL = left lateral; RL = right lateral; LP = left parasagittal; RP = right parasagittal. test_spectrograms/ Spectrograms assembled using exactly 10 minutes of EEG data. example_figures/ Larger copies of the example case images used on the overview tab."
    },
    {
        "name": "Jigsaw Rate Severity of Toxic Comments",
        "url": "https://www.kaggle.com/competitions/jigsaw-toxic-severity-rating",
        "overview_text": "Overview text not found",
        "description_text": "In Jigsaw's fourth Kaggle competition, we return to the Wikipedia Talk page comments featured in our first Kaggle competition. When we ask human judges to look at individual comments, without any context, to decide which ones are toxic and which ones are innocuous, it is rarely an easy task. In addition, each individual may have their own bar for toxicity. We've tried to work around this by aggregating the decisions with a majority vote. But many researchers have rightly pointed out that this discards meaningful information. A much easier task is to ask individuals which of two comments they find more toxic. But if both comments are non-toxic, people will often select randomly. When one comment is obviously the correct choice, the inter-annotator agreement results are much higher. In this competition, we will be asking you to score a set of about fourteen thousand comments. Pairs of comments were presented to expert raters, who marked one of two comments more harmful \u2014 each according to their own notion of toxicity. In this contest, when you provide scores for comments, they will be compared with several hundred thousand rankings. Your average agreement with the raters will determine your individual score. In this way, we hope to focus on ranking the severity of comment toxicity from innocuous to outrageous, where the middle matters as much as the extremes. Can you build a model that produces scores that rank each pair of comments the same way as our professional raters? Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive. The paper \"Ruddit: Norms of Offensiveness for English Reddit Comments\" by Hada et al. introduced a similar dataset that involved tuples of four sentences that were marked with best-worst scoring, and this data may be directly useful for building models. We also note \"Constructing Interval Variables via Faceted Rasch Measurement and Multitask Deep Learning: a Hate Speech Application\" by Kennedy et al. which compares a variety of different rating schemes and argues that binary classification as typically done in NLP tasks discards valuable information. Combining data from multiple sources, even with different annotation guidelines, may be essential for success in this competition. The English language resources from our first Kaggle competition, and our second Kaggle competition, which are both available in the TensorFlow datasets Wikipedia Toxicity Subtypes and Civil Comments can be used to build models. One example of a starting point is the open source UnitaryAI model. Google's Jigsaw team explores threats to open societies and builds technology that inspires scalable solutions. One Jigsaw product is PerspectiveAPI which is used by publishers and platforms worldwide as part of their overall moderation strategy. ",
        "dataset_text": "In this competition you will be ranking comments in order of severity of toxicity. You are given a list of comments, and each comment should be scored according to their relative toxicity. Comments with a higher degree of toxicity should receive a higher numerical value compared to comments with a lower degree of toxicity. Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive. Note, there is no training data for this competition. You can refer to previous Jigsaw competitions for data that might be useful to train models. But note that the task of previous competitions has been to predict the probability that a comment was toxic, rather than the degree or severity of a comment's toxicity. While we don't include training data, we do provide a set of paired toxicity rankings that can be used to validate models."
    },
    {
        "name": "RSNA Screening Mammography Breast Cancer Detection",
        "url": "https://www.kaggle.com/competitions/rsna-breast-cancer-detection",
        "overview_text": "Overview text not found",
        "description_text": "The goal of this competition is to identify breast cancer. You'll train your model with screening mammograms obtained from regular screening. Your work improving the automation of detection in screening mammography may enable radiologists to be more accurate and efficient, improving the quality and safety of patient care. It could also help reduce costs and unnecessary medical procedures. According to the WHO, breast cancer is the most commonly occurring cancer worldwide. In 2020 alone, there were 2.3 million new breast cancer diagnoses and 685,000 deaths. Yet breast cancer mortality in high-income countries has dropped by 40% since the 1980s when health authorities implemented regular mammography screening in age groups considered at risk. Early detection and treatment are critical to reducing cancer fatalities, and your machine learning skills could help streamline the process radiologists use to evaluate screening mammograms. Currently, early detection of breast cancer requires the expertise of highly-trained human observers, making screening mammography programs expensive to conduct. A looming shortage of radiologists in several countries will likely worsen this problem. Mammography screening also leads to a high incidence of false positive results. This can result in unnecessary anxiety, inconvenient follow-up care, extra imaging tests, and sometimes a need for tissue sampling (often a needle biopsy). The competition host, the Radiological Society of North America (RSNA) is a non-profit organization that represents 31 radiologic subspecialties from 145 countries around the world. RSNA promotes excellence in patient care and health care delivery through education, research, and technological innovation. Your efforts in this competition could help extend the benefits of early detection to a broader population. Greater access could further reduce breast cancer mortality worldwide.",
        "dataset_text": "Note: The dataset for this challenge contains radiographic breast images of female subjects.\nThe goal of this competition is to identify cases of breast cancer in mammograms from screening exams. It is important to identify cases of cancer for obvious reasons, but false positives also have downsides for patients. As millions of women get mammograms each year, a useful machine learning tool could help a great many people.\n\nThis competition uses a hidden test. When your submitted notebook is scored the actual test data (including a full length sample submission) will be made available to your notebook. [train/test]_images/[patient_id]/[image_id].dcm The mammograms, in dicom format. You can expect roughly 8,000 patients in the hidden test set. There are usually but not always 4 images per patient. Note that many of the images use the jpeg 2000 format which may you may need special libraries to load.\n\nsample_submission.csv A valid sample submission. Only the first few rows are available for download.\n\n[train/test].csv Metadata for each patient and image. Only the first few rows of the test set are available for download."
    },
    {
        "name": "Santa 2022 - The Christmas Card Conundrum",
        "url": "https://www.kaggle.com/competitions/santa-2022",
        "overview_text": "Overview text not found",
        "description_text": " This Christmas season, we\u2019re printing in house.\nOur vendor\u2019s supply eaten by a field mouse!\nThe paper was laid on the printer with care\nIn hopes that the arms soon would reach there;\nThe elves are nestled all snug in their pods;\nIn hopes that the picture would earn some applause.\nCan you help move the arms and put them in place?\nPlease optimize this configuration space! Santa\u2019s elves relied on the same vendor every year to print the annual Christmas card. But a mouse ate through the printer cables and the elves are forced to print at the North Pole Workshop this year! They managed to create their own giant printer with an 8-axis robotic arm that can print one pixel of the card at a time. But moving the arm and changing the color is not only expensive, the elves need to spend the least amount of time possible making the card so they can get back to making toys! Your job is to determine the most optimal way to craft this year\u2019s Christmas card, by selecting the most efficient path of both moving the robotic arm and changing the print color to craft this year\u2019s image. Each link of the printer arm can be moved independently each step, but you'll also need to account for the time needed to change the printing color. Can you save Christmas by finding the most efficient way to print Santa\u2019s Christmas cards? Photos by Kelly Sikkema and Eric Prouzet on Unsplash",
        "dataset_text": "Your task in this competition is to create a sequence of arm configurations covering every point on the image that minimizes the total movement of the arm and also the change in color from point to point. See the Evaluation page and Getting Started notebook for more details."
    },
    {
        "name": "Benetech - Making Graphs Accessible",
        "url": "https://www.kaggle.com/competitions/benetech-making-graphs-accessible",
        "overview_text": "Overview text not found",
        "description_text": "Millions of students have a learning, physical, or visual disability that prevents a person from reading conventional print. The majority of educational materials for science, technology, engineering, and math (STEM) fields are inaccessible to these students. Technology that makes the written word accessible exists. However, doing so for educational visuals like graphs remains complex and resource intensive. As a result, only a small fraction of educational materials are available for learners with this learning difference \u2014unless machine learning could help bridge that gap. The goal of this competition is to extract data represented by four types of charts commonly found in STEM textbooks. You will develop an automatic solution trained on a graph dataset. Your work will help make reading graphs accessible to millions of students with a learning difference or disability. Current efforts to make existing educational materials accessible are done through a manual process. Though this method produces accessible materials, it is expensive and time-consuming and has to go through several rounds of quality checking. Furthermore, the amount of effort involved in making these materials accessible often means these documents aren\u2019t available for schools or teachers without the funding to license them. Competition host Benetech is a nonprofit dedicated to reducing social and economic inequity in partnership with the communities that it serves through software for social good. Benetech\u2019s initiatives are transforming how students, job seekers, and older adults across the globe read, learn, and work. Benetech believes that access to information is a human right, and no person should encounter barriers to education, literacy, or employment due to differences or disabilities. The best submissions will be adopted into Benetech\u2019s PageAI product, which converts books and other educational materials into accessible formats. You could have a hand in serving accessible STEM materials to millions of students with a learning difference or disability. Benetech would like to thank Schmidt Futures for their support in making this work possible.          ",
        "dataset_text": "This competition's dataset comprises about 65,000 comprehensively-annotated scientific figures of four kinds: bar graphs (both horizontal and vertical), dot plots, line graphs, and scatter plots. While the majority of the figures are synthetic, we also include several thousand figures extracted from professionally-produced sources. Your task is to predict the data series depicted in the test set figures. This is a Code Competition, in which the actual test set is hidden. In this public version, we give some sample data drawn from the training set to help you author your solutions. When your submission is scored, this example test data will be replaced with the actual test set. The full test set comprises about 4,000 figures extracted from professionally-produced sources. The test set does not contain any generated figures. The extracted figures in the training and public test sets are drawn from the same set of sources. The figures in the private test set are drawn from a distinct set of sources. You should ensure your solutions are robust to a variety of figure styles. The distribution of chart types may not be identical in the public and private test sets."
    },
    {
        "name": "UBC Ovarian Cancer Subtype Classification and Outlier Detection (UBC-OCEAN)",
        "url": "https://www.kaggle.com/competitions/UBC-OCEAN",
        "overview_text": "The goal of the UBC Ovarian Cancer subtypE clAssification and outlier detectioN (UBC-OCEAN) competition is to classify ovarian cancer subtypes. You will build a model trained on the world's most extensive ovarian cancer dataset of histopathology images obtained from more than 20 medical centers.",
        "description_text": "Ovarian carcinoma is the most lethal cancer of the female reproductive system. There are five common subtypes of ovarian cancer: high-grade serous carcinoma, clear-cell ovarian carcinoma, endometrioid, low-grade serous, and mucinous carcinoma. Additionally, there are several rare subtypes (\"Outliers\"). These are all characterized by distinct cellular morphologies, etiologies, molecular and genetic profiles, and clinical attributes. Subtype-specific treatment approaches are gaining prominence, though first requires subtype identification, a process that could be improved with data science. Currently, ovarian cancer diagnosis relies on pathologists to assess subtypes. However, this presents several challenges, including disagreements between observers and the reproducibility of diagnostics. Furthermore, underserved communities often lack access to specialist pathologists, and even well-developed communities face a shortage of pathologists with expertise in gynecologic malignancies. Deep learning models have exhibited remarkable proficiency in analyzing histopathology images. Yet challenges still exist, such as the need for a significant amount of training data, ideally from a single source. Technical, ethical, and financial constraints, as well as confidentiality concerns, make training a challenge. In this competition, you will have access to the most extensive and diverse ovarian cancer dataset of histopathology images from more than 20 centers across four continents.  Competition host University of British Columbia (UBC) is a global center for teaching, learning, and research, consistently ranked among the top 20 public universities in the world. UBC embraces innovation and transforms ideas into action. Since 1915, UBC has been opening doors of opportunity for people with the curiosity, drive, and vision to shape a better world. Joining UBC is the BC Cancer, part of the Provincial Health Services Authority and its world-renowned Ovarian Cancer Research (OVCARE) team whose discoveries have led to progressive prevention strategies and improved diagnostics and treatments. BC Cancer provides a comprehensive cancer control program for the people of British Columbia in partnership with regional health authorities. We have also partnered with the Ovarian Tumour Tissue Analysis (OTTA) consortium, an international multidisciplinary network of investigators from more than 65 international teams across the globe. Finally, the OCEAN challenge is made possible through a generous donation from TD Bank Group through the BC Cancer Foundation. Your work could yield improved accuracy in identifying ovarian cancer subtypes. Better classification would enable clinicians to formulate personalized treatment strategies regardless of geographic location. This targeted approach has the potential to enhance treatment efficacy, reduce adverse effects, and ultimately contribute to better patient outcomes for those diagnosed with this deadly cancer. Should you choose to utilize this dataset in your studies, please ensure to follow the citation guidelines listed in the How to Cite This Challenge in Publications section found in the Overview section of the competition page.",
        "dataset_text": "Your challenge in this competition is to classify the type of ovarian cancer from microscopy scans of biopsy samples. This competition uses a hidden test. When your submitted notebook is scored, the actual test data (including a full length sample submission) will be made available to your notebook. Due to the size of the dataset the train images will not be available to your submission notebook. [train/test]_images A folder containing the relevant images. There are two categories of images: whole slide images (WSI) and tissue microarray (TMA). Whole slide images are at 20x magnification and can be quite large. The TMAs are smaller (roughly 4,000x4,000 pixels) but at 40x magnification.\nThe test set contains images from different source hospitals than the train set, with the largest area images almost 100,000 x 50,000 pixels. We strongly recommend taking an expansive approach to thinking about the scenarios your error handling should manage, including differences in image dimensions, quality, slide staining techniques, and more. Expect roughly 2,000 images in the test set, the majority of which are TMAs. The total size is 550 GB so simply loading the data will be time consuming. Be warned that the test set was specifically constructed to assess how well models generalize. [train/test].csv Labels for the train set. [train/test]_thumbnails A folder containing smaller .png copies of the whole slide images. Thumbnails are not provided for TMAs. sample_submission.csv A valid sample submission. Only the first row is available for download. supplemental masks Roughly 150 masks that show which parts of the relevant whole slide images from the train set are cancerous, healthy, or necrotic. These masks are served as a separate dataset available here. The mask file names equal the file names of the matching train images. Should you choose to utilize this dataset in your studies, please ensure to follow the citation guidelines listed in the How to Cite This Challenge in Publications section found in the Overview section of the competition page."
    },
    {
        "name": "Stable Diffusion - Image to Prompts",
        "url": "https://www.kaggle.com/competitions/stable-diffusion-image-to-prompts",
        "overview_text": "Overview text not found",
        "description_text": "The goal of this competition is to reverse the typical direction of a generative text-to-image model: instead of generating an image from a text prompt, can you create a model which can predict the text prompt given a generated image? You will make predictions on a dataset containing a wide variety of (prompt, image) pairs generated by Stable Diffusion 2.0, in order to understand how reversible the latent relationship is. The popularity of text-to-image models has spurned an entire new field of prompt engineering. Part art and part unsettled science, ML practitioners and researchers are rapidly grappling with understanding the relationships between prompts and the images they generate. Is adding \"4k\" to a prompt the best way to make it more photographic? Do small perturbations in prompts lead to highly divergent images? How does the order of prompt keywords impact the resulting generated scene? This competition tasks you with creating a model that can reliably invert the diffusion process that generated to a given image. In order to calculate prompt similarity in a robust way\u2014meaning that \"epic cat\" is scored as similar to \"majestic kitten\" in spite of character-level differences\u2014you will submit embeddings of your predicted prompts. Whether you model the embeddings directly or first predict prompts and then convert to embeddings is up to you! Good luck, and may you create \"highly quality, sharp focus, intricate, detailed, in the style of unreal robust cross validation\" models herein.",
        "dataset_text": "Your task for this challenge is to predict the prompts that were used to generate target images. Prompts for this challenge were generated using a variety of (non disclosed) methods, and range from fairly simple to fairly complex with multiple objects and modifiers. Images were generated from the prompts using Stable Diffusion 2.0 (768-v-ema.ckpt) and were generated with 50 steps at 768x768 px and then downsized to 512x512 for the competition dataset. (This script was used, with the majority of default parameters unchanged.)"
    },
    {
        "name": "March Machine Learning Mania 2023",
        "url": "https://www.kaggle.com/competitions/march-machine-learning-mania-2023",
        "overview_text": "Overview text not found",
        "description_text": "Another year, another chance to predict the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. In our ninth annual March Machine Learning Mania competition, Kagglers will once again join the millions of fans who attempt to predict the outcomes of this year's college basketball tournaments. Unlike most fans, you will pick the winners and losers using a combination of rich historical data and computing power, while the ground truth unfolds on television. You are provided data of historical NCAA games to forecast the outcomes of the Division 1 Men's and Women's basketball tournaments. This competition is the official 2023 edition, with points, medals, prizes, and basketball glory at stake. We have made several updates to the competition format compared to prior editions: We have also launched a companion warmup competition, which is setup as a practice leaderboard covering the previous five tournaments. Because its only for practice and the ground historical game outcomes are public information, the warmup competition does not count for points/medals and will be taken down once it has served its purpose. Prior to the start of the tournaments, the leaderboard of this competition will reflect all zero scores. Kaggle will periodically fill in the outcomes and rescore once games begin. Good luck and happy forecasting!",
        "dataset_text": "Each season there are thousands of NCAA basketball games played between Division I college basketball teams, culminating in March Madness\u00ae, the 68-team national championship that starts in the middle of March. We have provided a large amount of historical data about college basketball games and teams, going back many years. Armed with this historical data, you can explore it and develop your own distinctive ways of predicting March Madness\u00ae game outcomes. You can even evaluate and compare different approaches by seeing which of them would have done best at predicting tournament games from the past. If you are unfamiliar with the format and intricacies of the NCAA\u00ae tournament, we encourage reading the wikipedia pages for the men's and women's tournaments before before diving into the data.  The data description and schema may seem daunting at first, but is not as complicated as it appears. Please note that in previous years, there were separate competitions for predicting the men's tournament games or the women's tournament games. In this year's competition, you will be submitting combined prediction files that include predictions for both the men's tournament and the women's tournament. Thus the data files incorporate both men's data and women's data. The files that pertain only to men's data will start with the letter prefix M, and the files that pertain only to women's data will start with the letter prefix W. Some files span both men's and women's data, such as Cities and Conferences, and these files do not start with an M prefix or a W prefix. As a reminder, you are encouraged to incorporate your own sources of data. We have provided extensive historical data to jump-start the modeling process, and this data is self-consistent (for instance, dates and team ID's are always treated the same way). Nevertheless, you may also be able to make productive use of external data. If you head down this path, please be forewarned that many sources have their own distinctive way of identifying the names of teams, and this can make it challenging to link up with our data. The MTeamSpellings and WTeamSpellings files, which are listed in the bottom section below, may help you map external team references into our own Team ID structure, and you may also need to understand exactly how dates work in our data. We extend our gratitude to Kenneth Massey for providing much of the historical data. Special Acknowledgment to Jeff Sonas of Sonas Consulting for his support in assembling the dataset for this competition. Warmup Competition - You should submit predicted probabilities for every possible matchup in the past 5 NCAA\u00ae tournaments (2017-2019 and 2021-2022). Note that there was no tournament held in 2020. 2023 Competition - You should submit predicted probabilities for every possible matchup before the 2023 tournament begins. Refer to the Timeline page for specific dates. In both stages, the sample submission will tell you which games to predict. Below we describe the format and fields of the competition data files. All of the files are complete through February 7th of the current season. As we get closer to the tournament in mid-March, we will provide updates to these files to incorporate data from the remaining weeks of the current season. This section provides everything you need to build a simple prediction model and submit predictions. Special note about \"Season\" numbers: the college basketball season lasts from early November until the national championship tournament that starts in the middle of March. For instance, this year the first regular season games were played in November 2022 and the national championship games will be played in April 2023. Because a basketball season spans two calendar years like this, it can be confusing to refer to the year of the season. By convention, when we identify a particular season, we will reference the year that the season ends in, not the year that it starts in. So for instance, the current season will be identified in our data as the 2023 season, not the 2022 season or the 2022-23 season or the 2022-2023 season, though you may see any of these in everyday use outside of our data. Data Section 1 file: MTeams.csv and WTeams.csv These files identify the different college teams present in the dataset (MTeams is for the men's teams and WTeams is for the women's teams). Each school is uniquely identified by a 4 digit id number. Men's team id's start with a 1 and women's team id's start with a 3, and typically there is exactly a difference of 2000 between the men's and women's team id's for a given school. For example, the men's Arizona State team id is 1113 and the women's Arizona State team id is 3113. You will not see games present for all teams in all seasons, because the games listing is only for matchups where both teams are Division-I teams. There are 363 teams currently in Men's Division-I and 361 teams currently in Women's Division-I. Each year, some teams might start being Division-I programs, and others might stop being Division-I programs. So there will be some teams listed in the data only for historical seasons and not for the current season, and thus there are more than 363 men's teams and more than 361 women's teams listed.   Data Section 1 file: MSeasons.csv and WSeasons.csv These files identify the different seasons included in the historical data, along with certain season-level properties. There are separate files for men's data (MSeasons) and women's data (WSeasons). Data Section 1 file: MNCAATourneySeeds.csv and WNCAATourneySeeds.csv These files identify the seeds for all teams in each NCAA\u00ae tournament, for all seasons of historical data. Thus, there are between 64-68 rows for each year, depending on whether there were any play-in games and how many there were. In recent years the structure has settled at 68 total teams, with four \"play-in\" games leading to the final field of 64 teams entering Round 1 on Thursday/Friday of the first week (by definition, that is DayNum=136/137 each season). We will not know the seeds of the respective tournament teams, or even exactly which 68 teams it will be, until Selection Sunday on March 12, 2023 (DayNum=132). Data Section 1 file: MRegularSeasonCompactResults.csv and WRegularSeasonCompactResults.csv These files identify the game-by-game results for many seasons of historical data, starting with the 1985 season for men (the first year the NCAA\u00ae had a 64-team men's tournament) and the 1998 season for women. For each season, the file includes all games played from DayNum 0 through 132. It is important to realize that the \"Regular Season\" games are simply defined to be all games played on DayNum=132 or earlier (DayNum=132 is Selection Sunday, and there are always a few conference tournament finals actually played early in the day on Selection Sunday itself). Thus a game played on or before Selection Sunday will show up here whether it was a pre-season tournament, a non-conference game, a regular conference game, a conference tournament game, or whatever. Data Section 1 file: MNCAATourneyCompactResults.csv and WNCAATourneyCompactResults.csv These files identify the game-by-game NCAA\u00ae tournament results for all seasons of historical data. The data is formatted exactly like the corresponding RegularSeasonCompactResults data. All men's games will show up as neutral site (so WLoc is always N) and some women's games will show up as neutral site, depending on the specifics. Note that this tournament game data also includes the play-in games for those years that had play-in games. Thus each season you will see between 63 and 67 games listed, depending on how many play-in games there were. Because of the consistent structure of the NCAA\u00ae tournament schedule, you can generally tell what round a game was, depending on the exact DayNum. However, the men's 2021 tournament scheduling was slightly different, and the women's scheduling has varied a lot. Nevertheless, in general the men's schedule will be: Special note: Each year, there are also going to be other games that happened after Selection Sunday, which are not part of the NCAA\u00ae Tournament. This includes tournaments like the postseason NIT, the CBI, the CIT, and the Vegas 16. Such games are not listed in the Regular Season or the NCAA\u00ae Tourney files; they can be found in the \"Secondary Tourney\" data files (only for men's data) within Data Section 6. Although they would not be games you would ever be predicting directly for the NCAA\u00ae tournament, and they would not be games you would have data from at the time of predicting NCAA\u00ae tournament outcomes, you may nevertheless wish to make use of these games for model optimization, depending on your methodology. The more games that you can test your predictions against, the better your optimized model might eventually become, depending on how applicable all those games are. A similar argument might be advanced in favor of optimizing your predictions against conference tournament games, which might be viewed as reasonable proxies for NCAA\u00ae tournament games.  Data Section 1 file: SampleSubmissionWarmup.csv and SampleSubmission2023 These files illustrate the submission file format for the warmup competition and the 2023 competition. They reflect the simplest possible submission: a 50% winning percentage is predicted for each possible matchup. Please remember that this file includes predictions for both men's games and women's games, and that it includes all Division I teams, not just the tournament teams. A submission file lists every possible tournament matchup between Division-I teams for one or more years. In the 2023 competition, you will be asked to make predictions for the current season (2023). There are two very important differences between this year's submission file and the submission files used in previous years' competitions. IMPORTANT DIFFERENCE #1: The competition is a blend of men's predictions and women's predictions. Thus your score for a given season will be your overall score across the 126 games (63 men's and 63 women's) that were played among the final 64 teams in each tournament. You cannot submit predictions just for the men's competition or just for the women's competition; the submission file must include predictions for both. IMPORTANT DIFFERENCE #2: The competition supports submissions even before tournament brackets are announced. Therefore, a submission file includes all possible combinations of two teams, rather than all possible combinations of two tournament teams. For example, there are 363 men's Division-I teams this season. A submission file will need to include predictions for all possible pairs of those 363 men's teams, and there are 363*362/2 = 65,703 possible combinations. Similarly, on the women's side, there are 361 Division-I teams this season, which corresponds to 361*360/2=64,980 possible combinations. So a submission file for the 2023 season would have 65,703+64,980=130,683 data rows. Most prediction rows will be discarded because one or both teams failed to make the tournament, or didn't play each other in the tournament. If you want to know which predictions are needed, you can simply parse the data rows in the sample submission file, or you can calculate the combinations yourself. You will need to remember that not all teams in the MTeams or WTeams files are active each year. The easiest way to get a master list of the active teams each year is to look at the MTeamConferences and WTeamConferences data files, which will have one record each season for each active Division-I team, along with telling you which conference they were in. For example, you will find 363 data rows in the MTeamConferences file for the 2023 season, and you will find 361 data rows in the WTeamConferences file for the 2023 season. Example #1: You want to make a prediction for Duke (TeamID=1181) against Arizona (TeamID=1112) in the 2017 men's tournament, with Duke given a 53% chance to win and Arizona given a 47% chance to win. In this case, Arizona has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Arizona's perspective (47%): 2017_1112_1181,0.47 Example #2: You want to make a prediction for Duke (TeamID=3181) against North Carolina (TeamID=3314) in the 2018 women's tournament, with Duke given a 51.6% chance to win and North Carolina given a 48.4% chance to win. In this case, Duke has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Duke's perspective (51.6%): 2018_3181_3314,0.516 Also note that a single prediction row serves as a prediction for each of the two teams' winning chances. So for instance, in Example #1, the submission row of \"2017_1112_1181,0.47\" specifically gives a 47% chance for Arizona to win, and doesn't explicitly mention Duke's 53% chance to win. However, our evaluation utility will automatically infer the winning percentage in the other direction, so a 47% prediction for Arizona to win also means a 53% prediction for Duke to win. And similarly, because the submission row in Example #2 gives Duke a 51.6% chance to beat North Carolina, we will automatically figure out that this also means North Carolina has a 48.4% chance to beat Duke. This section provides game-by-game stats at a team level (free throws attempted, defensive rebounds, turnovers, etc.) for all regular season, conference tournament, and NCAA\u00ae tournament games since the 2002-03 season (men) or since the 2009-10 season (women).  Team Box Scores are provided in \"Detailed Results\" files rather than \"Compact Results\" files. However, the two files are strongly related. In a Detailed Results file, the first eight columns (Season, DayNum, WTeamID, WScore, LTeamID, LScore, WLoc, and NumOT) are exactly the same as a Compact Results file. However, in a Detailed Results file, there are many additional columns. The column names should be self-explanatory to basketball fans (as above, \"W\" or \"L\" refers to the winning or losing team): (and then the same set of stats from the perspective of the losing team: LFGM is the number of field goals made by the losing team, and so on up to LPF). Note: by convention, \"field goals made\" (either WFGM or LFGM) refers to the total number of fields goals made by a team, a combination of both two-point field goals and three-point field goals. And \"three point field goals made\" (either WFGM3 or LFGM3) is just the three-point fields goals made, of course. So if you want to know specifically about two-point field goals, you have to subtract one from the other (e.g., WFGM - WFGM3). And the total number of points scored is most simply expressed as 2*FGM + FGM3 + FTM. Data Section 2 file: MRegularSeasonDetailedResults.csv and WRegularSeasonDetailedResults.csv These files provide team-level box scores for many regular seasons of historical data, starting with the 2003 season (men) or starting with the 2010 season (women). All games listed in the MRegularSeasonCompactResults file since the 2003 season should exactly be present in the MRegularSeasonDetailedResults file, and similarly, all games listed in the WRegularSeasonCompactResults file since the 2010 season should exactly be present in the WRegularSeasonDetailedResults file. Data Section 2 file: MNCAATourneyDetailedResults.csv and WNCAATourneyDetailedResults.csv These files provide team-level box scores for many NCAA\u00ae tournaments, starting with the 2003 season (men) or starting with the 2010 season (women). Similarly, all games listed in the MNCAATourneyCompactResults or MNCAATourneyCompactResults file for those seasons should exactly be present in the corresponding MNCAATourneyDetailedResults or WNCAATourneyDetailedResults file. This section provides city locations of all regular season, conference tournament, and NCAA\u00ae tournament games since the 2009-10 season Data Section 3 file: Cities.csv This file provides a master list of cities that have been locations for games played. Please notice that the Cities and Conferences files are the only two that don't start with an M; this is because the data files are identical between men's and women's data, so you don't need to maintain separate listings of cities or conferences across the two datasets. Also note that if you created any supplemental data in previous years on cities (latitude/longitude, altitude, city-to-city distances, etc.), the CityID's match between previous years and this year, so you should be able to re-use that information. Data Section 3 file: MGameCities.csv and WGameCities.csv This file identifies all games, starting with the 2010 season, along with the city that the game was played in. Games from the regular season, the NCAA\u00ae tourney, and other post-season tournaments (men's data only), are all listed together. There should be no games since the 2010 season where the CityID is not known. Games from the 2009 season and before are not listed in this file. This section provides weekly team rankings (men's teams only) for dozens of top rating systems - Pomeroy, Sagarin, RPI, ESPN, etc., since the 2002-2003 season Data Section 4 file: MMasseyOrdinals.csv This file lists out rankings (e.g. #1, #2, #3, ..., #N) of men's teams going back to the 2002-2003 season, under a large number of different ranking system methodologies. The information was gathered by Kenneth Massey and provided on his College Basketball Ranking Composite page. Note that a rating system is more precise than a ranking system, because a rating system can provide insight about the strength gap between two adjacently-ranked teams. A ranking system will just tell you who is #1 or who is #2, but a rating system might tell you whether the gap between #1 and #2 is large or small. Nevertheless, it can be hard to compare two different rating systems that are expressed in different scales, so it can be very useful to express all the systems in terms of their ordinal ranking (1, 2, 3, ..., N) of teams. Disclaimer: you ought to be careful about your methodology when using or evaluating these ranking systems. They are presented on a weekly basis, and given a consistent date on the Massey Composite page that typically is a Sunday; that is how the ranking systems can be compared against each other on this page. However, these systems each follow their own timeline and some systems may be released on a Sunday and others on a Saturday or Monday or even Tuesday. You should remember that if a ranking is released on a Tuesday, and was calculated based on games played through Monday, it will make the system look unusually good at predicting if you use that system to forecast the very games played on Monday that already inform the rankings. To avoid this methodological trap, we have typically used a conservative RankingDayNum of Wednesday to represent the rankings that were released at approximately the end of the weekend, a few days before, even though those rankings are represented on the composite page as being on a Sunday. For some of the older years, a more precise timestamp was known for each ranking system that allowed a more precise assignment of a RankingDayNum. By convention, the final pre-tournament rankings are always expressed as RankingDayNum=133, even though sometimes the rankings for individual systems are not released until Tuesday (DayNum=134) or even Wednesday or Thursday. If you decide to use some rankings from these Massey Ordinals to inform your predictions, be forewarned that we have no control over when they are released, and not all systems may turn out to be available in time to make pre-tournament predictions by our submission deadline. In such a situation, you may wish to use the rankings from DayNum=128 or you may need to dig into the details of the actual source of the rankings, by following the respective links on the Massey Composite Page. We may also be able to provide partial releases of the final pre-tournament Massey Ordinals on the forums, so that as systems come in on Monday or Tuesday you can use them right away. This section contains additional supporting information, including coaches, conference affiliations, alternative team name spellings, bracket structure, and game results for NIT and other postseason tournaments. Data Section 5 file: MTeamCoaches.csv This file indicates the head coach for each team in each season, including a start/finish range of DayNum's to indicate a mid-season coaching change. For scenarios where a team had the same head coach the entire season, they will be listed with a DayNum range of 0 to 154 for that season. For head coaches whose term lasted many seasons, there will be many rows listed, most of which have a DayNum range of 0 to 154 for the corresponding season. Data Section 5 file: Conferences.csv This file indicates the Division I conferences that have existed over the years since 1985. Each conference is listed with an abbreviation and a longer name. There has been no attempt to link up conferences who merged with other conferences, or whose names changed over time. Thus you will see, for instance, a \"Pacific-10\" conference up through the 2011 season, and then a \"Pacific-12\" conference starting in the 2012 season, and these look like different conferences in the data, even though it was still mostly the same teams. Please notice that the Cities and Conferences files are the only two that don't start with an M; this is because the data files are identical between men's and women's data, so you don't need to maintain separate listings of cities or conferences across the two datasets. However, the Team Conferences data differs slightly between men's and women's, so those files do have the prefixes. Data Section 5 files: MTeamConferences.csv and WTeamConferences.csv These files indicate the conference affiliations for each team during each season. Some conferences have changed their names from year to year, and/or changed which teams are part of the conference. These files tracks this information historically, for men's and women's teams separately. Data Section 5 file: MConferenceTourneyGames.csv This file indicates which games were part of each year's post-season men's conference tournaments (all of which finished on Selection Sunday or earlier), starting from the 2001 season. Many of these conference tournament games are held on neutral sites, and many of the games are played by tournament-caliber teams just a few days before the NCAA\u00ae tournament. Thus these games could be considered as very similar to NCAA\u00ae tournament games, and (depending on your methodology) may be of use in optimizing your predictions. However, this is NOT a new listing of games; these games are already present within the MRegularSeasonCompactResults and MRegularSeasonDetailedResults files. So this file simply helps you to identify which of the \"regular season\" games since the 2001 season were actually conference tournament games, in case that is useful information. Data Section 5 file: MSecondaryTourneyTeams.csv This file identifies the teams that participated in post-season men's tournaments other than the NCAA\u00ae Tournament (such events would run in parallel with the NCAA\u00ae Tournament). These are teams that were not invited to the NCAA\u00ae Tournament and instead were invited to some other tournament, of which the NIT is the most prominent tournament, but there have also been the CBI, CIT, Vegas 16 (V16), and The Basketball Classic (TBC) at various points in recent years. Depending on your methodology, you might find it useful to have these additional game results, above and beyond what is available from the NCAA\u00ae Tournament results. Many of these teams, especially in the NIT, were \"bubble\" teams of comparable strength to several NCAA\u00ae Tournament invitees, and so these games may be of use in model optimization for predicting NCAA\u00ae Tournament results. Also note that this information could be determined just from inspecting the MSecondaryTourneyCompactResults file, but is presented in this file as well, for your convenience. Data Section 5 file: MSecondaryTourneyCompactResults.csv This file indicates the final scores for the tournament games of \"secondary\" post-season tournaments: the NIT, CBI, CIT, and Vegas 16. The detailed results (team box scores) have not been assembled for these games. For the most part, this file is exactly like other Compact Results listings, although it also has a column for Secondary Tourney. Also note that because these games are played after DayNum=132, they are NOT listed in the MRegularSeasonCompactResults file. Data Section 5 files: MTeamSpellings.csv and WTeamSpellings.csv These files indicate alternative spellings of many team names. They are intended for use in associating external spellings against our own TeamID numbers, thereby helping to relate the external data properly with our datasets. Over the years we have identified various external spellings of different team names (as an example, for Ball State we have seen \"ball st\", and \"ball st.\", and \"ball state\", and \"ball-st\", and \"ball-state\"). Other teams have had more significant changes to their names over the years; for example, \"Texas Pan-American\" and \"Texas-Rio Grande Valley\" are actually the same school. The current list is obviously not exhaustive, and we encourage participants to identify additional mappings and upload extended versions of this file to the forums. Data Section 5 files: MNCAATourneySlots and WNCAATourneySlots These files identify the mechanism by which teams are paired against each other, depending upon their seeds, as the tournament proceeds through its rounds. They can be of use in identifying, for a given historical game, what round it occurred in, and what the seeds/slots were for the two teams (the meaning of \"slots\" is described below). Because of the existence of play-in games for particular seed numbers, the pairings have small differences from year to year. You may need to know these specifics if you are trying to represent/simulate the exact workings of the tournament bracket. Data Section 5 file: MNCAATourneySeedRoundSlots.csv This file helps to represent the men's bracket structure in any given year. No matter where the play-in seeds are located, we can always know, for a given tournament seed, exactly what bracket slot they would be playing in, on each possible game round, and what the possible DayNum values would be for that round. Thus, if we know when a historical game was played, and what the team's seed was, we can identify the slot for that game. This can be useful in representing or simulating the tournament bracket structure. The women's scheduling has varied a lot more and does not lend itself to this common structure and so there is not a corresponding file for the women's data. Also note that the 2021 men's tournament had unusual scheduling and did not follow the traditional assignment of DayNums for each round."
    },
    {
        "name": "RSNA 2023 Abdominal Trauma Detection",
        "url": "https://www.kaggle.com/competitions/rsna-2023-abdominal-trauma-detection",
        "overview_text": "Overview text not found",
        "description_text": "Traumatic injury is the most common cause of death in the first four decades of life and a major public health problem around the world. There are estimated to be more than 5 million annual deaths worldwide from traumatic injury. Prompt and accurate diagnosis of traumatic injuries is crucial for initiating appropriate and timely interventions, which can significantly improve patient outcomes and survival rates. Computed tomography (CT) has become an indispensable tool in evaluating patients with suspected abdominal injuries due to its ability to provide detailed cross-sectional images of the abdomen. Interpreting CT scans for abdominal trauma, however, can be a complex and time-consuming task, especially when multiple injuries or areas of subtle active bleeding are present. This challenge seeks to harness the power of artificial intelligence and machine learning to assist medical professionals in rapidly and precisely detecting injuries and grading their severity. The development of advanced algorithms for this purpose has the potential to improve trauma care and patient outcomes worldwide. Blunt force abdominal trauma is among the most common types of traumatic injury, with the most frequent cause being motor vehicle accidents. Abdominal trauma may result in damage and internal bleeding of the internal organs, including the liver, spleen, kidneys, and bowel. Detection and classification of injuries are key to effective treatment and favorable outcomes. A large proportion of patients with abdominal trauma require urgent surgery. Abdominal trauma often cannot be diagnosed clinically by physical exam, patient symptoms, or laboratory tests. Prompt diagnosis of abdominal trauma using medical imaging is thus critical to patient care. AI tools that assist and expedite diagnosis of abdominal trauma have the potential to substantially improve patient care and health outcomes in the emergency setting. The RSNA Abdominal Trauma Detection AI Challenge, organized by the RSNA in collaboration with the American Society of Emergency Radiology (ASER) and the Society for Abdominal Radiology (SAR), gives researchers the task of building models that detect severe injury to the internal abdominal organs, including the liver, kidneys, spleen, and bowel, as well as any active internal bleeding.",
        "dataset_text": "The goal of this competition is to identify several potential injuries in CT scans of trauma patients. Any of these injuries can be fatal on a short time frame if untreated so there is great value in rapid diagnosis. This competition uses a hidden test. When your submitted notebook is scored, the actual test data (including a full length sample submission) will be made available to your notebook. train.csv Target labels for the train set. Note that patients labeled healthy may still have other medical issues, such as cancer or broken bones, that don't happen to be covered by the competition labels. [train/test]_images/[patient_id]/[series_id]/[image_instance_number].dcm The CT scan data, in DICOM format. Scans from dozens of different CT machines have been reprocessed to use the run length encoded lossless compression format but retain other differences such as the number of bits per pixel, pixel range, and pixel representation. Expect to see roughly 1,100 patients in the test set. [train/test]_series_meta.csv Each patient may have been scanned once or twice. Each scan contains a series of images. sample_submission.csv A valid sample submission. Only the first few rows are available for download. image_level_labels.csv Train only. Identifies specific images that contain either bowel or extravasation injuries. segmentations/ Model generated pixel-level annotations of the relevant organs and some major bones for a subset of the scans in the training set. This data is provided in the nifti file format. The filenames are series IDs. You can find a description of the source model (total segmentator) here and the data used to train that model here. Note that the NIFTI files and DICOM files are not in the same orientation. Use the NIFTI header information along with DICOM metadata to determine the appropriate orientation. [train/test]_dicom_tags.parquet DICOM tags from every image, extracted with Pydicom. Provided for convenience."
    },
    {
        "name": "Child Mind Institute - Detect Sleep States",
        "url": "https://www.kaggle.com/competitions/child-mind-institute-detect-sleep-states",
        "overview_text": "Your work will improve researchers' ability to analyze accelerometer data for sleep monitoring and enable them to conduct large-scale studies of sleep. Ultimately, the work of this competition could improve awareness and guidance surrounding the importance of sleep. The valuable insights into how environmental factors impact sleep, mood, and behavior can inform the development of personalized interventions and support systems tailored to the unique needs of each child.",
        "description_text": "The goal of this competition is to detect sleep onset and wake. You will develop a model trained on wrist-worn accelerometer data in order to determine a person's sleep state. Your work could make it possible for researchers to conduct more reliable, larger-scale sleep studies across a range of populations and contexts. The results of such studies could provide even more information about sleep. The successful outcome of this competition can also have significant implications for children and youth, especially those with mood and behavior difficulties. Sleep is crucial in regulating mood, emotions, and behavior in individuals of all ages, particularly children. By accurately detecting periods of sleep and wakefulness from wrist-worn accelerometer data, researchers can gain a deeper understanding of sleep patterns and better understand disturbances in children. The \u201cZzzs\u201d you catch each night are crucial for your overall health. Sleep affects everything from your development to cognitive functioning. Even so, research into sleep has proved challenging, due to the lack of naturalistic data capture alongside accurate annotation. If data science could help researchers better analyze wrist-worn accelerometer data for sleep monitoring, sleep experts could more easily conduct large-scale studies of sleep, thus improving the understanding of sleep's importance and function. Current approaches for annotating sleep data include sleep logs, which are the gold standard for detecting the onset of sleep. However, they are impractical for many participants to use reliably, and fail to capture the nuanced difference between heading to bed and falling asleep (or, conversely, waking up and getting out of bed). Heuristic-based software is another solution that attempts to identify sleep windows, though these rely on human-engineered features of sleep (i.e. arm angle) that vary across individuals and don\u2019t accurately summarize the sleep windows that experts can visually detect from their data. With improved tools to analyze sleep data on a large scale, researchers can explore the relationship between sleep and mood/behavioral difficulties. This knowledge can lead to more targeted interventions and treatment strategies. Competition host Child Mind Institute (CMI) transforms the lives of children and families struggling with mental health and learning disorders by giving them the help they need. CMI has become the leading independent nonprofit in children\u2019s mental health by providing gold-standard evidence-based care, delivering educational resources to millions of families each year, training educators in underserved communities, and developing tomorrow\u2019s breakthrough treatments. Established with a foundational grant from the Stavros Niarchos Foundation (SNF), the SNF Global Center for Child and Adolescent Mental Health at the Child Mind Institute works to accelerate global collaboration on under-researched areas of children\u2019s mental health and expand worldwide access to culturally appropriate trainings, resources, and treatment. A major goal of the SNF Global Center is to expand innovations in clinical assessment and intervention, to include building, testing, and deploying new technologies to augment mental health care and research, including mobile apps, sensors, and analytical tools. Your work will improve researchers' ability to analyze accelerometer data for sleep monitoring and enable them to conduct large-scale studies of sleep. Ultimately, the work of this competition could improve awareness and guidance surrounding the importance of sleep. The valuable insights into how environmental factors impact sleep, mood, and behavior can inform the development of personalized interventions and support systems tailored to the unique needs of each child. The data used for this competition was provided by the Healthy Brain Network, a landmark mental health study based in New York City that will help children around the world. In the Healthy Brain Network, families, community leaders, and supporters are partnering with the Child Mind Institute to unlock the secrets of the developing brain. In addition to generous support provided by the Kaggle team, financial support has been provided by the Stavros Niarchos Foundation (SNF) as part of its Global Health Initiative (GHI) through the SNF Global Center for Child and Adolescent Mental Health at the Child Mind Institute. ",
        "dataset_text": "The dataset comprises about 500 multi-day recordings of wrist-worn accelerometer data annotated with two event types: onset, the beginning of sleep, and wakeup, the end of sleep. Your task is to detect the occurrence of these two events in the accelerometer series. While sleep logbooks remain the gold-standard, when working with accelerometer data we refer to sleep as the longest single period of inactivity while the watch is being worn. For this data, we have guided raters with several concrete instructions: Though each series is a continuous recording, there may be periods in the series when the accelerometer device was removed. These period are determined as those where suspiciously little variation in the accelerometer signals occur over an extended period of time, which is unrealistic for typical human participants. Events are not annotated for these periods, and you should attempt to refrain from making event predictions during these periods: an event prediction will be scored as false positive. Each data series represents this continuous (multi-day/event) recording for a unique experimental subject. Note that this is a Code Competition, in which the actual test set is hidden. In this public version, we give some sample data in the correct format to help you author your solutions. The full test set contains about 200 series."
    },
    {
        "name": "Kaggle - LLM Science Exam",
        "url": "https://www.kaggle.com/competitions/kaggle-llm-science-exam",
        "overview_text": "Overview text not found",
        "description_text": "Inspired by the OpenBookQA dataset, this competition challenges participants to answer difficult science-based questions written by a Large Language Model. Your work will help researchers better understand the ability of LLMs to test themselves, and the potential of LLMs that can be run in resource-constrained environments. As the scope of large language model capabilities expands, a growing area of research is using LLMs to characterize themselves. Because many preexisting NLP benchmarks have been shown to be trivial for state-of-the-art models, there has also been interesting work showing that LLMs can be used to create more challenging tasks to test ever more powerful models. At the same time methods like quantization and knowledge distillation are being used to effectively shrink language models and run them on more modest hardware. The Kaggle environment provides a unique lens to study this as submissions are subject to both GPU and time limits. The dataset for this challenge was generated by giving gpt3.5 snippets of text on a range of scientific topics pulled from wikipedia, and asking it to write a multiple choice question (with a known answer), then filtering out easy questions. Right now we estimate that the largest models run on Kaggle are around 10 billion parameters, whereas gpt3.5 clocks in at 175 billion parameters. If a question-answering model can ace a test written by a question-writing model more than 10 times its size, this would be a genuinely interesting result; on the other hand if a larger model can effectively stump a smaller one, this has compelling implications on the ability of LLMs to benchmark and test themselves. ",
        "dataset_text": "Your challenge in this competition is to answer multiple-choice questions written by an LLM. While the specifics of the process used to generate these questions aren't public, we've included 200 sample questions with answers to show the format, and to give a general sense of the kind of questions in the test set. However, there may be a distributional shift between the sample questions and the test set, so solutions that generalize to a broad set of questions are likely to perform better. Each question consists of a prompt (the question), 5 options labeled A, B, C, D, and E, and the correct answer labeled answer (this holds the label of the most correct answer, as defined by the generating LLM). This competition uses a hidden test. When your submitted notebook is scored, the actual test data (including a sample submission) will be made available to your notebook. The test set has the same format as the provided test.csv but has ~4000 questions that may be different is subject matter."
    },
    {
        "name": "LEAP - Atmospheric Physics using AI (ClimSim)",
        "url": "https://www.kaggle.com/competitions/leap-atmospheric-physics-ai-climsim",
        "overview_text": "In this competition, you\u2019ll develop machine learning models that accurately emulate subgrid-scale atmospheric physics in an operational climate model\u2014an important step in improving climate projections and reducing uncertainty surrounding future climate trends.",
        "description_text": "Climate models are essential to understanding Earth\u2019s climate system. Because of the complexity of Earth\u2019s climate, these models rely on parameterizations to approximate the effects of physical processes that occur at scales smaller than the size of their grid cells. These approximations are imperfect, however, and their imperfections are a leading source of uncertainty in expected warming, changing precipitation patterns, and the frequency and severity of extreme events. The Multi-scale Modeling Framework (MMF) approach, by contrast, more explicitly represents these subgrid processes, but at a cost too high to be used for operational climate prediction. Your task is to develop ML models that emulate subgrid atmospheric processes\u2013such as storms, clouds, turbulence, rainfall, and radiation\u2013within E3SM-MMF, a multi-scale climate model backed by the U.S. Department of Energy. Because ML emulators are significantly cheaper to inference than MMF, progress on this front can help scientists realize a future in which high-resolution and physically credible long-term climate projections are broadly accessible, bringing greater clarity to the hazards associated with climate change and empowering policymakers with the knowledge necessary to mitigate them. This competition accompanies an upcoming 2024 ICML Machine Learning for Earth System Modeling (ML4ESM) Workshop and is based on the ClimSim paper and dataset which won the Outstanding Datasets and Benchmarks Paper award at NeurIPS 2023. Winning submissions will be highlighted at the upcoming ML4ESM ICML workshop, and participants in this Kaggle competition are also encouraged to submit workshop papers. The workshop itself will have its own best paper award and an accompanying cash prize of $3,000.",
        "dataset_text": "The dataset (both training and test) for this competition is generated by the (E3SM-MMF) climate model. The multi-scale nature of E3SM-MMF allows it to explicitly resolve the effects of small-scale processes such as clouds and storms on large-scale climate patterns. However, this multi-scale framework comes at a great computational cost, limiting its usage for experiments and ensemble climate projections. The goal is to train a model to emulate the effects of these small-scale processes at a fraction of the cost of explicitly resolving them. Every row of the training set corresponds to the inputs and outputs of a cloud-resolving model (CRM) in E3SM-MMF at some location and timestep. There are 556 columns corresponding to 25 input variables and 368 columns corresponding to 14 target variables. Some variables (like air temperature) span an entire atmospheric column and have 60 vertical levels, and other variables (like precipitation) are scalars. For the vertically resolved variables, an \"_\" followed by a number in the range [0,59] is appended to denote vertical level. Lower numbers denote higher positions in the atmosphere. Your goal is to create a model that predicts the target variables associated with a given set of input variables.  To make this competition more accessible, we make use of a subset of the low-resolution data from ClimSim using the full variable list. While we provide all of the data necessary for the competition on Kaggle, competitors are welcome to use HuggingFace if they'd like access to even more training data. A script for recreating the training data using files downloaded from HuggingFace can be found here. It is worth noting that the models in the ClimSim paper also make use of the low-resolution data but do not use the full-variable list. For the purposes of this competition, you will need the following three files to train your model and submit your predictions: Please remember to use the sample_submission.csv file to weight your predictions prior to submission, as the reference solution is weighted the same way. This weighting can also be calculated without downloading this file. The top 12 levels (0-11 inclusive) for 'ptend_q0001', 'ptend_q0002', 'ptend_q0003', 'ptend_u', and 'ptend_v' can be ignored. These values are zeroed out by the prediction weightings in sample_submission.csv. For more information, please consult the for_kaggle_users.py script in the ClimSim GitHub repo. Given the size of the data, we recommend downloading one file at a time and converting them to a more manageable format (like parquet or npy). If you have access to a High-Performance Computing (HPC) cluster with sufficient storage availability, you can also download the data directly to your remote cluster using the Kaggle API with the following command: kaggle competitions download -c ClimSim If you choose to go down this route, please be sure to follow the instructions listed on the Kaggle API GitHub repo. EDIT: The test.csv and sample_submission.csv have been updated. Please use these files for all future submissions. Submissions made prior to 6/18/24 will be invalidated, and competitors should be resubmitting with the new data. The previous versions are still available for your reference."
    },
    {
        "name": "Enefit - Predict Energy Behavior of Prosumers",
        "url": "https://www.kaggle.com/competitions/predict-energy-behavior-of-prosumers",
        "overview_text": "The goal of the competition is to create an energy prediction model of prosumers to reduce energy imbalance costs.",
        "description_text": "The number of prosumers is rapidly increasing, and solving the problems of energy imbalance and their rising costs is vital. If left unaddressed, this could lead to increased operational costs, potential grid instability, and inefficient use of energy resources. If this problem were effectively solved, it would significantly reduce the imbalance costs, improve the reliability of the grid, and make the integration of prosumers into the energy system more efficient and sustainable. Moreover, it could potentially incentivize more consumers to become prosumers, knowing that their energy behavior can be adequately managed, thus promoting renewable energy production and use. Enefit is one of the biggest energy companies in Baltic region. As experts in the field of energy, we help customers plan their green journey in a personal and flexible manner as well as implement it by using environmentally friendly energy solutions.\nAt present, Enefit is attempting to solve the imbalance problem by developing internal predictive models and relying on third-party forecasts. However, these methods have proven to be insufficient due to their low accuracy in forecasting the energy behavior of prosumers. The shortcomings of these current methods lie in their inability to accurately account for the wide range of variables that influence prosumer behavior, leading to high imbalance costs. By opening up the challenge to the world's best data scientists through the Kaggle platform, Enefit aims to leverage a broader pool of expertise and novel approaches to improve the accuracy of these predictions and consequently reduce the imbalance and associated costs.",
        "dataset_text": "Your challenge in this competition is to predict the amount of electricity produced and consumed by Estonian energy customers who have installed solar panels. You'll have access to weather data, the relevant energy prices, and records of the installed photovoltaic capacity. This is a forecasting competition using the time series API. The private leaderboard will be determined using real data gathered after the submission period closes. \ud83d\udca1 Nota bene:\nAll datasets follow the same time convention. Time is given in EET/EEST. Most of the variables are a sum or an average over a period of 1 hour. The datetime column (whatever its name) always gives the start of the 1-hour period. However, for the weather datasets, some variables such as temperature or cloud cover, are given for a specific time, which is always the end of the 1-hour period. train.csv gas_prices.csv client.csv electricity_prices.csv forecast_weather.csv Weather forecasts that would have been available at prediction time. Sourced from the European Centre for Medium-Range Weather Forecasts. historical_weather.csv Historic weather data. public_timeseries_testing_util.py An optional file intended to make it easier to run custom offline API tests. See the script's docstring for details. You will need to edit this file before using it. example_test_files/ Data intended to illustrate how the API functions. Includes the same files and columns delivered by the API. The first three data_block_ids are repeats of the last three data_block_ids in the train set. example_test_files/sample_submission.csv A valid sample submission, delivered by the API. See this notebook for a very simple example of how to use the sample submission. example_test_files/revealed_targets.csv The actual target values from the day before the forecast time. This amounts to two days of lag relative to the prediction times in the test.csv. enefit/ Files that enable the API. Expect the API to deliver all rows in under 15 minutes and to reserve less than 0.5 GB of memory. The copy of the API that you can download serves the data from example_test_files/. You must make predictions for those dates in order to advance the API but those predictions are not scored. Expect to see roughly three months of data delivered initially and up to ten months of data by the end of the forecasting period."
    },
    {
        "name": "AI Village Capture the Flag @ DEFCON31",
        "url": "https://www.kaggle.com/competitions/ai-village-capture-the-flag-defcon31",
        "overview_text": "The popular and competitive game, Capture the Flag (CTF), isn't only played outdoors. A digital CTF refers to a set of challenges or puzzles that require participants to exploit vulnerabilities, solve problems, or uncover hidden information within computer systems, software, or networks.",
        "description_text": "CTFs are puzzles meant to challenge you and help you learn new things. Sometimes they may be a little ambiguous or misleading. That's part of the challenge! Once you successfully solve a challenge, you\u2019ll capture a \"digital flag\" (a unique-to-you string). Save that flag, then upload it to receive credit and move up the Kaggle leaderboard. Feel free to drop questions in the Kaggle Discussion forums, but don\u2019t share code or spoilers! Please do not try and hack any infrastructure or share flags. Any teams found sharing flags will be disqualified. For help, contact us on Discord or use the Kaggle discussion board. Competition host AI Village is a community of hackers and data scientists working to educate the world on the use and abuse of artificial intelligence in security and privacy. They aim to bring more diverse viewpoints to this field and grow the community of hackers, engineers, researchers, and policymakers working on making AI safer. They believe that we need more people with a hacker mindset assessing and analyzing machine learning systems. They have a presence at DEFCON, the world\u2019s longest-running and largest hacking conference. Winners will be asked to share their notebooks. Checkout the starter notebook!",
        "dataset_text": "This CTF follows a different flow than most Kaggle competitions. Competitors will be interacting with an externally hosted API endpoint during each of the challenges, using the data provided below. Information about how to interact with this endpoint is provided in the template notebook. You may interact with the challenges from your local host or any other machine that can access the internet. The template notebook is just one method for submitting flags to the scoreboard - don't feel constrained to that single operating environment. Once you\u2019ve finished, add your flags to the notebook to update the scoreboard - find our template here. This notebook is simply to get you started with the challenges. For each challenge, there is a prompt to help guide your thinking toward solving the challenge, and a web request to help interact with the the model or flag server. Each challenge is unique, but you will be able to use the same code within levels - Granny 2 is an evolution of Granny and so on. Challenges generate a unique flag - so don't share them, flags will be obvious and will begin with the following pattern gAAAAABl. Use the starting query function provided to interact with the challenge. This is the only endpoint and scanning for other paths or services is not necessary. You should build your solutions and strategies around these web requests. Errors try to be helpful and nudge you in the right direction. To start, run the sample request manually, read the error or prompt, and start modifying your inputs until a flag falls out. Here is a handy link to Kaggle's competition documentation, which includes instructions on submitting predictions."
    },
    {
        "name": "Santa 2023 - The Polytope Permutation Puzzle",
        "url": "https://www.kaggle.com/competitions/santa-2023",
        "overview_text": "Overview text not found",
        "description_text": " Deck the halls with polyhedrons.\nFa la la la la la la la la\nSilly elves have dreadful deeds done\nFa la la la la la la la la\n\nMixed the colors of each side\nOh No No No No No No No No\nTo try and spoil this Christmastide\nOh No No No No No No No No\n\nWe need your help to fix our tree\nPlease Oh Please Oh Please Oh Please Oh Please\nBefore the Big Man comes to see\nPlease Oh Please Oh Please Oh Please Oh Please Every year at the North Pole, elves adorn the workshop's big tree with beautiful ornaments. These decorations come in many geometric shapes, with different colors and markings on each side. Unfortunately, rogue elves have tried to ruin Christmas and have mixed their sides up like unsolved puzzle cubes. Now we need your help to make things right before Santa notices! In this competition, you will solve cube-like puzzles in the fewest moves, but instead of the usual cubes, the puzzles come in a variety of geometric shapes. This sort of magic is no sweat for Santa, but may challenge your data science skills. The entire workshop is counting on you! Can you help the elves efficiently solve their ornament puzzles?",
        "dataset_text": "Your task in this competition is to solve each of the given permutation puzzles in the fewest number of moves. See the Evaluation page for more details."
    },
    {
        "name": "March Machine Learning Mania 2024",
        "url": "https://www.kaggle.com/competitions/march-machine-learning-mania-2024",
        "overview_text": "You will be forecasting the outcomes of both the men's and women's 2024 collegiate basketball tournaments, by submitting an portfolio of brackets based on historical data.",
        "description_text": "Another year, another chance to predict the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. In our tenth annual March Machine Learning Mania competition, Kagglers will once again join the millions of fans who attempt to predict the outcomes of this year's college basketball tournaments. Unlike most fans, you will pick the winners and losers using a combination of rich historical data and computing power, while the ground truth unfolds on television. You are provided data of historical NCAA games to forecast the outcomes of the Division 1 Men's and Women's basketball tournaments. This competition is the official 2024 edition, with points, medals, prizes, and basketball glory at stake. We have made an update to the competition format this year. You will be making bracket predictions for both tournaments, similar to traditional tournament contests, but you will be allowed to submit an entire portfolio of brackets, anywhere from 1 to 100,000. Good luck and happy forecasting! Please note that submissions to this competition must be made through Kaggle Notebooks. This will enable Kaggle to rerun your submissions using the final qualifying tournament teams, prior to the start of the tournaments. See Code Requirements below for more details on how to submit.",
        "dataset_text": "Each season there are thousands of NCAA basketball games played between Division I college basketball teams, culminating in March Madness\u00ae, the 68-team national championship that starts in the middle of March. We have provided a large amount of historical data about college basketball games and teams, going back many years. Armed with this historical data, you can explore it and develop your own distinctive ways of predicting March Madness\u00ae game outcomes. You can even evaluate and compare different approaches by seeing which of them would have done best at predicting tournament games from the past. If you are unfamiliar with the format and intricacies of the NCAA\u00ae tournament, we encourage reading the wikipedia pages for the men's and women's tournaments before before diving into the data.  The data description and schema may seem daunting at first, but is not as complicated as it appears. Please note that in previous years, there were separate competitions for predicting the men's tournament games or the women's tournament games. That changed last year. Once again, you will be submitting combined prediction files that include predictions for both the men's tournament and the women's tournament. Thus the data files incorporate both men's data and women's data. The files that pertain only to men's data will start with the letter prefix M, and the files that pertain only to women's data will start with the letter prefix W. Some files span both men's and women's data, such as Cities and Conferences, and these files do not start with an M prefix or a W prefix. As a reminder, you are encouraged to incorporate your own sources of data. We have provided extensive historical data to jump-start the modeling process, and this data is self-consistent (for instance, dates and team ID's are always treated the same way). Nevertheless, you may also be able to make productive use of external data. If you head down this path, please be forewarned that many sources have their own distinctive way of identifying the names of teams, and this can make it challenging to link up with our data. The MTeamSpellings and WTeamSpellings files, which are listed in the bottom section below, may help you map external team references into our own Team ID structure, and you may also need to understand exactly how dates work in our data. We extend our gratitude to Kenneth Massey for providing much of the historical data. Special Acknowledgment to Jeff Sonas of Sonas Consulting for his support in assembling the dataset for this competition. Please note that this is a Code Competition. You will submit a notebook that produces a portfolio of bracket predictions for both the Men's and Women's tournaments. During the submission phase of the competition, we provide a 2024_tourney_seeds.csv file that actually contains the seeds from 2023 and will score your submissions against the 2023 tournament results. After Selection Sunday, when the competition has closed, we will replace this file with the actual 2024 tournament selections and rescore your submissions against the 2024 results. You may find details of the submission format and evaluation metric on the Evaluation page. Below we describe the format and fields of the competition data files. All of the files are complete through February 14th of the current season. As we get closer to the tournament, we will periodically provide updates to these files to incorporate data from the remaining weeks of the current season. This section provides everything you need to build a simple prediction model and submit predictions. Special note about \"Season\" numbers: the college basketball season lasts from early November until the national championship tournament that starts in the middle of March. For instance, this year the first regular season games were played in November 2023 and the national championship games will be played in April 2024. Because a basketball season spans two calendar years like this, it can be confusing to refer to the year of the season. By convention, when we identify a particular season, we will reference the year that the season ends in, not the year that it starts in. So for instance, the current season will be identified in our data as the 2024 season, not the 2023 season or the 2023-24 season or the 2023-2024 season, though you may see any of these in everyday use outside of our data. Data Section 1 file: MTeams.csv and WTeams.csv These files identify the different college teams present in the dataset (MTeams is for the men's teams and WTeams is for the women's teams). Each school is uniquely identified by a 4 digit id number. Men's team id's start with a 1 and women's team id's start with a 3, and typically there is exactly a difference of 2000 between the men's and women's team id's for a given school. For example, the men's Arizona State team id is 1113 and the women's Arizona State team id is 3113. You will not see games present for all teams in all seasons, because the games listing is only for matchups where both teams are Division-I teams. There are 362 teams currently in Men's Division-I and 360 teams currently in Women's Division-I. Each year, some teams might start being Division-I programs, and others might stop being Division-I programs. For example, this year there is one new Division-I program (Le Moyne) and two programs that have stopped being Division-I programs (Hartford and St Francis NY). So there will be some teams listed in the data only for historical seasons and not for the current season, and thus there are more than 362 men's teams and more than 360 women's teams listed.   Data Section 1 file: MSeasons.csv and WSeasons.csv These files identify the different seasons included in the historical data, along with certain season-level properties. There are separate files for men's data (MSeasons) and women's data (WSeasons). Data Section 1 file: MNCAATourneySeeds.csv and WNCAATourneySeeds.csv These files identify the seeds for all teams in each NCAA\u00ae tournament, for all seasons of historical data. Thus, there are between 64-68 rows for each year, depending on whether there were any play-in games and how many there were. In recent years the structure has settled at 68 total teams, with four \"play-in\" games leading to the final field of 64 teams entering Round 1 on Thursday/Friday of the first week (by definition, that is DayNum=136/137 each season). We will not know the seeds of the respective tournament teams, or even exactly which 68 teams it will be, until Selection Sunday on March 17, 2024 (DayNum=132). Data Section 1 file: MRegularSeasonCompactResults.csv and WRegularSeasonCompactResults.csv These files identify the game-by-game results for many seasons of historical data, starting with the 1985 season for men (the first year the NCAA\u00ae had a 64-team men's tournament) and the 1998 season for women. For each season, the file includes all games played from DayNum 0 through 132. It is important to realize that the \"Regular Season\" games are simply defined to be all games played on DayNum=132 or earlier (DayNum=132 is Selection Sunday, and there are always a few conference tournament finals actually played early in the day on Selection Sunday itself). Thus a game played on or before Selection Sunday will show up here whether it was a pre-season tournament, a non-conference game, a regular conference game, a conference tournament game, or whatever. Data Section 1 file: MNCAATourneyCompactResults.csv and WNCAATourneyCompactResults.csv These files identify the game-by-game NCAA\u00ae tournament results for all seasons of historical data. The data is formatted exactly like the corresponding RegularSeasonCompactResults data. All men's games will show up as neutral site (so WLoc is always N) and some women's games will show up as neutral site, depending on the specifics. Note that this tournament game data also includes the play-in games for those years that had play-in games. Thus each season you will see between 63 and 67 games listed, depending on how many play-in games there were. Because of the consistent structure of the NCAA\u00ae tournament schedule, you can generally tell what round a game was, depending on the exact DayNum. However, the men's 2021 tournament scheduling was slightly different, and the women's scheduling has varied a lot. Nevertheless, in general the men's schedule will be: Special note: Each year, there are also going to be other games that happened after Selection Sunday, which are not part of the NCAA\u00ae Tournament. This includes tournaments like the postseason NIT, the CBI, the CIT, and the Vegas 16. Such games are not listed in the Regular Season or the NCAA\u00ae Tourney files; they can be found in the \"Secondary Tourney\" data files (only for men's data) within Data Section 6. Although they would not be games you would ever be predicting directly for the NCAA\u00ae tournament, and they would not be games you would have data from at the time of predicting NCAA\u00ae tournament outcomes, you may nevertheless wish to make use of these games for model optimization, depending on your methodology. The more games that you can test your predictions against, the better your optimized model might eventually become, depending on how applicable all those games are. A similar argument might be advanced in favor of optimizing your predictions against conference tournament games, which might be viewed as reasonable proxies for NCAA\u00ae tournament games.  This section provides game-by-game stats at a team level (free throws attempted, defensive rebounds, turnovers, etc.) for all regular season, conference tournament, and NCAA\u00ae tournament games since the 2002-03 season (men) or since the 2009-10 season (women).  Team Box Scores are provided in \"Detailed Results\" files rather than \"Compact Results\" files. However, the two files are strongly related. In a Detailed Results file, the first eight columns (Season, DayNum, WTeamID, WScore, LTeamID, LScore, WLoc, and NumOT) are exactly the same as a Compact Results file. However, in a Detailed Results file, there are many additional columns. The column names should be self-explanatory to basketball fans (as above, \"W\" or \"L\" refers to the winning or losing team): (and then the same set of stats from the perspective of the losing team: LFGM is the number of field goals made by the losing team, and so on up to LPF). Note: by convention, \"field goals made\" (either WFGM or LFGM) refers to the total number of fields goals made by a team, a combination of both two-point field goals and three-point field goals. And \"three point field goals made\" (either WFGM3 or LFGM3) is just the three-point fields goals made, of course. So if you want to know specifically about two-point field goals, you have to subtract one from the other (e.g., WFGM - WFGM3). And the total number of points scored is most simply expressed as 2*FGM + FGM3 + FTM. Data Section 2 file: MRegularSeasonDetailedResults.csv and WRegularSeasonDetailedResults.csv These files provide team-level box scores for many regular seasons of historical data, starting with the 2003 season (men) or starting with the 2010 season (women). All games listed in the MRegularSeasonCompactResults file since the 2003 season should exactly be present in the MRegularSeasonDetailedResults file, and similarly, all games listed in the WRegularSeasonCompactResults file since the 2010 season should exactly be present in the WRegularSeasonDetailedResults file. Data Section 2 file: MNCAATourneyDetailedResults.csv and WNCAATourneyDetailedResults.csv These files provide team-level box scores for many NCAA\u00ae tournaments, starting with the 2003 season (men) or starting with the 2010 season (women). Similarly, all games listed in the MNCAATourneyCompactResults or MNCAATourneyCompactResults file for those seasons should exactly be present in the corresponding MNCAATourneyDetailedResults or WNCAATourneyDetailedResults file. This section provides city locations of all regular season, conference tournament, and NCAA\u00ae tournament games since the 2009-10 season Data Section 3 file: Cities.csv This file provides a master list of cities that have been locations for games played. Please notice that the Cities and Conferences files are the only two that don't start with an M; this is because the data files are identical between men's and women's data, so you don't need to maintain separate listings of cities or conferences across the two datasets. Also note that if you created any supplemental data in previous years on cities (latitude/longitude, altitude, city-to-city distances, etc.), the CityID's match between previous years and this year, so you should be able to re-use that information. Data Section 3 file: MGameCities.csv and WGameCities.csv This file identifies all games, starting with the 2010 season, along with the city that the game was played in. Games from the regular season, the NCAA\u00ae tourney, and other post-season tournaments (men's data only), are all listed together. There should be no games since the 2010 season where the CityID is not known. Games from the 2009 season and before are not listed in this file. This section provides weekly team rankings (men's teams only) for dozens of top rating systems - Pomeroy, Sagarin, RPI, ESPN, etc., since the 2002-2003 season Data Section 4 file: MMasseyOrdinals.csv This file lists out rankings (e.g. #1, #2, #3, ..., #N) of men's teams going back to the 2002-2003 season, under a large number of different ranking system methodologies. The information was gathered by Kenneth Massey and provided on his College Basketball Ranking Composite page. The format has been changed this year on this website. Note that a rating system is more precise than a ranking system, because a rating system can provide insight about the strength gap between two adjacently-ranked teams. A ranking system will just tell you who is #1 or who is #2, but a rating system might tell you whether the gap between #1 and #2 is large or small. Nevertheless, it can be hard to compare two different rating systems that are expressed in different scales, so it can be very useful to express all the systems in terms of their ordinal ranking (1, 2, 3, ..., N) of teams. Disclaimer: you ought to be careful about your methodology when using or evaluating these ranking systems. They are presented on a weekly basis, and given a consistent date on the Massey Composite page that typically is a Sunday; that is how the ranking systems can be compared against each other on this page. However, these systems each follow their own timeline and some systems may be released on a Sunday and others on a Saturday or Monday or even Tuesday. You should remember that if a ranking is released on a Tuesday, and was calculated based on games played through Monday, it will make the system look unusually good at predicting if you use that system to forecast the very games played on Monday that already inform the rankings. To avoid this methodological trap, we have typically used a conservative RankingDayNum of Wednesday to represent the rankings that were released at approximately the end of the weekend, a few days before, even though those rankings are represented on the composite page as being on a Sunday. For some of the older years, a more precise timestamp was known for each ranking system that allowed a more precise assignment of a RankingDayNum. By convention, the final pre-tournament rankings are always expressed as RankingDayNum=133, even though sometimes the rankings for individual systems are not released until Tuesday (DayNum=134) or even Wednesday or Thursday. If you decide to use some rankings from these Massey Ordinals to inform your predictions, be forewarned that we have no control over when they are released, and not all systems may turn out to be available in time to make pre-tournament predictions by our submission deadline. In such a situation, you may wish to use the rankings from DayNum=128 or you may need to dig into the details of the actual source of the rankings, by following the respective links on the Massey Composite Page. We may also be able to provide partial releases of the final pre-tournament Massey Ordinals on the forums, so that as systems come in on Monday or Tuesday you can use them right away. This section contains additional supporting information, including coaches, conference affiliations, alternative team name spellings, bracket structure, and game results for NIT and other postseason tournaments. Data Section 5 file: MTeamCoaches.csv This file indicates the head coach for each team in each season, including a start/finish range of DayNum's to indicate a mid-season coaching change. For scenarios where a team had the same head coach the entire season, they will be listed with a DayNum range of 0 to 154 for that season. For head coaches whose term lasted many seasons, there will be many rows listed, most of which have a DayNum range of 0 to 154 for the corresponding season. Data Section 5 file: Conferences.csv This file indicates the Division I conferences that have existed over the years since 1985. Each conference is listed with an abbreviation and a longer name. There has been no attempt to link up conferences who merged with other conferences, or whose names changed over time. Thus you will see, for instance, a \"Pacific-10\" conference up through the 2011 season, and then a \"Pacific-12\" conference starting in the 2012 season, and these look like different conferences in the data, even though it was still mostly the same teams. Please notice that the Cities and Conferences files are the only two that don't start with an M; this is because the data files are identical between men's and women's data, so you don't need to maintain separate listings of cities or conferences across the two datasets. However, the Team Conferences data differs slightly between men's and women's, so those files do have the prefixes. That's because there's two programs (VMI and The Citadel) that only have men's teams and not women's teams. Data Section 5 files: MTeamConferences.csv and WTeamConferences.csv These files indicate the conference affiliations for each team during each season. Some conferences have changed their names from year to year, and/or changed which teams are part of the conference. These files tracks this information historically, for men's and women's teams separately. Data Section 5 file: MConferenceTourneyGames.csv This file indicates which games were part of each year's post-season men's conference tournaments (all of which finished on Selection Sunday or earlier), starting from the 2001 season. Many of these conference tournament games are held on neutral sites, and many of the games are played by tournament-caliber teams just a few days before the NCAA\u00ae tournament. Thus these games could be considered as very similar to NCAA\u00ae tournament games, and (depending on your methodology) may be of use in optimizing your predictions. However, this is NOT a new listing of games; these games are already present within the MRegularSeasonCompactResults and MRegularSeasonDetailedResults files. So this file simply helps you to identify which of the \"regular season\" games since the 2001 season were actually conference tournament games, in case that is useful information. Data Section 5 file: MSecondaryTourneyTeams.csv This file identifies the teams that participated in post-season men's tournaments other than the NCAA\u00ae Tournament (such events would run in parallel with the NCAA\u00ae Tournament). These are teams that were not invited to the NCAA\u00ae Tournament and instead were invited to some other tournament, of which the NIT is the most prominent tournament, but there have also been the CBI, CIT, Vegas 16 (V16), and The Basketball Classic (TBC) at various points in recent years. Depending on your methodology, you might find it useful to have these additional game results, above and beyond what is available from the NCAA\u00ae Tournament results. Many of these teams, especially in the NIT, were \"bubble\" teams of comparable strength to several NCAA\u00ae Tournament invitees, and so these games may be of use in model optimization for predicting NCAA\u00ae Tournament results. Also note that this information could be determined just from inspecting the MSecondaryTourneyCompactResults file, but is presented in this file as well, for your convenience. Data Section 5 file: MSecondaryTourneyCompactResults.csv This file indicates the final scores for the tournament games of \"secondary\" post-season tournaments: the NIT, CBI, CIT, and Vegas 16. The detailed results (team box scores) have not been assembled for these games. For the most part, this file is exactly like other Compact Results listings, although it also has a column for Secondary Tourney. Also note that because these games are played after DayNum=132, they are NOT listed in the MRegularSeasonCompactResults file. Data Section 5 files: MTeamSpellings.csv and WTeamSpellings.csv These files indicate alternative spellings of many team names. They are intended for use in associating external spellings against our own TeamID numbers, thereby helping to relate the external data properly with our datasets. Over the years we have identified various external spellings of different team names (as an example, for Ball State we have seen \"ball st\", and \"ball st.\", and \"ball state\", and \"ball-st\", and \"ball-state\"). Other teams have had more significant changes to their names over the years; for example, \"Texas Pan-American\" and \"Texas-Rio Grande Valley\" are actually the same school. The current list is obviously not exhaustive, and we encourage participants to identify additional mappings and upload extended versions of this file to the forums. Data Section 5 files: MNCAATourneySlots and WNCAATourneySlots These files identify the mechanism by which teams are paired against each other, depending upon their seeds, as the tournament proceeds through its rounds. They can be of use in identifying, for a given historical game, what round it occurred in, and what the seeds/slots were for the two teams (the meaning of \"slots\" is described below). Because of the existence of play-in games for particular seed numbers, the pairings have small differences from year to year. You may need to know these specifics if you are trying to represent/simulate the exact workings of the tournament bracket. Data Section 5 file: MNCAATourneySeedRoundSlots.csv This file helps to represent the men's bracket structure in any given year. No matter where the play-in seeds are located, we can always know, for a given tournament seed, exactly what bracket slot they would be playing in, on each possible game round, and what the possible DayNum values would be for that round. Thus, if we know when a historical game was played, and what the team's seed was, we can identify the slot for that game. This can be useful in representing or simulating the tournament bracket structure. The women's scheduling has varied a lot more and does not lend itself to this common structure and so there is not a corresponding file for the women's data. Also note that the 2021 men's tournament had unusual scheduling and did not follow the traditional assignment of DayNums for each round."
    },
    {
        "name": "UM - Game-Playing Strength of MCTS Variants",
        "url": "https://www.kaggle.com/competitions/um-game-playing-strength-of-mcts-variants",
        "overview_text": "In this competition, you\u2019ll create a model to predict how well one Monte-Carlo tree search (MCTS) variant will do against another in a given game, based on a list of features describing the game. This challenge aims to help us figure out which MCTS variants work best in different types of games, so we can make more informed choices when applying these algorithms to new problems.",
        "description_text": "MCTS is a widely used search algorithm for developing agents that can play board games intelligently. Over the past two decades, researchers have proposed dozens, if not hundreds, of MCTS variants. Despite this, it's been challenging to determine which variants are best suited for specific types of games. In most studies, researchers demonstrate that a new MCTS variant outperforms one or a few other variants in a limited set of games. However, it\u2019s uncommon for a new variant to consistently outperform others across a broad range of games, making it unclear which types of games certain MCTS variants are best at. Answering this question would greatly improve our understanding of MCTS algorithms, and help us make better decisions about which variants to apply to new games or other decision-making problems. This competition challenges you to develop a model that can predict the performance of one MCTS variant against another in a given game, based on the features of the game. Your work could help pave the way for identifying the strengths and weaknesses of different MCTS variants, advancing our understanding of where they work best in various scenarios.",
        "dataset_text": "For this competition we have generated a dataset of outcomes from different variants of Monte-Carlo tree search (MCTS) agents playing over a thousand distinct board games. All of the games are two-player, sequential, zero-sum board games with perfect information. Your task is to predict the degree of advantage the first agent has over the other. This competition uses a hidden test. When your submitted notebook is scored, the actual test data and sample submission will be provided to your notebook in batches of 100 rows via an evaluation API. train.csv - Every row of data represents a set of plays between an ordered pair of two specific agents, in a single game, with a single outcome per play. test.csv - The same as train.csv minus the following columns: num_wins_agent1, num_draws_agent1, num_losses_agent1, and utility_agent1 columns. Expect approximately 60,000 rows in the hidden test set. sample_submission.csv - An example valid submission file. Note that the evaluation API will generate the final submission. concepts.csv - A file, exported from the publicly available Ludii database, containing information on the Concept-based features of games. This may be useful for filtering out certain categories of features. For example, as concepts of the \"Visual\" category should in principle not have any effect on AI playing strength you may wish to drop concepts with a TypeId of 9 (although there may be correlations between types of games and how humans choose to visualise them, so you may also choose not to do this). kaggle_evaluation/ Files that implement the evaluation API. Some of the implementation details may be of interest for offline testing, but we recommend beginning with this starter notebook. All agent string descriptions in training and test data are in the following format: MCTS-<SELECTION>-<EXPLORATION_CONST>-<PLAYOUT>-<SCORE_BOUNDS>, where: For example, an MCTS agent that uses the UCB1GRAVE selection strategy, an exploration constant of 0.1, the NST play-out strategy, and Score Bounds, will be described as MCTS-UCB1GRAVE-0.1-NST-true. You may treat every distinct agent string as a completely separate agent, but you may also try to leverage the compositional nature of the MCTS agents and split it up into its components."
    },
    {
        "name": "BirdCLEF 2024",
        "url": "https://www.kaggle.com/competitions/birdclef-2024",
        "overview_text": "Birds are excellent indicators of biodiversity change since they are highly mobile and have diverse habitat requirements. Changes in species assemblage and the number of birds can thus indicate the success or failure of a restoration project. However, frequently conducting traditional observer-based bird biodiversity surveys over large areas is expensive and logistically challenging. In comparison, passive acoustic monitoring (PAM) combined with new analytical tools based on machine learning allows conservationists to sample much greater spatial scales with higher temporal resolution and explore the relationship between restoration interventions and biodiversity in depth.",
        "description_text": "Description text not found",
        "dataset_text": "Your challenge in this competition is to identify which birds are calling in recordings made in a Global Biodiversity Hotspot in the Western Ghats. This is an important task for scientists who monitor bird populations for conservation purposes. More accurate solutions could enable more comprehensive monitoring. This competition uses a hidden test set. When your submitted notebook is scored, the actual test data will be made available to your notebook. train_audio/ The training data consists of short recordings of individual bird calls generously uploaded by users of xenocanto.org. These files have been downsampled to 32 kHz where applicable to match the test set audio and converted to the ogg format. The training data should have nearly all relevant files; we expect there is no benefit to looking for more on xenocanto.org and appreciate your cooperation in limiting the burden on their servers. test_soundscapes/ When you submit a notebook, the test_soundscapes directory will be populated with approximately 1,100 recordings to be used for scoring. They are 4 minutes long and in ogg audio format. The file names are randomized but have the general form of soundscape_xxxxxx.ogg. It should take your submission notebook approximately five minutes to load all of the test soundscapes. unlabeled_soundscapes/ Unlabeled audio data from the same recording locations as the test soundscapes. train_metadata.csv A wide range of metadata is provided for the training data. The most directly relevant fields are: sample_submission.csv A valid sample submission. eBird_Taxonomy_v2021.csv - Data on the relationships between different species."
    },
    {
        "name": "NeurIPS - Ariel Data Challenge 2024",
        "url": "https://www.kaggle.com/competitions/ariel-data-challenge-2024",
        "overview_text": "Are you ready to embark on a journey that pushes the boundaries of astronomical data analysis?",
        "description_text": " The discovery of exoplanets\u2014planets orbiting stars other than our Sun\u2014has transformed our cosmic perspective, challenging conventional notions about Earth's uniqueness and the potential for life elsewhere. As of today, we are aware of over 5,600 exoplanets. Detecting these worlds is the initial step; we must also comprehend and characterise their nature by studying their atmospheres. In 2029, ESA Ariel Mission will conduct the first comprehensive study of 1,000 extrasolar planets in our galactic neighbourhood. Observing these atmospheres is one of the hardest data-analysis problems in contemporary astronomy. When an exoplanet transits its host star in our line of sight, a tiny fraction of starlight (50\u2013200 photons per million) passes through the planet's atmospheric annulus and interacts with its chemistry, clouds, and winds. These faint signals typically range from 50ppm (for Super-Earth like planets) to 200ppm (for Jupiter like planets) in magnitude and are regularly corrupted by the noise of the instrument. A major component of this noise is due to the inevitable vibration of the spacecraft in space, known as \u2018jitter noise\u2019. This noise arises from the difficulties of maintaining precise pointing in low-gravity environments, as the spacecraft relies on spinning momentum wheels for stability. Akin to taking long-exposure images with a shaky hand, this noise poses a far greater challenge than the motion blur encountered in commercial photography applications. The photometric variation (\u223c200 ppm) caused by jitter noise alone is comparable to the variation exhibited by the planetary signal we aim to detect, undermining signals from small planets like Earths and super-Earths. Coupled with other sources of correlated and uncorrelated noises, it is proving difficult for us to achieve the strict technical requirement of the Ariel Payload design. The task of this competition is to extract the atmospheric spectra from each observation, with an estimate of its level of uncertainty. In order to obtain such a spectrum, we require the participant to detrend a large number of sequential 2D images of the spectral focal plane taken over several hours of observing the exoplanet as it eclipses its host star. Performing this detrending process to extract atmospheric spectra and their associated errorbars from raw observational data is a crucial and common prerequisite step for any modern astronomical instrument before the data can undergo scientific analysis.",
        "dataset_text": "Characterizing the chemistry of exoplanets is of the great active projects in astronomy. The European Space Agency's Atmospheric Remote-sensing Infrared Exoplanet Large-survey (ARIEL) mission will gather data on roughly 1,000 exoplanets by observing them while they transit in front of their host stars. Even with the powerful instruments on board ARIEL, the resulting data will be based on a limited number of photons and include a fair amount of noise. Your challenge in this competition is to extract the chemical spectrum of the atmospheres of exoplanets using simulated ARIEL data. This competition uses a hidden test set. When your submitted notebook is scored, the actual test data (including a full length sample submission) will be made available to your notebook. Expect to see roughly 800 exoplanets in the hidden test set. A dozen of the exoplanet simulations in the test set were based directly real exoplanets. All of those cases are ignored for scoring purposes. The dataset comprises time series imagery from two separate instruments plus calibration data. Ariel contains multiple optical instruments, each specialising in different spectral bands and observing modes. FGS1 is the first channel of Ariel's Fine Guidance System (FGS). The main task of the Fine Guidance System is to enable centering, focusing, and guiding of the satellite but it will also provide high-precision photometry of the target star in the visible spectrum. It has a sensitivity between 0.60 and 0.80 \u00b5m. AIRS-CH0 is the first channel (CH0) of the Ariel InfraRed Spectrometer (AIRS). It is an infrared spectrometer with a sensitivity between 1.95 and 3.90 \u00b5m, and has a resolving power of approximately R=100. For more information about Ariel please visit the Ariel red book. When examining frames from either instrument, you'll notice the flux levels oscillate between low and high counts. This is intentional and due to the design of the observing mode. Each pixel in the telescope's detector acts like a charge accumulator. The process begins with a sub-exposure recording the initial charge level before accumulation starts. After a set exposure time, another sub-exposure captures the new charge level. The pixels are then reset, and this cycle repeats for the duration of the observation. This allows for detailed temporal information to be captured. It's particularly useful for observing rapid changes, such as those occurring during a planetary transit. In this observation, the process generates 135,000 frames for FGS1 and 11,250 frames for AIRS-CH0. Calibration files record the electronic characteristics of the sensor and serve as \"supporting frames\" used in image post-processing to image signal to noise ratio. This website contains a brief introduction about the concept of calibration frames. This notebook may also be useful. The calibration files are duplicated for the train set but vary in the test set."
    },
    {
        "name": "CIBMTR - Equity in post-HCT Survival Predictions",
        "url": "https://www.kaggle.com/competitions/equity-post-HCT-survival-predictions",
        "overview_text": "In this competition, you\u2019ll develop models to improve the prediction of transplant survival rates for patients undergoing allogeneic Hematopoietic Cell Transplantation (HCT) \u2014 an important step in ensuring that every patient has a fair chance at a successful outcome, regardless of their background.",
        "description_text": "Improving survival predictions for allogeneic HCT patients is a vital healthcare challenge. Current predictive models often fall short in addressing disparities related to socioeconomic status, race, and geography. Addressing these gaps is crucial for enhancing patient care, optimizing resource utilization, and rebuilding trust in the healthcare system. This competition aims to encourage participants to advance predictive modeling by ensuring that survival predictions are both precise and fair for patients across diverse groups. By using synthetic data\u2014which mirrors real-world situations while protecting patient privacy\u2014participants can build and improve models that more effectively consider diverse backgrounds and conditions. You\u2019re challenged to develop advanced predictive models for allogeneic HCT that enhance both accuracy and fairness in survival predictions. The goal is to address disparities by bridging diverse data sources, refining algorithms, and reducing biases to ensure equitable outcomes for patients across diverse race groups. Your work will help create a more just and effective healthcare environment, ensuring every patient receives the care they deserve.",
        "dataset_text": "The dataset consists of 59 variables related to hematopoietic stem cell transplantation (HSCT), encompassing a range of demographic and medical characteristics of both recipients and donors, such as age, sex, ethnicity, disease status, and treatment details. The primary outcome of interest is event-free survival, represented by the variable efs, while the time to event-free survival is captured by the variable efs_time. These two variables together encode the target for a censored time-to-event analysis. The data, which features equal representation across recipient racial categories including White, Asian, African-American, Native American, Pacific Islander, and More than One Race, was synthetically generated using the data generator from synthcity, trained on a large cohort of real CIBMTR data. We have used the SurvivalGAN method, introduced in the paper \"SurvivalGAN: Generating Time-to-Event Data for Survival Analysis\" which addresses the generation of synthetic survival data with special considerations for censoring. SurvivalGAN is adept at capturing the intricate relationships and interactions among variables within survival data and their influence on time-to-event outcomes. This generative model utilizes a conditional Generative Adversarial Network (GAN) framework, which is specifically tailored to address the complexities of survival analysis, including the critical task of managing censored data. By conditioning on additional information such as censoring status and actual survival times, SurvivalGAN effectively learns the underlying distribution of the data, ensuring that the generated synthetic dataset retains the essential interactions among variables that are predictive of survival outcomes. Note: The rerun test data contains approximately the same number of observations as the training data."
    },
    {
        "name": "Image Matching Challenge 2024 - Hexathlon",
        "url": "https://www.kaggle.com/competitions/image-matching-challenge-2024",
        "overview_text": "The goal of this competition is to construct precise 3D maps using sets of images in diverse scenarios and environments. You\u2019ll develop a model to generate accurate spatial representations, regardless of the source domain\u2014images taken from drones, amidst dense forests, during nighttime, or any of the six problem categories.",
        "description_text": "Landmarks and historical sites are some of the most frequently photographed places on Earth. Yet, each shot has a slightly different angle and shadows vary on times of day or year. One photo could be taken from the ground, another up some steps, and a third from a drone. Matching many images across different viewpoints is a fundamental Computer Vision problem that is far from solved. Factors like surface texture or surroundings can cause an otherwise well-performing algorithm to degrade. The process of reconstructing a 3D model of an environment from a collection of images is called Structure from Motion (SfM). These images are often captured by trained operators or with additional sensor data. This ensures homogeneous, high-quality data. It\u2019s much more difficult to build 3D models from assorted images, the real-world examples we\u2019ve put together for this competition. In fact, we\u2019ve designated 6 categories, each with its distinct challenge: Your efforts could contribute to a better understanding of this fundamental problem in Computer Vision. We hope to encourage knowledge sharing between traditional image-matching techniques and cutting-edge machine learning by including many categories. Header photos by Sandro Gonzalez and Ljubomir \u017darkovi\u0107 on Unsplash. Dmytro Mishkin (Czech Technical University in Prague/HOVER Inc), Eduard Trulls (Google), Fabio Bellavia (University of Palermo), Luca Morelli (University of Trento/Bruno Kessler Foundation), Fabio Remondino (Bruno Kessler Foundation), Weiwei Sun (University of British Columbia), Kwang Moo Yi (University of British Columbia/Haiper), Amy Tabb (USDA-ARS-AFRS), Jiri Matas (Czech Technical University in Prague)",
        "dataset_text": "Building a 3-D model of a scene given an unstructured collection of images taken around it is a longstanding problem in computer vision research. Your challenge in this competition is to generate 3-D reconstructions from image sets showing different types of scenes and accurately pose those images. This competition uses a hidden test. When your submitted notebook is scored, the actual test data (including a sample submission) will be made available to your notebook. Expect to find roughly 1,000 images in the hidden test set. sample_submission.csv A valid, randomly-generated sample submission with the following fields: [train/test]/*/images A batch of images all taken near the same location. Some of training datasets may also contain a folder named images_full with additional images. The published test folder comprises a subset of the church scene from train and is provided solely for example purposes. The training data usually has a sequential capture ordering and significant image-to-image content overlap while the test set has limited image-to-image overlap and the image ordering is randomized. train/*/sfm A 3-D reconstruction for this batch of images, which can be opened with colmap, the 3-D structure-from-motion library bundled with this competition. train/*/LICENSE.txt The license for this dataset. train/train_labels.csv A list of images in these datasets, with ground truth."
    },
    {
        "name": "APTOS 2019 Blindness Detection",
        "url": "https://www.kaggle.com/competitions/aptos2019-blindness-detection",
        "overview_text": "Overview text not found",
        "description_text": "Imagine being able to detect blindness before it happened. Millions of people suffer from diabetic retinopathy, the leading cause of blindness among working aged adults. Aravind Eye Hospital in India hopes to detect and prevent this disease among people living in rural areas where medical screening is difficult to conduct. Successful entries in this competition will improve the hospital\u2019s ability to identify potential patients. Further, the solutions will be spread to other Ophthalmologists through the 4th Asia Pacific Tele-Ophthalmology Society (APTOS) Symposium Currently, Aravind technicians travel to these rural areas to capture images and then rely on highly trained doctors to review the images and provide diagnosis. Their goal is to scale their efforts through technology; to gain the ability to automatically screen images for disease and provide information on how severe the condition may be. In this synchronous Kernels-only competition, you'll build a machine learning model to speed up disease detection. You\u2019ll work with thousands of images collected in rural areas to help identify diabetic retinopathy automatically. If successful, you will not only help to prevent lifelong blindness, but these models may be used to detect other sorts of diseases in the future, like glaucoma and macular degeneration.  Get started today! ",
        "dataset_text": "You are provided with a large set of retina images taken using fundus photography under a variety of imaging conditions. A clinician has rated each image for the severity of diabetic retinopathy on a scale of 0 to 4: Like any real-world data set, you will encounter noise in both the images and labels. Images may contain artifacts, be out of focus, underexposed, or overexposed. The images were gathered from multiple clinics using a variety of cameras over an extended period of time, which will introduce further variation. In a synchronous Kernels-only competition, the files you can observe and download will be different than the private test set and sample submission. The files may have different ids, may be a different size, and may vary in other ways, depending on the problem. You should structure your code so that it returns predictions for the public test set images in the format specified by the public sample_submission.csv, but does not hard code aspects like the id or number of rows. When Kaggle runs your Kernel privately, it substitutes the private test set and sample submission in place of the public ones.\nYou can plan on the private test set consisting of 20GB of data across 13,000 images (approximately). "
    },
    {
        "name": "M5 Forecasting - Accuracy",
        "url": "https://www.kaggle.com/competitions/m5-forecasting-accuracy",
        "overview_text": "Overview text not found",
        "description_text": "Note: This is one of the two complementary competitions that together comprise the M5 forecasting challenge. Can you estimate, as precisely as possible, the point forecasts of the unit sales of various products sold in the USA by Walmart? If you are interested in estimating the uncertainty distribution of the realized values of the same series, be sure to check out its companion competition How much camping gear will one store sell each month in a year? To the uninitiated, calculating sales at this level may seem as difficult as predicting the weather. Both types of forecasting rely on science and historical data. While a wrong weather forecast may result in you carrying around an umbrella on a sunny day, inaccurate business forecasts could result in actual or opportunity losses. In this competition, in addition to traditional forecasting methods you\u2019re also challenged to use machine learning to improve forecast accuracy. The Makridakis Open Forecasting Center (MOFC) at the University of Nicosia conducts cutting-edge forecasting research and provides business forecast training. It helps companies achieve accurate predictions, estimate the levels of uncertainty, avoiding costly mistakes, and apply best forecasting practices. The MOFC is well known for its Makridakis Competitions, the first of which ran in the 1980s. In this competition, the fifth iteration, you will use hierarchical sales data from Walmart, the world\u2019s largest company by revenue, to forecast daily sales for the next 28 days. The data, covers stores in three US States (California, Texas, and Wisconsin) and includes item level, department, product categories, and store details. In addition, it has explanatory variables such as price, promotions, day of the week, and special events. Together, this robust dataset can be used to improve forecasting accuracy. If successful, your work will continue to advance the theory and practice of forecasting. The methods used can be applied in various business areas, such as setting up appropriate inventory or service levels. Through its business support and training, the MOFC will help distribute the tools and knowledge so others can achieve more accurate and better calibrated forecasts, reduce waste and be able to appreciate uncertainty and its risk implications. Acknowledgements\nAdditional thanks go to other partner organizations and prize sponsors, National Technical University of Athens (NTUA), INSEAD, Google, Uber and IIF.",
        "dataset_text": "In the challenge, you are predicting item sales at stores in various locations for two 28-day time periods. Information about the data is found in the M5 Participants Guide."
    },
    {
        "name": "M5 Forecasting - Uncertainty",
        "url": "https://www.kaggle.com/competitions/m5-forecasting-uncertainty",
        "overview_text": "Overview text not found",
        "description_text": "Note: This is one of the two complementary competitions that together comprise the M5 forecasting challenge. Can you estimate, as precisely as possible, the uncertainty distribution of the unit sales of various products sold in the USA by Walmart? This specific competition is the first of its kind, opening up new directions for both academic research and how uncertainty could be assessed and used in organizations. If you are interested in providing point (accuracy) forecasts for the same series, be sure to check out its companion competition. How much camping gear will one store sell each month in a year? To the uninitiated, calculating sales at this level may seem as difficult as predicting the weather. Both types of forecasting rely on science and historical data. While a wrong weather forecast may result in you carrying around an umbrella on a sunny day, inaccurate business forecasts could result in actual or opportunity losses. In this competition, in addition to traditional forecasting methods you\u2019re also challenged to use machine learning to improve forecast accuracy. The Makridakis Open Forecasting Center (MOFC) at the University of Nicosia conducts cutting-edge forecasting research and provides business forecast training. It helps companies achieve accurate predictions, estimate the levels of uncertainty, avoiding costly mistakes, and apply best forecasting practices. The MOFC is well known for its Makridakis Competitions, the first of which ran in the 1980s. In this competition, the fifth iteration, you will use hierarchical sales data from Walmart, the world\u2019s largest company by revenue, to forecast daily sales for the next 28 days and to make uncertainty estimates for these forecasts. The data, covers stores in three US States (California, Texas, and Wisconsin) and includes item level, department, product categories, and store details. In addition, it has explanatory variables such as price, promotions, day of the week, and special events. Together, this robust dataset can be used to improve forecasting accuracy. If successful, your work will continue to advance the theory and practice of forecasting. The methods used can be applied in various business areas, such as setting up appropriate inventory or service levels. Through its business support and training, the MOFC will help distribute the tools and knowledge so others can achieve more accurate and better calibrated forecasts, reduce waste and be able to appreciate uncertainty and its risk implications. Acknowledgements\nAdditional thanks go to other partner organizations and prize sponsors, National Technical University of Athens (NTUA), INSEAD, Google, Uber and IIF.",
        "dataset_text": "In the challenge, you are predicting 9 quartiles of item sales at stores in various locations for two 28-day time periods. Information about the data is found in the M5 Participants Guide."
    },
    {
        "name": "Jigsaw Multilingual Toxic Comment Classification",
        "url": "https://www.kaggle.com/competitions/jigsaw-multilingual-toxic-comment-classification",
        "overview_text": "Overview text not found",
        "description_text": "It only takes one toxic comment to sour an online discussion. The Conversation AI team, a research initiative founded by Jigsaw and Google, builds technology to protect voices in conversation. A main area of focus is machine learning models that can identify toxicity in online conversations, where toxicity is defined as anything rude, disrespectful or otherwise likely to make someone leave a discussion. If these toxic contributions can be identified, we could have a safer, more collaborative internet. In the previous 2018 Toxic Comment Classification Challenge, Kagglers built multi-headed models to recognize toxicity and several subtypes of toxicity. In 2019, in the Unintended Bias in Toxicity Classification Challenge, you worked to build toxicity models that operate fairly across a diverse range of conversations. This year, we're taking advantage of Kaggle's new TPU support and challenging you to build multilingual models with English-only training data. Jigsaw's API, Perspective, serves toxicity models and others in a growing set of languages (see our documentation for the full list). Over the past year, the field has seen impressive multilingual capabilities from the latest model innovations, including few- and zero-shot learning. We're excited to learn whether these results \"translate\" (pun intended!) to toxicity classification. Your training data will be the English data provided for our previous two competitions and your test data will be Wikipedia talk page comments in several different languages. As our computing resources and modeling capabilities grow, so does our potential to support healthy conversations across the globe. Develop strategies to build effective multilingual models and you'll help Conversation AI and the entire industry realize that potential. Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.",
        "dataset_text": "The primary data for the competition is, in each provided file, the comment_text column. This contains the text of a comment which has been classified as toxic or non-toxic (0\u20261 in the toxic column). The train set\u2019s comments are entirely in english and come either from Civil Comments or Wikipedia talk page edits. The test data's comment_text columns are composed of multiple non-English languages. The *-train.csv files and validation.csv file also contain a toxic column that is the target to be trained on. The jigsaw-toxic-comment-train.csv and jigsaw-unintended-bias-train.csv contain training data (comment_text and toxic) from the two previous Jigsaw competitions, as well as additional columns that you may find useful. *-seqlen128.csv files contain training, validation, and test data that has been processed for input into BERT. You are predicting the probability that a comment is toxic. A toxic comment would receive a 1.0. A benign, non-toxic comment would receive a 0.0. In the test set, all comments are classified as either a 1.0 or a 0.0. The \"Toxic Comment Classification\" dataset is released under CC0, with the underlying comment text being governed by Wikipedia's CC-SA-3.0. The \"Unintended Bias in Toxicity\" dataset is released under CC0, as is the underlying comment text."
    },
    {
        "name": "Bristol-Myers Squibb \u2013 Molecular Translation",
        "url": "https://www.kaggle.com/competitions/bms-molecular-translation",
        "overview_text": "Overview text not found",
        "description_text": "In a technology-forward world, sometimes the best and easiest tools are still pen and paper. Organic chemists frequently draw out molecular work with the Skeletal formula, a structural notation used for centuries. Recent publications are also annotated with machine-readable chemical descriptions (InChI), but there are decades of scanned documents that can't be automatically searched for specific chemical depictions. Automated recognition of optical chemical structures, with the help of machine learning, could speed up research and development efforts. Unfortunately, most public data sets are too small to support modern machine learning models. Existing tools produce 90% accuracy but only under optimal conditions. Historical sources often have some level of image corruption, which reduces performance to near zero. In these cases, time-consuming, manual work is required to reliably convert scanned chemical structure images into a machine-readable format. Bristol-Myers Squibb is a global biopharmaceutical company working to transform patients' lives through science. Their mission is to discover, develop, and deliver innovative medicines that help patients prevail over serious diseases. In this competition, you\u2019ll interpret old chemical images. With access to a large set of synthetic image data generated by Bristol-Myers Squibb, you'll convert images back to the underlying chemical structure annotated as InChI text. Tools to curate chemistry literature would be a significant benefit to researchers. If successful, you'll help chemists expand access to collective chemical research. In turn, this would speed up research and development efforts in many key fields by avoiding repetition of previously published chemistries and identifying novel trends via mining large data sets. Photo by Terry Vlisidis on Unsplash",
        "dataset_text": "In this competition, you are provided with images of chemicals, with the objective of predicting the corresponding International Chemical Identifier (InChI) text string of the image. The images provided (both in the training data as well as the test data) may be rotated to different angles, be at various resolutions, and have different noise levels. Note: There are about 4m total images in this dataset. Unzipping the downloaded data will take a non-trivial amount of time."
    },
    {
        "name": "RANZCR CLiP - Catheter and Line Position Challenge",
        "url": "https://www.kaggle.com/competitions/ranzcr-clip-catheter-line-classification",
        "overview_text": "Overview text not found",
        "description_text": "Serious complications can occur as a result of malpositioned lines and tubes in patients. Doctors and nurses frequently use checklists for placement of lifesaving equipment to ensure they follow protocol in managing patients. Yet, these steps can be time consuming and are still prone to human error, especially in stressful situations when hospitals are at capacity. Hospital patients can have catheters and lines inserted during the course of their admission and serious complications can arise if they are positioned incorrectly. Nasogastric tube malpositioning into the airways has been reported in up to 3% of cases, with up to 40% of these cases demonstrating complications [1-3]. Airway tube malposition in adult patients intubated outside the operating room is seen in up to 25% of cases [4,5]. The likelihood of complication is directly related to both the experience level and specialty of the proceduralist. Early recognition of malpositioned tubes is the key to preventing risky complications (even death), even more so now that millions of COVID-19 patients are in need of these tubes and lines. The gold standard for the confirmation of line and tube positions are chest radiographs. However, a physician or radiologist must manually check these chest x-rays to verify that the lines and tubes are in the optimal position. Not only does this leave room for human error, but delays are also common as radiologists can be busy reporting other scans. Deep learning algorithms may be able to automatically detect malpositioned catheters and lines. Once alerted, clinicians can reposition or remove them to avoid life-threatening complications. The Royal Australian and New Zealand College of Radiologists (RANZCR) is a not-for-profit professional organisation for clinical radiologists and radiation oncologists in Australia, New Zealand, and Singapore. The group is one of many medical organisations around the world (including the NHS) that recognizes malpositioned tubes and lines as preventable. RANZCR is helping design safety systems where such errors will be caught. In this competition, you\u2019ll detect the presence and position of catheters and lines on chest x-rays. Use machine learning to train and test your model on 40,000 images to categorize a tube that is poorly placed. The dataset has been labelled with a set of definitions to ensure consistency with labelling. The normal category includes lines that were appropriately positioned and did not require repositioning. The borderline category includes lines that would ideally require some repositioning but would in most cases still function adequately in their current position. The abnormal category included lines that required immediate repositioning. If successful, your efforts may help clinicians save lives. Earlier detection of malpositioned catheters and lines is even more important as COVID-19 cases continue to surge. Many hospitals are at capacity and more patients are in need of these tubes and lines. Quick feedback on catheter and line placement could help clinicians better treat these patients. Beyond COVID-19, detection of line and tube position will ALWAYS be a requirement in many ill hospital patients.",
        "dataset_text": "In this competition, you\u2019ll detect the presence and position of catheters and lines on chest x-rays. Use machine learning to train and test your model on 40,000 images to categorize a tube that is poorly placed. You will need the train and test images. This is a code-only competition so there is a hidden test set (approximately 4x larger, with ~14k images) as well.\ntrain.csv contains image IDs, binary labels, and patient IDs.\nTFRecords are available for both train and test. (They are also available for the hidden test set.)\nWe've also included train_annotations.csv. These are segmentation annotations for training samples that have them. They are included solely as additional information for competitors."
    },
    {
        "name": "VinBigData Chest X-ray Abnormalities Detection",
        "url": "https://www.kaggle.com/competitions/vinbigdata-chest-xray-abnormalities-detection",
        "overview_text": "Overview text not found",
        "description_text": "When you have a broken arm, radiologists help save the day\u2014and the bone. These doctors diagnose and treat medical conditions using imaging techniques like CT and PET scans, MRIs, and, of course, X-rays. Yet, as it happens when working with such a wide variety of medical tools, radiologists face many daily challenges, perhaps the most difficult being the chest radiograph. The interpretation of chest X-rays can lead to medical misdiagnosis, even for the best practicing doctor. Computer-aided detection and diagnosis systems (CADe/CADx) would help reduce the pressure on doctors at metropolitan hospitals and improve diagnostic quality in rural areas. Existing methods of interpreting chest X-ray images classify them into a list of findings. There is currently no specification of their locations on the image which sometimes leads to inexplicable results. A solution for localizing findings on chest X-ray images is needed for providing doctors with more meaningful diagnostic assistance. Established in August 2018 and funded by the Vingroup JSC, the Vingroup Big Data Institute (VinBigData) aims to promote fundamental research and investigate novel and highly-applicable technologies. The Institute focuses on key fields of data science and artificial intelligence: computational biomedicine, natural language processing, computer vision, and medical image processing. The medical imaging team at VinBigData conducts research in collecting, processing, analyzing, and understanding medical data. They're working to build large-scale and high-precision medical imaging solutions based on the latest advancements in artificial intelligence to facilitate effective clinical workflows. In this competition, you\u2019ll automatically localize and classify 14 types of thoracic abnormalities from chest radiographs. You'll work with a dataset consisting of 18,000 scans that have been annotated by experienced radiologists. You can train your model with 15,000 independently-labeled images and will be evaluated on a test set of 3,000 images. These annotations were collected via VinBigData's web-based platform, VinLab. Details on building the dataset can be found in our recent paper \u201cVinDr-CXR: An open dataset of chest X-rays with radiologist's annotations\u201d. If successful, you'll help build what could be a valuable second opinion for radiologists. An automated system that could accurately identify and localize findings on chest radiographs would relieve the stress of busy doctors while also providing patients with a more accurate diagnosis. Challenge Organizing Team Data Contributors\nThe dataset used in this competition was created by assembling de-identified Chest X-ray studies provided by two hospitals in Vietnam: the Hospital 108 and the Hanoi Medical University Hospital.",
        "dataset_text": "In this competition, we are classifying common thoracic lung diseases and localizing critical findings. This is an object detection and classification problem. For each test image, you will be predicting a bounding box and class for all findings. If you predict that there are no findings, you should create a prediction of \"14 1 0 0 1 1\" (14 is the class ID for no finding, and this provides a one-pixel bounding box with a confidence of 1.0). The images are in DICOM format, which means they contain additional data that might be useful for visualizing and classifying. The dataset comprises 18,000 postero-anterior (PA) CXR scans in DICOM format, which were de-identified to protect patient privacy.\nAll images were labeled by a panel of experienced radiologists for the presence of 14 critical radiographic findings as listed below: The \"No finding\" observation (14) was intended to capture the absence of all findings above. Note that a key part of this competition is working with ground truth from multiple radiologists."
    },
    {
        "name": "MLB Player Digital Engagement Forecasting",
        "url": "https://www.kaggle.com/competitions/mlb-player-digital-engagement-forecasting",
        "overview_text": "Overview text not found",
        "description_text": "A player hits a walk-off home run. A pitcher throws a no-hitter. A team gets red hot going into the Postseason. We know some of the catalysts that increase baseball fan interest. Now Major League Baseball (MLB) and Google Cloud want the Kaggle community\u2019s help to identify the many other factors which pique supporter engagement and create deeper relationships betweens players and fans. The sport has a long history of being numbers-driven. Nearly every day from at least April through October, baseball fans watch, read, and search for information about players. Which individuals they seek can depend on player performance, team standings, popularity, among other, currently unknown factors\u2014which could be better understood thanks to data science. Since at least the early 1990s, MLB has led the sports world in the use of data, showing fans, players, coaches, and media what\u2019s possible when you combine data with human performance. MLB continues its leadership using technology to engage fans and provide new fans innovative ways to experience America\u2019s Favorite Pastime.  MLB has teamed up with Google Cloud to transform the fan experience through data. Google Cloud proudly supports this Kaggle contest to celebrate the launch of Vertex AI: Google Cloud\u2019s new platform to unify your ML workflows. In this competition, you\u2019ll predict how fans engage with MLB players\u2019 digital content on a daily basis for a future date range. You\u2019ll have access to player performance data, social media data, and team factors like market size. Successful models will provide new insights into what signals most strongly correlate with and influence engagement. Imagine if you could predict MLB All Stars all season long or when each of a team\u2019s 25 players has his moment in the spotlight. These insights are possible when you dive deeper into the fandom of America\u2019s pastime. Be part of the first method of its kind to try to understand digital engagement at the player level in this granular, day-to-day fashion. Simultaneously help MLB build innovation more easily using Google Cloud\u2019s data analytics, Vertex AI and MLOps tools. You could play a part in shaping the future of MLB fan and player engagement. ",
        "dataset_text": "You are tasked with forecasting four different measures of engagement (target1-target4) for a subset of MLB players who are active in the 2021 season. The data contains a set of static files that do not change with time (players.csv, teams.csv, seasons.csv, awards.csv) as well as daily data (train.csv) which is grouped by day. When predicting on a given date, you are forecasting the target variables for the next day (i.e. for date d, you're predicting the engagement for day d+1). This is a code competition that relies on a time-series module to ensure models do not peek forward in time. The time series module provides you with the test data and writes your submission file automatically. The test data arrives in a data frame identical in format to train.csv, except it does not contain the target values. To submit, follow the instructions on the Evaluation page. When you submit your notebook, it will be rerun on an unseen test set: Your code will need to be robust and make predictions for any date_playerId combination requested by the module. Each team's selected notebooks (up to 2 per team, selected by the Final Submission Deadline) will be rerun during the Evaluation phase. Before diving into specifics, some high level qualifications about the data: This file has awards won by players in the training set prior to the beginning of the daily data (i.e. before 2018). ^indicates that a more complete walkthrough is below This contains data on MLB players active at some point since 2018. Predictions are only scored for those players active in 2021 (see above), but previous seasons\u2019 players are included here to provide more data for exploration and modeling purposes. ^indicates that a more complete walkthrough is below target1-target4 are each daily indexes of digital engagement on a 0-100 scale.  Twitter following data was collected by MLB from Twitter APIs for Major League players, on the first of each month, dating back to 1/1/2018. This dataset is not exhaustive of all players over all months, as not every player has/had a Twitter account, players may create/delete/reinstate accounts at random, or other scenarios preventing follower data from being collected on a given date. Twitter following data was collected by MLB from Twitter APIs for all 30 Major League teams, on the first of each month, dating back to 1/1/2018."
    },
    {
        "name": "H&M Personalized Fashion Recommendations",
        "url": "https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations",
        "overview_text": "Overview text not found",
        "description_text": "H&M Group is a family of brands and businesses with 53 online markets and approximately 4,850 stores. Our online store offers shoppers an extensive selection of products to browse through. But with too many choices, customers might not quickly find what interests them or what they are looking for, and ultimately, they might not make a purchase. To enhance the shopping experience, product recommendations are key. More importantly, helping customers make the right choices also has a positive implications for sustainability, as it reduces returns, and thereby minimizes emissions from transportation. In this competition, H&M Group invites you to develop product recommendations based on data from previous transactions, as well as from customer and product meta data. The available meta data spans from simple data, such as garment type and customer age, to text data from product descriptions, to image data from garment images. There are no preconceptions on what information that may be useful \u2013 that is for you to find out. If you want to investigate a categorical data type algorithm, or dive into NLP and image processing deep learning, that is up to you.",
        "dataset_text": "For this challenge you are given the purchase history of customers across time, along with supporting metadata. Your challenge is to predict what articles each customer will purchase in the 7-day period immediately after the training data ends. Customer who did not make any purchase during that time are excluded from the scoring. NOTE: You must make predictions for all customer_id values found in the sample submission. All customers who made purchases during the test period are scored, regardless of whether they had purchase history in the training data."
    },
    {
        "name": "NBME - Score Clinical Patient Notes",
        "url": "https://www.kaggle.com/competitions/nbme-score-clinical-patient-notes",
        "overview_text": "Overview text not found",
        "description_text": "When you visit a doctor, how they interpret your symptoms can determine whether your diagnosis is accurate. By the time they\u2019re licensed, physicians have had a lot of practice writing patient notes that document the history of the patient\u2019s complaint, physical exam findings, possible diagnoses, and follow-up care. Learning and assessing the skill of writing patient notes requires feedback from other doctors, a time-intensive process that could be improved with the addition of machine learning. Until recently, the Step 2 Clinical Skills examination was one component of the United States Medical Licensing Examination\u00ae (USMLE\u00ae). The exam required test-takers to interact with Standardized Patients (people trained to portray specific clinical cases) and write a patient note. Trained physician raters later scored patient notes with rubrics that outlined each case\u2019s important concepts (referred to as features). The more such features found in a patient note, the higher the score (among other factors that contribute to the final score for the exam). However, having physicians score patient note exams requires significant time, along with human and financial resources. Approaches using natural language processing have been created to address this problem, but patient notes can still be challenging to score computationally because features may be expressed in many ways. For example, the feature \"loss of interest in activities\" can be expressed as \"no longer plays tennis.\" Other challenges include the need to map concepts by combining multiple text segments, or cases of ambiguous negation such as \u201cno cold intolerance, hair loss, palpitations, or tremor\u201d corresponding to the key essential \u201clack of other thyroid symptoms.\u201d In this competition, you\u2019ll identify specific clinical concepts in patient notes. Specifically, you'll develop an automated method to map clinical concepts from an exam rubric (e.g., \u201cdiminished appetite\u201d) to various ways in which these concepts are expressed in clinical patient notes written by medical students (e.g., \u201ceating less,\u201d \u201cclothes fit looser\u201d). Great solutions will be both accurate and reliable. If successful, you'll help tackle the biggest practical barriers in patient note scoring, making the approach more transparent, interpretable, and easing the development and administration of such assessments. As a result, medical practitioners will be able to explore the full potential of patient notes to reveal information relevant to clinical skills assessment. This competition is sponsored by the National Board of Medical Examiners\u00ae (NBME\u00ae). Through research and innovation, NBME supports medical school and residency program educators in addressing issues around the evolution of teaching, learning, technology, and the need for meaningful feedback. NBME offers high-quality assessments and educational services for students, professionals, educators, regulators, and institutions dedicated to the evolving needs of medical education and health care. To serve these communities, NBME collaborates with a diverse and comprehensive array of practicing health professionals, medical educators, state medical board members, test developers, academic researchers, scoring experts and public representatives. NBME gratefully acknowledges the valuable input of Dr Le An Ha from the University of Wolverhampton\u2019s Research Group in Computational Linguistics.",
        "dataset_text": "The text data presented here is from the USMLE\u00ae Step 2 Clinical Skills examination, a medical licensure exam. This exam measures a trainee's ability to recognize pertinent clinical facts during encounters with standardized patients. During this exam, each test taker sees a Standardized Patient, a person trained to portray a clinical case. After interacting with the patient, the test taker documents the relevant facts of the encounter in a patient note. Each patient note is scored by a trained physician who looks for the presence of certain key concepts or features relevant to the case as described in a rubric. The goal of this competition is to develop an automated way of identifying the relevant features within each patient note, with a special focus on the patient history portions of the notes where the information from the interview with the standardized patient is documented. To help you author submission code, we include a few example instances selected from the training set. When your submitted notebook is scored, this example data will be replaced by the actual test data. The patient notes in the test set will be added to the patient_notes.csv file. These patient notes are from the same clinical cases as the patient notes in the training set. There are approximately 2000 patient notes in the test set."
    },
    {
        "name": "USPTO - Explainable AI for Patent Professionals",
        "url": "https://www.kaggle.com/competitions/uspto-explainable-ai",
        "overview_text": "The goal of this competition is to generate Boolean search queries that effectively characterize collections of patent documents. You are challenged to create a query generation model that, given an input set of related patents, outputs a Boolean query that returns the same set of patent documents.",
        "description_text": "Inventions are legally protected by patents. Governments grant patents to inventors, offering exclusive rights for a defined period in exchange for public disclosure to foster innovation in various fields. But before an inventor can obtain a patent, a patent professional must assess whether the invention meets the necessary criteria. AI-powered search tools could help patent professionals streamline these tasks. When using search tools, patent professionals receive certain information on documents in the result set. This information may include text and metadata snippets (such as the classification term(s)) that played a significant role in selecting included information, as well as quantitative measures such as similarity scores. However, this provided information may not always fully explain why the specific documents in the result set were returned. Patent professionals are most familiar with leveraging and reading Boolean search expressions to determine whether they have sufficiently searched the patent space. Your work will help translate result sets from AI and other search tools into the language of patent professionals. By combining the benefits of AI with the familiarity of the Boolean search system with which patent professionals are most familiar, you can help make the patent search process more efficient, effective, and explainable.",
        "dataset_text": "Interpretability is a challenge for many applications of machine learning. AI tools can help patent professionals review new applications, but it's difficult to determine if a given set of results is adequate. Given the prevalence of Boolean-based search engines for patent documents, access to Boolean search expressions that effectively characterize result sets from the AI tools would help patent professionals assess whether they have everything they need to complete their review. Your objective in this competition is to generate a query suitable for traditional patent search tools that returns a specific set of similar patents. This is a code competition. Only a few example rows equivalent to the real test set are available for download. When your submission is scored the test folders will be replaced with versions containing the complete test set. patent_metadata.parquet Metadata for the most recent patents in each patent family. nearest_neighbors.csv The 50 patents most similar to the target patent. patent_data/[year_month].parquet Patent text from Google Patents Public Data by IFI CLAIMS Patent Services and Google. The competition data is current through July, 2023. sample_submission.csv A sample submission file in the correct format. test.csv A subset of nearest_neighbors.csv that will cover 2,500 patents in the hidden dataset. train_index_patent_ids.json A list of the patents included in the Whoosh index. train_index A Whoosh text search index equivalent in size and setup to the index the metric will use to evaluate submitted queries. Only includes patents published on or after 1975. The subset of patents covered by the actual metric index will not be disclosed even to your submission notebook. The following fields are searchable: This competition relies on a patent search database emulator, built with Whoosh. See this notebook for examples of the basic functionality you are likely to need. Note that stopwords and numbers are not indexed. The following search operators are supported: Boolean:OR, AND, NOT, XOR. Proximity: Wildcards: Note that the wildcards are incompatible with the proximity operators and may have substantial performance impacts."
    },
    {
        "name": "LLM 20 Questions",
        "url": "https://www.kaggle.com/competitions/llm-20-questions",
        "overview_text": "In this simulation competition, you must create a language model capable of playing the game 20 Questions. Teams will be paired in 2 vs 2 player matchups and race to deduce the secret word first.",
        "description_text": "Is it a person, place or thing? Is it smaller than a bread box? Is it smaller than a 70B parameter model? 20 Questions is an age-old deduction game where you try to guess a secret word in twenty questions or fewer, using only yes-or-no questions. Players try to deduce the word by narrowing their questions from general to specific, in hopes of guessing the word in the fewest number of questions. Each team will consist of one guesser LLM, responsible for asking questions and making guesses, and one answerer LLM, responsible for responding with \"yes\" or \"no\" answers. Through strategic questioning and answering, the goal is for the guesser to correctly identify the secret word in as few rounds as possible. This competition will evaluate LLMs on key skills like deductive reasoning, efficient information gathering through targeted questioning, and collaboration between paired agents. It also presents a constrained setting requiring creativity and strategy with a limited number of guesses.Success will demonstrate LLMs' capacity for not just answering questions, but also asking insightful questions, performing logical inference, and quickly narrowing down possibilities.",
        "dataset_text": "Code and configuration for LLM 20 Questions as of kaggle-environments 1.14.5. See the latest at https://github.com/Kaggle/kaggle-environments."
    },
    {
        "name": "NeurIPS 2024 - Predict New Medicines with BELKA",
        "url": "https://www.kaggle.com/competitions/leash-BELKA",
        "overview_text": "In this competition, you\u2019ll develop machine learning (ML) models to predict the binding affinity of small molecules to specific protein targets \u2013 a critical step in drug development for the pharmaceutical industry that would pave the way for more accurate drug discovery. You\u2019ll help predict which drug-like small molecules (chemicals) will bind to three possible protein targets.",
        "description_text": "Small molecule drugs are chemicals that interact with cellular protein machinery and affect the functions of this machinery in some way. Often, drugs are meant to inhibit the activity of single protein targets, and those targets are thought to be involved in a disease process. A classic approach to identify such candidate molecules is to physically make them, one by one, and then expose them to the protein target of interest and test if the two interact. This can be a fairly laborious and time-intensive process. The US Food and Drug Administration (FDA) has approved roughly 2,000 novel molecular entities in its entire history. However, the number of chemicals in druglike space has been estimated to be 10^60, a space far too big to physically search. There are likely effective treatments for human ailments hiding in that chemical space, and better methods to find such treatments are desirable to us all. To evaluate potential search methods in small molecule chemistry, competition host Leash Biosciences physically tested some 133M small molecules for their ability to interact with one of three protein targets using DNA-encoded chemical library (DEL) technology. This dataset, the Big Encoded Library for Chemical Assessment (BELKA), provides an excellent opportunity to develop predictive models that may advance drug discovery. Datasets of this size are rare and restricted to large pharmaceutical companies. The current best-curated public dataset of this kind is perhaps bindingdb, which, at 2.8M binding measurements, is much smaller than BELKA. This competition aims to revolutionize small molecule binding prediction by harnessing ML techniques. Recent advances in ML approaches suggest it might be possible to search chemical space by inference using well-trained computational models rather than running laboratory experiments. Similar progress in other fields suggest using ML to search across vast spaces could be a generalizable approach applicable to many domains. We hope that by providing BELKA we will democratize aspects of computational drug discovery and assist the community in finding new lifesaving medicines. Here, you\u2019ll build predictive models to estimate the binding affinity of unknown chemical compounds to specified protein targets. You may use the training data provided; alternatively, there are a number of methods to make small molecule binding predictions without relying on empirical binding data (e.g. DiffDock, and this contest was designed to allow for such submissions). Your work will contribute to advances in small molecule chemistry used to accelerate drug discovery.",
        "dataset_text": "The examples in the competition dataset are represented by a binary classification of whether a given small molecule is a binder or not to one of three protein targets. The data were collected using DNA-encoded chemical library (DEL) technology. We represent chemistry with SMILES (Simplified Molecular-Input Line-Entry System) and the labels as binary binding classifications, one per protein target of three targets. [train/test].[csv/parquet] - The train or test data, available in both the csv and parquet formats. sample_submission.csv - A sample submission file in the correct format All data were generated in-house at Leash Biosciences. We are providing roughly 98M training examples per protein, 200K validation examples per protein, and 360K test molecules per protein. To test generalizability, the test set contains building blocks that are not in the training set. These datasets are very imbalanced: roughly 0.5% of examples are classified as binders; we used 3 rounds of selection in triplicate to identify binders experimentally. Following the competition, Leash will make all the data available for future use (3 targets * 3 rounds of selection * 3 replicates * 133M molecules, or 3.6B measurements). Proteins are encoded in the genome, and names of the genes encoding those proteins are typically bestowed by their discoverers and regulated by the Hugo Gene Nomenclature Committee. The protein products of these genes can sometimes have different names, often due to the history of their discovery. We screened three protein targets for this competition. The first target, epoxide hydrolase 2, is encoded by the EPHX2 genetic locus, and its protein product is commonly named \u201csoluble epoxide hydrolase\u201d, or abbreviated to sEH. Hydrolases are enzymes that catalyze certain chemical reactions, and EPHX2/sEH also hydrolyzes certain phosphate groups. EPHX2/sEH is a potential drug target for high blood pressure and diabetes progression, and small molecules inhibiting EPHX2/sEH from earlier DEL efforts made it to clinical trials. EPHX2/sEH was also screened with DELs, and hits predicted with ML approaches, in a recent study but the screening data were not published. We included EPHX2/sEH to allow contestants an external gut check for model performance by comparing to these previously-published results. We screened EPHX2/sEH purchased from Cayman Chemical, a life sciences commercial vendor. For those contestants wishing to incorporate protein structural information in their submissions, the amino sequence is positions 2-555 from UniProt entry P34913, the crystal structure can be found in PDB entry 3i28, and predicted structure can be found in AlphaFold2 entry 34913. Additional EPHX2/sEH crystal structures with ligands bound can be found in PDB. The second target, bromodomain 4, is encoded by the BRD4 locus and its protein product is also named BRD4. Bromodomains bind to protein spools in the nucleus that DNA wraps around (called histones) and affect the likelihood that the DNA nearby is going to be transcribed, producing new gene products. Bromodomains play roles in cancer progression and a number of drugs have been discovered to inhibit their activities. BRD4 has been screened with DEL approaches previously but the screening data were not published. We included BRD4 to allow contestants to evaluate candidate molecules for oncology indications. We screened BRD4 purchased from Active Motif, a life sciences commercial vendor. For those contestants wishing to incorporate protein structural information in their submissions, the amino acid sequence is positions 44-460 from UniProt entry O60885-1, the crystal structure (for a single domain) can be found in PDB entry 7USK and predicted structure can be found in AlphaFold2 entry O60885. Additional BRD4 crystal structures with ligands bound can be found in PDB. The third target, serum albumin, is encoded by the ALB locus and its protein product is also named ALB. The protein product is sometimes abbreviated as HSA, for \u201chuman serum albumin\u201d. ALB, the most common protein in the blood, is used to drive osmotic pressure (to bring fluid back from tissues into blood vessels) and to transport many ligands, hormones, fatty acids, and more. Albumin, being the most abundant protein in the blood, often plays a role in absorbing candidate drugs in the body and sequestering them from their target tissues. Adjusting candidate drugs to bind less to albumin and other blood proteins is a strategy to help these candidate drugs be more effective. ALB has been screened with DEL approaches previously but the screening data were not published. We included ALB to allow contestants to build models that might have a larger impact on drug discovery across many disease types. The ability to predict ALB binding well would allow drug developers to improve their candidate small molecule therapies much more quickly than physically manufacturing many variants and testing them against ALB empirically in an iterative process. We screened ALB purchased from Active Motif. For those contestants wishing to incorporate protein structural information in their submissions, the amino acid sequence is positions 25 to 609 from UniProt entry P02768, the crystal structure can be found in PDB entry 1AO6, and predicted structure can be found in AlphaFold2 entry P02768. Additional ALB crystal structures with ligands bound can be found in PDB. Good luck!"
    },
    {
        "name": "Learning Agency Lab - Automated Essay Scoring 2.0",
        "url": "https://www.kaggle.com/competitions/learning-agency-lab-automated-essay-scoring-2",
        "overview_text": "The first automated essay scoring competition to tackle automated grading of student-written essays was twelve years ago. How far have we come from this initial competition? With an updated dataset and light years of new ideas we hope to see if we can get to the latest in automated grading to provide a real impact to overtaxed teachers who continue to have challenges with providing timely feedback, especially in underserved communities.",
        "description_text": "Essay writing is an important method to evaluate student learning and performance. It is also time-consuming for educators to grade by hand. Automated Writing Evaluation (AWE) systems can score essays to supplement an educator\u2019s other efforts. AWEs also allow students to receive regular and timely feedback on their writing. However, due to their costs, many advancements in the field are not widely available to students and educators. Open-source solutions to assess student writing are needed to reach every community with these important educational tools.\nPrevious efforts to develop open-source AWEs have been limited by small datasets that were not nationally diverse or focused on common essay formats. The first Automated Essay Scoring competition scored student-written short-answer responses, however, this is a writing task not often used in the classroom. To improve upon earlier efforts, a more expansive dataset that includes high-quality, realistic classroom writing samples was required. Further, to broaden the impact, the dataset should include samples across economic and location populations to mitigate the potential of algorithmic bias.\nIn this competition, you will work with the largest open-access writing dataset aligned to current standards for student-appropriate assessments. Can you help produce an open-source essay scoring algorithm that improves upon the original Automated Student Assessment Prize (ASAP) competition hosted in 2012?\nCompetition host Vanderbilt University is a private research university in Nashville, Tennessee. For this competition, Vanderbilt has partnered with The Learning Agency Lab, an Arizona-based independent nonprofit focused on developing the science of learning-based tools and programs for the social good.\nTo ensure the results of this competition are widely available, winning solutions will be released as open source. More robust and accessible AWE options will help more students get the frequent feedback they need and provide educators with additional support, especially in underserved districts. Vanderbilt University and the Learning Agency Lab would like to thank the Bill & Melinda Gates Foundation, Schmidt Futures, and Chan Zuckerberg Initiative for their support in making this work possible.\n                         ",
        "dataset_text": "The competition dataset comprises about 24000 student-written argumentative essays. Each essay was scored on a scale of 1 to 6 (Link to the Holistic Scoring Rubric). Your goal is to predict the score an essay received from its text. Please note that this is a Code Competition."
    },
    {
        "name": "RSNA 2024 Lumbar Spine Degenerative Classification",
        "url": "https://www.kaggle.com/competitions/rsna-2024-lumbar-spine-degenerative-classification",
        "overview_text": "The goal of this competition is to create models that can be used to aid in the detection and classification of degenerative spine conditions using lumbar spine MR images. Competitors will develop models that simulate a radiologist's performance in diagnosing spine conditions.",
        "description_text": "Low back pain is the leading cause of disability worldwide, according to the World Health Organization, affecting 619 million people in 2020. Most people experience low back pain at some point in their lives, with the frequency increasing with age. Pain and restricted mobility are often symptoms of spondylosis, a set of degenerative spine conditions including degeneration of intervertebral discs and subsequent narrowing of the spinal canal (spinal stenosis), subarticular recesses, or neural foramen with associated compression or irritations of the nerves in the low back. Magnetic resonance imaging (MRI) provides a detailed view of the lumbar spine vertebra, discs and nerves, enabling radiologists to assess the presence and severity of these conditions. Proper diagnosis and grading of these conditions help guide treatment and potential surgery to help alleviate back pain and improve overall health and quality of life for patients. RSNA has teamed with the American Society of Neuroradiology (ASNR) to conduct this competition exploring whether artificial intelligence can be used to aid in the detection and classification of degenerative spine conditions using lumbar spine MR images. The challenge will focus on the classification of five lumbar spine degenerative conditions: Left Neural Foraminal Narrowing, Right Neural Foraminal Narrowing, Left Subarticular Stenosis, Right Subarticular Stenosis, and Spinal Canal Stenosis. For each imaging study in the dataset, we\u2019ve provided severity scores (Normal/Mild, Moderate, or Severe) for each of the five conditions across the intervertebral disc levels L1/L2, L2/L3, L3/L4, L4/L5, and L5/S1. To create the ground truth dataset, the RSNA challenge planning task force collected imaging data sourced from eight sites on five continents. This multi-institutional, expertly curated dataset promises to improve standardized classification of degenerative lumbar spine conditions and enable development of tools to automate accurate and rapid disease classification. Challenge winners will be recognized at an event during the RSNA 2024 annual meeting. For more information on the challenge, contact RSNA Informatics staff at informatics@rsna.org.",
        "dataset_text": "The goal of this competition is to identify medical conditions affecting the lumbar spine in MRI scans. This competition uses a hidden test. When your submitted notebook is scored, the actual test data (including a full length sample submission) will be made available to your notebook. train.csv Labels for the train set. train_label_coordinates.csv sample_submission.csv [train/test]_images/[study_id]/[series_id]/[instance_number].dcm The imagery data. [train/test]_series_descriptions.csv"
    },
    {
        "name": "LLMs - You Can't Please Them All",
        "url": "https://www.kaggle.com/competitions/llms-you-cant-please-them-all",
        "overview_text": "This competition challenges you to identify exploits for an LLM-as-a-judge system designed to evaluate the quality of essays. You'll be given a list of essay topics and your goal will be to submit an essay that maximizes disagreement between the LLM judges. Your work will help to form a better understanding of the capabilities and limitations of using LLMs for subjective evaluations tasks at scale.",
        "description_text": "It\u2019s increasingly common to use LLMs for subjective evaluations such as ranking and scoring the quality of generated text. However, any automated rating system is vulnerable to exploits. Different models will have different degrees of self-bias, position-bias, length-bias, and style-bias that might negatively impact their ability to provide robust assessments (Zheng 2023, Wang 2023, Panickssery 2024). Likewise, different models will have different degrees of vulnerabilities to targeted exploits, such as universal jailbreaks, that can be used to misguide the system (Wallace 2021, Zou 2023, Li 2024, Rando 2024). One method to improve the robustness of automated judging systems is to include multiple LLM models to form a LLM-judging committee. Each model is distantly related to the other, decreasing the chances of having common vulnerabilities. An advantage of LLM-judging committees is that they are less sensitive to exploits that impact only a single model. This competition attempts to answer the question of whether or not individual LLM judges can be coerced into returning inflated scores that diverge substantially from a group consensus. By identifying exploits used to unfairly bias an evaluation in a given direction, you will help the ML community better understand the strengths and weaknesses of using AI systems to make subjective decisions at scale.",
        "dataset_text": "The competition dataset consists of a list of essay topics. The goal of the competition is to come up with short essays that cause the LLM-judges to diverge in their scoring. There is no training data. Please note that this is a Code Competition. When your submission is scored, this example test data will be replaced with the full test set. There are only 3 rows in the public data but you can expect approximately 1000 rows in the hidden test set."
    },
    {
        "name": "NeurIPS 2024 - Lux AI Season 3",
        "url": "https://www.kaggle.com/competitions/lux-ai-season-3",
        "overview_text": "Welcome to Season 3! The goal of this competition is to create and/or train AI bots to play a novel multi-agent 1v1 game against other submitted agents. We hope to create a fun multi-agent competition for all in addition to advancing research into AI, imitation/reinforcement learning, and meta learning. Learn more about the competition specifics by reading below!",
        "description_text": "With the help 600+ space organizations, Mars has been terraformed successfully. Colonies have been established successfully by multiple space organizations thanks to the rapidly growing lichen fields on the planet. The introduction of an atmosphere has enabled colonists to start thinking about the future, beyond Mars. Mysteriously, new deep-space telescopes launched from Mars revealed some ancient architectures floating beyond the solar system, hidden in a midst of asteroids and nebula gas. Perhaps they were relics of a previous sentient species? Seeking to learn more about the secrets of the universe, new expeditions of ships were set out into deep space to explore these ancient relics and study them. What will they discover, which expedition will be remembered for the rest of history for unlocking the secrets of the relics? The Lux AI Challenge is a competition where competitors design agents to tackle a multi-variable optimization, resource gathering, and allocation problem in a 1v1 scenario against other competitors. In addition to optimization, successful agents must be capable of analyzing their opponents and developing appropriate policies to get the upper hand. All code can be found at our Github, make sure to give it a star while you are there! Make sure to join our community discord to chat, strategize, and learn with other competitors! We will be posting announcements on the Kaggle Forums and on the discord.",
        "dataset_text": "This is the folder for the Python kit. Please make sure to read the instructions as they are important regarding how you will write a bot and submit it to the competition. For kits in other languages please see our Github repository"
    },
    {
        "name": "FIDE & Google Efficient Chess AI Challenge",
        "url": "https://www.kaggle.com/competitions/fide-google-efficiency-chess-ai-challenge",
        "overview_text": "In this simulation competition, you are challenged with developing an agent that plays chess under strict CPU and memory limitations.",
        "description_text": "\"Thinking rigorously about the construction of a chess-playing computer might act as a wedge in attacking other problems of a similar nature and of greater significance\" - Claude Shannon (1950) Chess, often referred to as the \"royal game,\" is a two-player strategy board game renowned for its intricate complexities and demanding mental challenges. Mastering chess necessitates a profound comprehension of both strategic planning and tactical execution. It's a battlefield where foresight, calculation, and adaptability reign supreme. Chess has long been a grand challenge for artificial intelligence, a proving ground for pushing the boundaries of algorithms and computational power. While advancements like AlphaZero and Stockfish engines have achieved superhuman performance, they often rely on vast resources inaccessible to most developers. This particular competition, however, introduces a fascinating twist by emphasizing efficiency and elegance in addition to raw strategic prowess. This competition aims to shift the focus from brute-force computation to elegant and efficient design. Forget massive pre-computed tables and endless search trees \u2013 we're leveling the playing field and focusing on efficiency and strategic thinking. You're challenged to devise innovative and efficient solutions to play chess against other agents, thereby further expanding the frontiers of AI research. Your exploration of novel, optimized techniques can address a growing complexity and scale of problems, like advancements in modeling and inference techniques and improvements upon traditional heuristic-based algorithms, beyond the realm of chess.",
        "dataset_text": "Code and configuration for Chess as of kaggle-environments 1.14.5. See the latest at https://github.com/Kaggle/kaggle-environments."
    },
    {
        "name": "WSDM Cup - Multilingual Chatbot Arena",
        "url": "https://www.kaggle.com/competitions/wsdm-cup-multilingual-chatbot-arena",
        "overview_text": "This competition challenges you to predict which responses users will prefer in a head-to-head battle between chatbots powered by large language models (LLMs). You'll be given a dataset of conversations from the Chatbot Arena, where different LLMs generate answers to user prompts. By developing a winning machine learning model, you'll help improve how chatbots interact with humans and ensure they better align with human preferences.",
        "description_text": "Large language models (LLMs) are rapidly entering our lives, but ensuring their responses resonate with users is critical for successful interaction. This competition presents a unique opportunity to tackle this challenge with real-world data and help us bridge the gap between LLM capability and human preference. We utilized a large dataset collected from Chatbot Arena, where users chat with two anonymous LLMs and choose the answer they prefer. Your task in this competition is to predict which response a user will prefer in these head-to-head battles. This challenge aligns with the concept of \"reward models\" or \"preference models\" in reinforcement learning from human feedback (RLHF). Previous research has identified limitations in directly prompting an existing LLM for preference predictions. These limitations often stem from biases such as favoring responses presented first (position bias), being overly verbose (verbosity bias), or exhibiting self-promotion (self-enhancement bias). We encourage you to explore various machine-learning techniques to build a model that can effectively predict user preferences. Your work will be instrumental in developing LLMs that can tailor responses to individual user preferences, ultimately leading to more user-friendly and widely accepted AI-powered conversation systems.",
        "dataset_text": "The competition dataset consists of user interactions from the ChatBot Arena (formerly LMSYS). In each user interaction a judge provides one prompt to two different large language models and then indicates which of the models gave the more satisfactory response. This is a Code Competition. When your submission is scored the example test data will be replaced with the full test set. The competition will proceed in two phases: The Chatbot Arena team may release additional data during the model training phase. train.parquet test.parquet sample_submission.csv A submission file in the correct format. Note that the dataset for this competition contains text that may be considered profane, vulgar, or offensive."
    },
    {
        "name": "Santa 2024 - The Perplexity Permutation Puzzle",
        "url": "https://www.kaggle.com/competitions/santa-2024",
        "overview_text": "Overview text not found",
        "description_text": " Minimizing perplexity, a task quite grand, A neural network\u2019s mission, across the land. A model, trained on texts, both vast and deep, To rearrange the words, while others sleep. It sorts and sifts, with algorithms bright, To find the clearest path, the words made right. A digital reindeer, with circuits aglow, Reducing confusion, a wondrous show. No longer puzzled, the reader\u2019s mind at ease, As the model\u2019s magic, effortlessly, please. A gift of clarity, a present divine, A neural network\u2019s work, a clever design. The Challenge Just like those mischievous elves who mixed up the ornaments on Santa's Christmas tree, someone has scrambled the words in classic Christmas tales! Your task, dear Kagglers, is to put those words back in order, minimizing the perplexity of each passage. The lower the perplexity, the more sense the story makes! The Data Rudolph has provided a sleigh-full of jumbled text passages, each one a beloved Christmas story in disarray. He's even included a special metric to guide you, a measure of how puzzled a reader would be by the mixed-up words. The Goal Use your coding magic and linguistic skills to rearrange the words, making the stories flow smoothly and beautifully once more. The Kagglers who achieve the lowest perplexity scores will win Rudolph's heart and earn a place on the leaderboard! So join Rudolph and his friends in this festive challenge! Untangle the words, spread Christmas cheer, and help make this a holiday to remember!",
        "dataset_text": "Your task in this competition is to minimize the perplexity score of each id row by shuffling individual words in the text column. See the Evaluation page for more details on the evaluation."
    },
    {
        "name": "Allstate Purchase Prediction Challenge",
        "url": "https://www.kaggle.com/competitions/allstate-purchase-prediction-challenge",
        "overview_text": "Overview text not found",
        "description_text": " As a customer shops an insurance policy, he/she will receive a number of quotes with different coverage options before purchasing a plan. This is represented in this challenge as a series of rows that include a customer ID, information about the customer, information about the quoted policy, and the cost. Your task is to predict the purchased coverage options using a limited subset of the total interaction history. If the eventual purchase can be predicted sooner in the shopping window, the quoting process is shortened and the issuer is less likely to lose the customer's business. Using a customer\u2019s shopping history, can you predict what policy they will end up choosing?",
        "dataset_text": "The training and test sets contain transaction history for customers that ended up purchasing a policy. For each customer_ID, you are given their quote history. In the training set you have the entire quote history, the last row of which contains the coverage options they purchased. In the test set, you have only a partial history of the quotes and do not have the purchased coverage options. These are truncated to certain lengths to simulate making predictions with less history (higher uncertainty) or more history (lower uncertainty). For each customer_ID in the test set, you must predict the seven coverage options they end up purchasing. Each customer has many shopping points, where a shopping point is defined by a customer with certain characteristics viewing a product and its associated cost at a particular time. Each product has 7 customizable options selected by customers, each with 2, 3, or 4 ordinal values possible:  A product is simply a vector with length 7 whose values are chosen from each of the options listed above. The cost of a product is a function of both the product options and customer characteristics. customer_ID - A unique identifier for the customer\nshopping_pt - Unique identifier for the shopping point of a given customer\nrecord_type - 0=shopping point, 1=purchase point\nday - Day of the week (0-6, 0=Monday)\ntime - Time of day (HH:MM)\nstate - State where shopping point occurred\nlocation - Location ID where shopping point occurred\ngroup_size - How many people will be covered under the policy (1, 2, 3 or 4)\nhomeowner - Whether the customer owns a home or not (0=no, 1=yes)\ncar_age - Age of the customer\u2019s car\ncar_value - How valuable was the customer\u2019s car when new\nrisk_factor - An ordinal assessment of how risky the customer is (1, 2, 3, 4)\nage_oldest - Age of the oldest person in customer's group\nage_youngest - Age of the youngest person in customer\u2019s group\nmarried_couple - Does the customer group contain a married couple (0=no, 1=yes)\nC_previous - What the customer formerly had or currently has for product option C (0=nothing, 1, 2, 3,4)\nduration_previous -  how long (in years) the customer was covered by their previous issuer\nA,B,C,D,E,F,G - the coverage options\ncost - cost of the quoted coverage options"
    },
    {
        "name": "Coupon Purchase Prediction",
        "url": "https://www.kaggle.com/competitions/coupon-purchase-prediction",
        "overview_text": "Overview text not found",
        "description_text": "Recruit Ponpare is Japan's leading joint coupon site, offering huge discounts on everything from hot yoga, to gourmet sushi, to a summer concert bonanza. Ponpare's coupons open doors for customers they've only dreamed of stepping through. They can learn difficult to acquire skills, go on unheard of adventures, and dine like (and with) the stars. Investing in a new experience is not cheap. We fear wasting our time and money on a product or service that we may not enjoy or fully understand. Ponpare takes the high price out of this equation, making it easier for you to take the leap towards your first sky-dive or diamond engagement ring. Using past purchase and browsing behavior, this competition asks you to predict which coupons a customer will buy in a given period of time. The resulting models will be used to improve Ponpare's recommendation system, so they can make sure their customers don't miss out on their next favorite thing. ",
        "dataset_text": "You are provided with a year of transactional data for 22,873 users on the site ponpare.jp. The training set spans the dates 2011-07-01 to 2012-06-23. The test set spans the week after the end of the training set, 2012-06-24 to 2012-06-30. The goal of the competition is to recommend a ranked list of coupons for each user in the dataset (found in user_list.csv). Your predictions are scored against the actual coupon purchases, made during the test set week, of the 310 possible test set coupons. The dataset has relational format, with hashed ID columns for each entity. "
    },
    {
        "name": "The Winton Stock Market Challenge",
        "url": "https://www.kaggle.com/competitions/the-winton-stock-market-challenge",
        "overview_text": "Overview text not found",
        "description_text": "Do you laugh (and then get down to work) in the face of terabytes of noisy, non-stationary data? Winton Capital is looking for data scientists who excel at finding the hidden signal in the proverbial haystack, and who are excited by creating novel statistical modelling and data mining techniques.  In this recruiting competition, Winton challenges you to take on the very difficult task of predicting the future (stock returns). Given historical stock performance and a host of masked features, can you predict intra and end of day returns without being deceived by all the noise?  Research scientists at Winton have crafted this competition to be challenging and fun for the community while providing a taste of the types of problems they work on everyday. They're excited to connect with Kagglers who bring a unique background and creative approach to the competition. Winton is offering cash prizes to winning teams as a reward for their work, but the intent of the competition is not commercial. The intellectual property you create remains your own and will be evaluated in the context of suitability for employment.   For more on the culture at Winton, check out the About Winton page or their careers page.",
        "dataset_text": "Updated 2015-12-21: Winton have added new data into the test set. If you downloaded the test set before 2015-12-21 please re-download the data set and submit predictions on this instead.  In this competition the challenge is to predict the return of a stock, given the history of the past few days.  We provide 5-day windows of time, days D-2, D-1, D, D+1, and D+2. You are given returns in days D-2, D-1, and part of day D, and you are asked to predict the returns in the rest of day D, and in days D+1 and D+2. During day D, there is intraday return data, which are the returns at different points in the day. We provide 180 minutes of data, from t=1 to t=180. In the training set you are given the full 180 minutes, in the test set just the first 120 minutes are provided. For each 5-day window, we also provide 25 features, Feature_1 to Feature_25. These may or may not be useful in your prediction. Each row in the dataset is an arbitrary stock at an arbitrary 5 day time window.  How these returns are calculated is defined by Winton, and will not to be revealed to you in this competition. The data set is designed to be representative of real data and so should bring about a number of challenges."
    },
    {
        "name": "Predicting Red Hat Business Value",
        "url": "https://www.kaggle.com/competitions/predicting-red-hat-business-value",
        "overview_text": "Overview text not found",
        "description_text": " Like most companies, Red Hat is able to gather a great deal of information over time about the behavior of individuals who interact with them. They\u2019re in search of better methods of using this behavioral data to predict which individuals they should approach\u2014and even when and how to approach them. In this competition, Kagglers are challenged to create a classification algorithm that accurately identifies which customers have the most potential business value for Red Hat based on their characteristics and activities. With an improved prediction model in place, Red Hat will be able to more efficiently prioritize resources to generate more business and better serve their customers. ",
        "dataset_text": "This competition uses two separate data files that may be joined together to create a single, unified data table: a people file and an activity file. The people file contains all of the unique people (and the corresponding characteristics) that have performed activities over time. Each row in the people file represents a unique person. Each person has a unique people_id. The activity file contains all of the unique activities (and the corresponding activity characteristics) that each person has performed over time. Each row in the activity file represents a unique activity performed by a person on a certain date. Each activity has a unique activity_id. The challenge of this competition is to predict the potential business value of a person who has performed a specific activity. The business value outcome is defined by a yes/no field attached to each unique activity in the activity file. The outcome field indicates whether or not each person has completed the outcome within a fixed window of time after each unique activity was performed. The activity file contains several different categories of activities. Type 1 activities are different from type 2-7 activities because there are more known characteristics associated with type 1 activities (nine in total) than type 2-7 activities (which have only one associated characteristic). To develop a predictive model with this data, you will likely need to join the files together into a single data set. The two files can be joined together using person_id as the common key. All variables are categorical, with the exception of 'char_38' in the people file, which is a continuous numerical variable."
    },
    {
        "name": "Statoil/C-CORE Iceberg Classifier Challenge",
        "url": "https://www.kaggle.com/competitions/statoil-iceberg-classifier-challenge",
        "overview_text": "Overview text not found",
        "description_text": " Drifting icebergs present threats to navigation and activities in areas such as offshore of the East Coast of Canada. Currently, many institutions and companies use aerial reconnaissance and shore-based support to monitor environmental conditions and assess risks from icebergs. However, in remote areas with particularly harsh weather, these methods are not feasible, and the only viable monitoring option is via satellite. Statoil, an international energy company operating worldwide, has worked closely with companies like C-CORE. C-CORE have been using satellite data for over 30 years and have built a computer vision based surveillance system. To keep operations safe and efficient, Statoil is interested in getting a fresh new perspective on how to use machine learning to more accurately detect and discriminate against threatening icebergs as early as possible. In this competition, you\u2019re challenged to build an algorithm that automatically identifies if a remotely sensed target is a ship or iceberg. Improvements made will help drive the costs down for maintaining safe working conditions.",
        "dataset_text": "In this competition, you will predict whether an image contains a ship or an iceberg. The labels are provided by human experts and geographic knowledge on the target. All the images are 75x75 images with two bands. The data (train.json, test.json) is presented in json format.\nThe files consist of a list of images, and for each image, you can find the following fields: Please note that we have included machine-generated images in the test set to prevent hand labeling. They are excluded in scoring. The submission file in the correct format:"
    },
    {
        "name": "Google Cloud & NCAA\u00ae ML Competition 2018-Men's",
        "url": "https://www.kaggle.com/competitions/mens-machine-learning-competition-2018",
        "overview_text": "Overview text not found",
        "description_text": "Google Cloud and NCAA\u00ae have teamed up to bring you this year\u2019s version of the Kaggle machine learning competition. Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. Kagglers will join the millions of fans who attempt to forecast the outcomes of March Madness\u00ae during this year's NCAA Division I Men\u2019s and Women\u2019s Basketball Championships. But unlike most fans, you will pick your bracket using a combination of NCAA\u2019s historical data and your computing power, while the ground truth unfolds on national television.  In the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset. In the second stage, competitors will forecast outcomes of all possible match-ups in the 2018 NCAA Division I Men\u2019s and Women\u2019s Basketball Championships. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2018 results. This page is for the NCAA Division I Men's tournament. Check out the NCAA Division I Women's tournament here.",
        "dataset_text": "Each season there are thousands of NCAA basketball games played between Division I men's teams, culminating in March Madness\u00ae, the 68-team national championship that starts in the middle of March. We have provided a large amount of historical data about college basketball games and teams, going back many years. Armed with this historical data, you can explore it and develop your own distinctive ways of predicting March Madness\u00ae game outcomes. You can even evaluate and compare different approaches by seeing which of them would have done best at predicting tournament games from the past. If you are unfamiliar with the format and intricacies of the NCAA\u00ae tournament, we encourage reading the wikipedia page before diving into the data.  The data description and schema may seem daunting at first, but is not as complicated as it appears. As a reminder, you are encouraged to incorporate your own sources of data. We have provided extensive historical data to jump-start the modeling process, and this data is self-consistent (for instance, dates and team ID's are always treated the same way). Nevertheless, you may also be able to make productive use of external data. If you head down this path, please be forewarned that many sources have their own distinctive way of identifying the names of teams, and this can make it challenging to link up with our data. The TeamSpellings file, which is listed in the bottom section below, may help you map external team references into our own Team ID structure, and you may also need to understand exactly how dates work in our data. Please also note that we have standardized the spelling of column names and some filenames, so if you are re-using code from previous instances of this contest, you may need to adjust for this. For example, we are universally referencing Team ID columns with a spelling of \"TeamID\" rather than \"team_id\". And this year the seeds file is NCAATourneySeeds.csv rather than TourneySeeds.csv We extend our gratitude to Kenneth Massey for providing much of the historical data. Special Acknowledgment to Jeff Sonas of Sonas Consulting for his support in assembling the dataset for this competition. Stage 1 - You should submit predicted probabilities for every possible matchup in the past 4 NCAA\u00ae tournaments (2014-2017). Stage 2 - You should submit predicted probabilities for every possible matchup before the 2018 tournament begins. Refer to the Timeline page for specific dates. In both stages, the sample submission will tell you which games to predict. Below we describe the format and fields of the contest data files. The data will likely be refreshed once in late February while Stage 1 of the competition is running. At the start of Stage 2, we will provide updates to these files to incorporate data from the current season. This section provides everything you need to build a simple prediction model and submit predictions. Special note about \"Season\" numbers: the college basketball season lasts from early November until the national championship tournament that starts in the middle of March. For instance, this year the first men\u2019s Division I games were played on November 10th, 2017 and the men\u2019s national championship game will be played on April 2nd, 2018. Because a basketball season spans two calendar years like this, it can be confusing to refer to the year of the season. By convention, when we identify a particular season, we will reference the year that the season ends in, not the year that it starts in. So for instance, the current season will be identified in our data as the 2018 season, not the 2017 season or the 2017-18 season or the 2017-2018 season, though you may see any of these in everyday use outside of our data. Data Section 1 file: Teams.csv This file identifies the different college teams present in the dataset. You will not see games present for all teams in all seasons, because the games listing is only for matchups where both teams are Division-I teams. There are 351 teams currently in Division-I, and an overall total of 364 teams in our team listing (each year, some teams might start being Division-I programs, and others might stop being Division-I programs). Each team has a 4 digit id number. Data Section 1 file: Seasons.csv This file identifies the different seasons included in the historical data, along with certain season-level properties. Data Section 1 file: NCAATourneySeeds.csv This file identifies the seeds for all teams in each NCAA\u00ae tournament, for all seasons of historical data. Thus, there are between 64-68 rows for each year, depending on whether there were any play-in games and how many there were. In recent years the structure has settled at 68 total teams, with eight \"play-in\" games leading to the final field of 64 teams entering Round 1 on Thursday of the first week. We will not know the seeds of the respective tournament teams, or even exactly which 68 teams it will be, until Selection Sunday on March 11, 2018. Data Section 1 file: RegularSeasonCompactResults.csv This file identifies the game-by-game results for many seasons of historical data, starting with the 1985 season (the first year the NCAA\u00ae had a 64-team tournament). For each season, the file includes all games played from daynum 0 through 132. It is important to realize that the \"Regular Season\" games are simply defined to be all games played on DayNum=132 or earlier (DayNum=132 is Selection Sunday, and there are always a few conference tournament finals actually played early in the day on Selection Sunday itself). Thus a game played on or before Selection Sunday will show up here whether it was a pre-season tournament, a non-conference game, a regular conference game, a conference tournament game, or whatever. Data Section 1 file: NCAATourneyCompactResults.csv This file identifies the game-by-game NCAA\u00ae tournament results for all seasons of historical data. The data is formatted exactly like the RegularSeasonCompactResults data. Note that these games also include the play-in games (which always occurred on day 134/135) for those years that had play-in games. Thus each season you will see between 63 and 67 games listed, depending on how many play-in games there were. Because of the consistent structure of the NCAA\u00ae tournament schedule, you can actually tell what round a game was, depending on the exact DayNum. Thus: Special note: Each year, there are also going to be other games that happened after Selection Sunday, which are not part of the NCAA\u00ae Tournament. This includes tournaments like the postseason NIT, the CBI, the CIT, and the Vegas 16. Such games are not listed in the Regular Season or the NCAA\u00ae Tourney files; they can be found in the \"Secondary Tourney\" data files within Data Section 6. Although they would not be games you would ever be predicting directly for the NCAA\u00ae tournament, and they would not be games you would have data from at the time of predicting NCAA\u00ae tournament outcomes, you may nevertheless wish to make use of these games for model optimization, depending on your methodology. The more games that you can test your predictions against, the better your optimized model might eventually become, depending on how applicable all those games are. A similar argument might be advanced in favor of optimizing your predictions against conference tournament games, which might be viewed as fairly similar to NCAA\u00ae tournament games.  Data Section 1 file: SampleSubmissionStage1.csv This file illustrates the submission file format for Stage 1. It is the simplest possible submission: a 50% winning percentage is predicted for each possible matchup. A submission file lists every possible matchup between tournament teams for one or more years. During Stage 1, you are asked to make predictions for all possible matchups from the past four NCAA\u00ae tournaments (seasons 2014, 2015, 2016, and 2017). In Stage 2, you will be asked to make predictions for all possible matchups from the current NCAA\u00ae tournament (season 2018). When there are 68 teams in the tournament, there are 68*67/2=2,278 predictions to make for that year, so a Stage 1 submission file will have 2,278*4=9,112 data rows. Example #1: You want to make a prediction for Duke (TeamID=1181) against Arizona (TeamID=1112) in the 2012 tournament, with Duke given a 53% chance to win and Arizona given a 47% chance to win. In this case, Arizona has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Arizona's perspective (47%): 2012_1112_1181,0.47 Example #2: You want to make a prediction for Duke (TeamID=1181) against North Carolina (TeamID=1314) in the 2012 tournament, with Duke given a 51.6% chance to win and North Carolina given a 48.4% chance to win. In this case, Duke has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Duke's perspective (51.6%): 2012_1181_1314,0.516 This section provides game-by-game stats at a team level (free throws attempted, defensive rebounds, turnovers, etc.) for all regular season, conference tournament, and NCAA\u00ae tournament games since the 2002-03 season.  Team Box Scores are provided in \"Detailed Results\" files rather than \"Compact Results\" files. However, the two files are strongly related. In a Detailed Results file, the first eight columns (Season, DayNum, WTeamID, WScore, LTeamID, LScore, WLoc, and NumOT) are exactly the same as a Compact Results file. However, in a Detailed Results file, there are many additional columns. The column names should be self-explanatory to basketball fans (as above, \"W\" or \"L\" refers to the winning or losing team): (and then the same set of stats from the perspective of the losing team: LFGM is the number of field goals made by the losing team, and so on up to LPF). Note: by convention, \"field goals made\" (either WFGM or LFGM) refers to the total number of fields goals made by a team, a combination of both two-point field goals and three-point field goals. And \"three point field goals made\" (either WFGM3 or LFGM3) is just the three-point fields goals made, of course. So if you want to know specifically about two-point field goals, you have to subtract one from the other (e.g., WFGM - WFGM3). And the total number of points scored is most simply expressed as 2*FGM + FGM3 + FTM. Data Section 2 file: RegularSeasonDetailedResults.csv This file provides team-level box scores for many regular seasons of historical data, starting with the 2003 season. All games listed in the RegularSeasonCompactResults file since the 2003 season should exactly be present in the RegularSeasonDetailedResults file. Data Section 2 file: NCAATourneyDetailedResults.csv This file provides team-level box scores for many NCAA\u00ae tournaments, starting with the 2003 season. All games listed in the NCAATourneyCompactResults file since the 2003 season should exactly be present in the NCAATourneyDetailedResults file. This section provides city locations of all regular season, conference tournament, and NCAA\u00ae tournament games since the 2009-10 season Data Section 3 file: Cities.csv This file provides a master list of cities that have been locations for games played. Data Section 3 file: GameCities.csv This file identifies all games, starting with the 2010 season, along with the city that the game was played in. Games from the regular season, the NCAA\u00ae tourney, and other post-season tournaments, are all listed together. This section provides weekly team rankings for dozens of top rating systems - Pomeroy, Sagarin, RPI, ESPN, etc., since the 2002-2003 season Data Section 4 file: MasseyOrdinals.zip containing MasseyOrdinals.csv This zip file contains a large CSV file, listing out rankings (e.g. #1, #2, #3, ..., #N) of teams going back to the 2002-2003 season, under a large number of different ranking system methodologies. The information was gathered by Kenneth Massey and provided on his College Basketball Ranking Composite page. Note that a rating system is more precise than a ranking system, because a rating system can provide insight about the strength gap between two teams. A ranking system will just tell you who is #1 or who is #2, but a rating system might tell you whether the gap between #1 and #2 is large or small. Nevertheless, it can be hard to compare two different rating systems that are expressed in different scales, so it can be very useful to express all the systems in terms of their ordinal ranking (1, 2, 3, ..., N) of teams. Disclaimer: you ought to be careful about your methodology when using or evaluating these ranking systems. They are presented on a weekly basis, and given a consistent date on the Massey Composite page that typically is a Sunday; that is how the ranking systems can be compared against each other on this page. However, these systems each follow their own timeline and some systems may be released on a Sunday and others on a Saturday or Monday or even Tuesday. You should remember that if a ranking is released on a Tuesday, and was calculated based on games played through Monday, it will make the system look unusually good at predicting if you use that system to forecast the very games played on Monday that already inform the rankings. To avoid this methodological trap, we have typically used a conservative RankingDayNum of Wednesday to represent the rankings that were released at approximately the end of the weekend, a few days before, even though those rankings are represented on the composite page as being on a Sunday. For some of the older years, a more precise timestamp was known for each ranking system that allowed a more precise assignment of a RankingDayNum. By convention, the final pre-tournament rankings are always expressed as RankingDayNum=133, even though sometimes the rankings for individual systems are not released until Tuesday (DayNum=134) or even Wednesday or Thursday. If you decide to use some rankings from these Massey Ordinals to inform your predictions, be forewarned that we have no control over when they are released, and not all systems may turn out to be available in time to make pre-tournament predictions by our submission deadline. In such a situation, you may wish to use the rankings from DayNum=128 or you may need to dig into the details of the actual source of the rankings, by following the respective links on the Massey Composite Page. We may also be able to provide partial releases of the final pre-tournament Massey Ordinals on the forums, so that as systems come in on Monday or Tuesday you can use them right away. This section provides play-by-play event logs for 99% of regular season, conference tournament, and NCAA\u00ae tournament games since the 2009-10 season - including plays by individual players. Data Section 5 files: PlayByPlay_201X.zip containing Events_201X.csv and Players_201X.csv Each zip file (PlayByPlay_2010.zip, PlayByPlay_2011.zip, ..., PlayByPlay_2017.zip) contains two CSV files, listing the play-by-play event logs for almost all games from that season. Each event is assigned to either a team or one of the team's players (by name). The players are listed by PlayerID within the Players csv file for that year, and the play-by-play events are listed (including a PlayerID) within the Events csv file for that year.. Data Section 5 file: Events_201X.csv Event Types: Data Section 5 file: Players_201X.csv Note: there are errors within the events, in that they don't necessarily add up to the final stats for the game. Nevertheless, this was the highest quality data we could achieve for the near-complete set of games. This section contains additional supporting information, including coaches, conference affiliations, alternative team name spellings, bracket structure, game results for NIT and other postseason tournaments Data Section 6 file: TeamCoaches.csv This file indicates the head coach for each team in each season, including a start/finish range of DayNums to indicate a mid-season coaching change. For scenarios where a team had the same head coach the entire year, they will be listed with a DayNum range of 0 to 154 for that season. For head coaches whose term lasted many years, there will be many rows listed, most of which have a DayNum range of 0 to 154 for the corresponding year. Data Section 6 file: Conferences.csv This file indicates the Division I conferences that have existed over the years since 1985. Each conference is listed with an abbreviation and a longer name. Data Section 6 file: TeamConferences.csv This file indicates the conference affiliations for each team during each season. Some conferences have changed their names from year to year, and/or changed which teams are part of the conference. This file tracks this information historically. Data Section 6 file: ConferenceTourneyGames.csv This file indicates which games were part of each year's post-season conference tournaments (all of which finished on Selection Sunday or earlier), starting from the 2001 season. Many of these conference tournament games are held on neutral sites, and many of the games are played by tournament-caliber teams just a few days before the NCAA\u00ae tournament. Thus these games could be considered as very similar to NCAA\u00ae tournament games, and (depending on your methodology) may be of use in optimizing your predictions. However, this is NOT a new listing of games; these games are already present within the RegularSeasonCompactResults and RegularSeasonDetailedResults files. So this file simply helps you to identify which of the \"regular season\" games since the 2001 season were actually conference tournament games, in case that is useful information. Data Section 6 file: SecondaryTourneyTeams.csv This file identifies the teams that participated in post-season tournaments other than the NCAA\u00ae Tournament (such events would run in parallel with the NCAA\u00ae Tournament). These are teams that were not invited to the NCAA\u00ae Tournament and instead were invited to some other tournament, of which the NIT is the most prominent tournament, but there have also been the CBI, CIT, and Vegas 16 (V16) at various points in recent years. Depending on your methodology, you might find it useful to have these additional game results, above and beyond what is available from the NCAA\u00ae Tournament results. Many of these teams, especially in the NIT, were \"bubble\" teams of comparable strength to several NCAA\u00ae Tournament invitees, and so these games may be of use in model optimization for predicting NCAA\u00ae Tournament results. Data Section 6 file: SecondaryTourneyCompactResults.csv This file indicates the final scores for the tournament games of \"secondary\" post-season tournaments: the NIT, CBI, CIT, and Vegas 16. The detailed results (team box scores) have not been assembled for these games. For the most part, this file is exactly like other Compact Results listings, although it also has a column for Secondary Tourney. Data Section 6 file: TeamSpellings.csv This file indicates alternative spellings of many team names. It is intended for use in associating external spellings against our own TeamID numbers, thereby helping to relate the external data properly with our datasets. Over the years we have identified various external spellings of different team names (as an example, for Ball State we have seen \"ball st\", and \"ball st.\", and \"ball state\", and \"ball-st\", and \"ball-state\"). Other teams have had more significant changes to their names over the years; for example, \"Texas Pan-American\" and \"Texas-Rio Grande Valley\" are actually the same school. The current list is obviously not exhaustive, and we encourage participants to identify additional mappings and upload extended versions of this file to the forums. Data Section 6 file: NCAATourneySlots This file identifies the mechanism by which teams are paired against each other, depending upon their seeds, as the tournament proceeds through its rounds. It can be of use in identifying, for a given historical game, what round it occurred in, and what the seeds/slots were for the two teams (the meaning of \"slots\" is described below). Because of the existence of play-in games for particular seed numbers, the pairings have small differences from year to year. You may need to know these specifics if you are trying to represent/simulate the exact workings of the tournament bracket. Data Section 6 file: NCAATourneySeedRoundSlots.csv This file helps to represent the bracket structure in any given year. No matter where the play-in seeds are located, we can always know, for a given tournament seed, exactly what bracket slot they would be playing in, on each possible game round, and what the possible DayNum values would be for that round. Thus, if we know when a historical game was played, and what the team's seed was, we can identify the slot for that game. This can be useful in representing or simulating the tournament bracket structure."
    },
    {
        "name": "Google Cloud & NCAA\u00ae ML Competition 2018-Women's",
        "url": "https://www.kaggle.com/competitions/womens-machine-learning-competition-2018",
        "overview_text": "Overview text not found",
        "description_text": "Google Cloud and NCAA\u00ae have teamed up to bring you this year\u2019s version of the Kaggle machine learning competition. Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. Kagglers will join the millions of fans who attempt to forecast the outcomes of March Madness\u00ae during this year's NCAA Division I Men\u2019s and Women\u2019s Basketball Championships. But unlike most fans, you will pick your bracket using a combination of NCAA\u2019s historical data and your computing power, while the ground truth unfolds on national television.  In the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset. In the second stage, competitors will forecast outcomes of all possible match-ups in the 2018 NCAA Division I Men\u2019s and Women\u2019s Basketball Championships. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2018 results. This page is for the NCAA Division I Women's tournament. Check out the NCAA Division I Men's tournament here.",
        "dataset_text": "Each season there are thousands of NCAA\u00ae basketball games played between Division I women's teams, culminating in March Madness\u00ae, the 68-team national championship that starts in the middle of March. We have provided a large amount of historical data about college basketball games and teams, going back many years. Armed with this historical data, you can explore it and develop your own distinctive ways of predicting March Madness\u00ae game outcomes. You can even evaluate and compare different approaches by seeing which of them would have done best at predicting tournament games from the past. If you are unfamiliar with the format and intricacies of the tournament, we encourage reading the wikipedia page before diving into the data. The data description and schema may seem daunting at first, but is not as complicated as it appears. As a reminder, you are encouraged to incorporate your own sources of data. We have provided extensive historical data to jump-start the modeling process, and this data is self-consistent (for instance, dates and team ID's are always treated the same way). Nevertheless, you may also be able to make productive use of external data. If you head down this path, please be forewarned that many sources have their own distinctive way of identifying the names of teams, and this can make it challenging to link up with our data. The WTeamSpellings file, which is listed in the bottom section below, may help you map external team references into our own Team ID structure, and you may also need to understand exactly how dates work in our data. Please also note that we have standardized the spelling of column names and some filenames, and we have assigned a \"W\" prefix to all files pertinent to women's college basketball, so if you are re-using code from previous instances of this contest, or from the men's contest, you may need to adjust for this. We extend our gratitude to Kenneth Massey for providing much of the historical data. Special Acknowledgment to Jeff Sonas of Sonas Consulting for his support in assembling the dataset for this competition. Stage 1 - You should submit predicted probabilities for every possible matchup in the past 4 NCAA\u00ae tournaments (2014-2017). Stage 2 - You should submit predicted probabilities for every possible matchup before the 2018 tournament begins. Refer to the Timeline page for specific dates. In both stages, the sample submission will tell you which games to predict. Below we describe the format and fields of the contest data files. The data will likely be refreshed once in late February while Stage 1 of the competition is running. At the start of Stage 2, we will provide updates to these files to incorporate data from the current season. This section provides everything you need to build a simple prediction model and submit predictions. Special note about \"Season\" numbers: the college basketball season lasts from early November until the national championship tournament that starts in the middle of March. For instance, this year the first women\u2019s Division I games were played on November 10th, 2017 and the women\u2019s national championship game will be played on April 1st, 2018. Because a basketball season spans two calendar years like this, it can be confusing to refer to the year of the season. By convention, when we identify a particular season, we will reference the year that the season ends in, not the year that it starts in. So for instance, the current season will be identified in our data as the 2018 season, not the 2017 season or the 2017-18 season or the 2017-2018 season, though you may see any of these in everyday use outside of our data. Data Section 1 file: WTeams.csv This file identifies the different college teams present in the dataset. You will not see games present for all teams in all seasons, because the games listing is only for matchups where both teams are Division-I teams. There are 351 teams currently in Division-I, and an overall total of 364 teams in our team listing (each year, some teams might start being Division-I programs, and others might stop being Division-I programs). Each team has a 4 digit id number. Data Section 1 file: WSeasons.csv This file identifies the different seasons included in the historical data, along with certain season-level properties. The game dates in this dataset are expressed in relative terms, as the number of days since the start of the regular season, and aligned for each season so that day number #133 is the Monday right before the tournament, when team selections are made. During any given season, day number zero is defined to be exactly 19 weeks earlier than Selection Monday, so Day #0 is a Monday in late October or early November such that Day #132 is Selection Sunday (for the men's tournament) and Day #133 is Selection Monday (for the women's tournament). This doesn't necessarily mean that the regular season will always start exactly on Day #0; in fact, during the past decade, regular season games typically start being played on a Friday that is either Day #4 or Day #11, but further back there was more variety. Data Section 1 file: WNCAATourneySeeds.csv This file identifies the seeds for all teams in each NCAA\u00ae tournament, for all seasons of historical data. Thus, there are between 64 rows for each year, since there are no play-in games in the women's tournament. We will not know the seeds of the respective tournament teams, or even exactly which 64 teams it will be, until Selection Monday on March 12, 2018. Data Section 1 file: WRegularSeasonCompactResults.csv This file identifies the game-by-game results for many seasons of historical data, starting with the 1998 season. For each season, the file includes all games played from daynum 0 through 132. It is important to realize that the \"Regular Season\" games are simply defined to be all games played on DayNum=133 or earlier (DayNum=133 is Selection Monday). Thus a game played on or before Selection Monday will show up here whether it was a pre-season tournament, a non-conference game, a regular conference game, a conference tournament game, or whatever. Data Section 1 file: WNCAATourneyCompactResults.csv This file identifies the game-by-game NCAA\u00ae tournament results for all seasons of historical data. The data is formatted exactly like the WRegularSeasonCompactResults data. Each season you will see 63 games listed, since there are no women's play-in games. Under current tournament scheduling the men's round of 64 starts on Day #136 (Thursday) and the women's round of 64 starts on Day #137 (Friday), although from 2003-2014 the women's round of 64 started on Day #138 (Saturday). The scheduling of the women's Final Four games has also been adjusted a bit over the years. Currently the last two rounds are on days 151/153 (Friday/Sunday), just as they were from 1998-2002, whereas from 2003-2016 they were on days 153/155 (Sunday/Tuesday) instead. Although the scheduling of the men's tournament rounds has been consistent for many years, there has been more variety in the scheduling of the women's rounds. There have been four different schedules over the course of the past 20 years for the women's tournament, as follows: 2017 season and 2018 season: 2015 season and 2016 season: 2003 season through 2014 season: 1998 season through 2002 season: Data Section 1 file: WSampleSubmissionStage1.csv This file illustrates the submission file format for Stage 1. It is the simplest possible submission: a 50% winning percentage is predicted for each possible matchup. A submission file lists every possible matchup between tournament teams for one or more years. During Stage 1, you are asked to make predictions for all possible matchups from the past four NCAA\u00ae tournaments (seasons 2014, 2015, 2016, and 2017). In Stage 2, you will be asked to make predictions for all possible matchups from the current NCAA\u00ae tournament (season 2018). Since there are 64 teams in the tournament, there are 64*63/2=2,016 predictions to make for that year, so a Stage 1 submission file will have 2,016*4=8,064 data rows. Example #1: You want to make a prediction for Duke (TeamID=3181) against Arizona (TeamID=3112) in the 2005 tournament, with Duke given a 53% chance to win and Arizona given a 47% chance to win. In this case, Arizona has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Arizona's perspective (47%): 2005_3112_3181,0.47 Example #2: You want to make a prediction for Duke (TeamID=3181) against North Carolina (TeamID=3314) in the 2005 tournament, with Duke given a 51.6% chance to win and North Carolina given a 48.4% chance to win. In this case, Duke has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Duke's perspective (51.6%): 2005_3181_3314,0.516 This section provides city locations of all regular season, conference tournament, and NCAA\u00ae tournament games since the 2014-15 season Data Section 2 file: WCities.csv This file provides a master list of cities that have been locations for games played. Data Section 2 file: WGameCities.csv This file identifies all games, starting with the 2015 season, along with the city that the game was played in. Games from the regular season, the NCAA\u00ae tourney, and other post-season tournaments, are all listed together. This section contains additional supporting information, including alternative team name spellings and representations of bracket structure Data Section 3 file: WTeamSpellings.csv This file indicates alternative spellings of many team names. It is intended for use in associating external spellings against our own TeamID numbers, thereby helping to relate the external data properly with our datasets. Over the years we have identified various external spellings of different team names (as an example, for Ball State we have seen \"ball st\", and \"ball st.\", and \"ball state\", and \"ball-st\", and \"ball-state\"). Other teams have had more significant changes to their names over the years; for example, \"Texas Pan-American\" and \"Texas-Rio Grande Valley\" are actually the same school. The current list is obviously not exhaustive, and we encourage participants to identify additional mappings and upload extended versions of this file to the forums. Data Section 3 file: WNCAATourneySlots This file identifies the mechanism by which teams are paired against each other, depending upon their seeds, as the tournament proceeds through its rounds. It can be of use in identifying, for a given historical game, what round it occurred in, and what the seeds/slots were for the two teams (the meaning of \"slots\" is described below). You may need to know these specifics if you are trying to represent/simulate the exact workings of the tournament bracket."
    },
    {
        "name": "Elo Merchant Category Recommendation",
        "url": "https://www.kaggle.com/competitions/elo-merchant-category-recommendation",
        "overview_text": "Overview text not found",
        "description_text": " Imagine being hungry in an unfamiliar part of town and getting restaurant recommendations served up, based on your personal preferences, at just the right moment. The recommendation comes with an attached discount from your credit card provider for a local place around the corner! Right now, Elo, one of the largest payment brands in Brazil, has built partnerships with merchants in order to offer promotions or discounts to cardholders. But do these promotions work for either the consumer or the merchant? Do customers enjoy their experience? Do merchants see repeat business? Personalization is key. Elo has built machine learning models to understand the most important aspects and preferences in their customers\u2019 lifecycle, from food to shopping. But so far none of them is specifically tailored for an individual or profile. This is where you come in. In this competition, Kagglers will develop algorithms to identify and serve the most relevant opportunities to individuals, by uncovering signal in customer loyalty. Your input will improve customers\u2019 lives and help Elo reduce unwanted campaigns, to create the right experience for customers.",
        "dataset_text": "Note: All data is simulated and fictitious, and is not real customer data You will need, at a minimum, the train.csv and test.csv files. These contain the card_ids that we'll be using for training and prediction. The historical_transactions.csv and new_merchant_transactions.csv files contain information about each card's transactions. historical_transactions.csv contains up to 3 months' worth of transactions for every card at any of the provided merchant_ids. new_merchant_transactions.csv contains the transactions at new merchants (merchant_ids that this particular card_id has not yet visited) over a period of two months. merchants.csv contains aggregate information for each merchant_id represented in the data set. The data is formatted as follows: train.csv and test.csv contain card_ids and information about the card itself - the first month the card was active, etc. train.csv also contains the target. historical_transactions.csv and new_merchant_transactions.csv are designed to be joined with train.csv, test.csv, and merchants.csv. They contain information about transactions for each card, as described above. merchants can be joined with the transaction sets to provide additional merchant-level information. You are predicting a loyalty score for each card_id represented in test.csv and sample_submission.csv. Data field descriptions are provided in Data Dictionary.xlsx."
    },
    {
        "name": "TensorFlow 2.0 Question Answering",
        "url": "https://www.kaggle.com/competitions/tensorflow2-question-answering",
        "overview_text": "Overview text not found",
        "description_text": "\u201cWhy is the sky blue?\u201d This is a question an open-domain question answering (QA) system should be able to respond to. QA systems emulate how people look for information by reading the web to return answers to common questions. Machine learning can be used to improve the accuracy of these answers. Existing natural language models have been focused on extracting answers from a short paragraph rather than reading an entire page of content for proper context. As a result, the responses can be complicated or lengthy. A good answer will be both succinct and relevant. In this competition, your goal is to predict short and long answer responses to real questions about Wikipedia articles. The dataset is provided by Google's Natural Questions, but contains its own unique private test set. A visualization of examples shows long and\u2014where available\u2014short answers. In addition to prizes for the top teams, there is a special set of awards for using TensorFlow 2.0 APIs. If successful, this challenge will help spur the development of more effective and robust QA systems. TensorFlow is an open source platform for machine learning. With TensorFlow 2.0, tf.keras is the preferred high-level API for TensorFlow, to make model building easier and more intuitive. You may use the tf.keras built-in compile()/fit() methods, or write your own custom training loops. See the Effective TensorFlow 2.0 guide and the tf.keras guide for more details. TensorFlow 2.0 was recently released and this competition is to challenge Kagglers to use TensorFlow 2.0\u2019s APIs focused on usability, and easier, more intuitive development, to make advancements on Question Answering.",
        "dataset_text": "In this competition, we are tasked with selecting the best short and long answers from Wikipedia articles to the given questions. Each sample contains a Wikipedia article, a related question, and the candidate long form answers. The training examples also provide the correct long and short form answer or answers for the sample, if any exist. For each article + question pair, you must predict / select long and short form answers to the question drawn directly from the article. There is more detail about the data and what you're predicting on the Github page for the Natural Questions dataset. This page also contains helpful utilities and scripts. Note that we are using the simplified text version of the data - most of the HTML tags have been removed, and only those necessary to break up paragraphs / sections are included."
    },
    {
        "name": "Big Data Derby 2022",
        "url": "https://www.kaggle.com/competitions/big-data-derby-2022",
        "overview_text": "Overview text not found",
        "description_text": "The goal of this competition is to analyze horse racing tactics, drafting strategies, and path efficiency. You will develop a model using never-before-released coordinate data along with basic race information. Your work will help racing horse owners, trainers, and veterinarians better understand how equine performance and welfare fit together. With better data analysis, equine welfare could significantly improve. Injury prevention is a critical component in modern athletics. Sports that involve animals, such as horse racing, are no different than human sport. Typically, efficiency in movement correlates to both improvements in performance and injury prevention. A wealth of data is now collected, including measures for heart rate, EKG, longitudinal movement, dorsal/ventral movement, medial/lateral deviation, total power and total landing vibration. Your data science skills and analysis are needed to decipher what makes the most positive impact. In this competition, you will create a model to interpret one aspect of this new data. You\u2019ll be among the first to access X/Y coordinate mapping of horses during races. Using the data, you might analyze jockey decision making, compare race surfaces, or measure the relative importance of drafting. With considerable data, contestants can flex their creativity problem solving skills. The New York Racing Association (NYRA) and the New York Thoroughbred Horsemen's Association (NYTHA) conduct world class thoroughbred racing at Aqueduct Racetrack, Belmont Park and Saratoga Race Course. With your help, NYRA and NYTHA will better understand their vast data set, which could lead to new ways of racing and training in a highly traditional industry. With improved use of horse tracking data, you could help improve equine welfare, performance and rider decision making.",
        "dataset_text": "nyra_start_table.csv nyra_race_table.csv nyra_tracking_table.csv nyra_2019_complete.csv - This file is the combined 3 files into one table. The keys to join them trakus with race - track_id, race_date, race_number. To join trakus with start - track_id, race_date, race_number, program_number."
    },
    {
        "name": "NeurIPS 2023 - Machine Unlearning",
        "url": "https://www.kaggle.com/competitions/neurips-2023-machine-unlearning",
        "overview_text": "Deep learning has recently driven tremendous progress in a wide array of applications, ranging from realistic image generation and impressive retrieval systems to language models that can hold human-like conversations. While this progress is very exciting, the widespread use of deep neural network models requires caution: researchers should seek to develop AI technologies responsibly by understanding and mitigating potential risks, such as the propagation and amplification of unfair biases and protecting user privacy.",
        "description_text": "Fully erasing the influence of the data requested to be deleted is challenging since, aside from simply deleting it from databases where it\u2019s stored, it also requires erasing the influence of that data on other artifacts such as trained machine learning models. Moreover, recent research [1, 2] has shown that in some cases it may be possible to infer with high accuracy whether an example was used to train a machine learning model using membership inference attacks (MIAs). This can raise privacy concerns, as it implies that even if an individual's data is deleted from a database, it may still be possible to infer whether that individual's data was used to train a model. Given the above, machine unlearning is an emergent subfield of machine learning that aims to remove the influence of a specific subset of training examples \u2014 the \"forget set\" \u2014 from a trained model. Furthermore, an ideal unlearning algorithm would remove the influence of certain examples while maintaining other beneficial properties, such as the accuracy on the rest of the train set and generalization to held-out examples. A straightforward way to produce this unlearned model is to retrain the model on an adjusted training set that excludes the samples from the forget set. However, this is not always a viable option, as retraining deep models can be computationally expensive. An ideal unlearning algorithm would instead use the already-trained model as a starting point and efficiently make adjustments to remove the influence of the requested data. The competition considers a realistic scenario in which an age predictor has been trained on face images, and, after training, a certain subset of the training images must be forgotten to protect the privacy or rights of the individuals concerned. Your goal is to submit code that takes as input the trained predictor, the forget and retain sets, and outputs the weights of a predictor that has unlearned the designated forget set. We will evaluate submissions based on both the strength of the forgetting algorithm and model utility. We also enforce a hard cut-off that rejects unlearning algorithms that run slower than a fraction of the time it takes to retrain. A valuable outcome of this competition will be to characterize the trade-offs of different unlearning algorithms.  The starting kit provides an example of unlearning algorithms for participants to build their unlearning models upon. Note that the starting kit uses the CIFAR10 dataset, while submissions will be scored using a different (secret) dataset. You may also want to review this example submission notebook.",
        "dataset_text": "Removing examples from a trained machine learning model is a major unsolved problem of ML privacy research. This competition challenges you to cause a simple model to forget images from several individuals. This is a code competition with a hidden dataset which is completely unavailable for download. We highly recommend reviewing this example notebook for a demonstration of how to generate a valid submission. [retain/forget/validation].csv The data used to train and validate the original model checkpoint, with the original train set split into ids that need to be retained and images from 15 subjects that must be unlearned. Unlike most competitions, the public and private scores are generated using the same dataset with different splits and evaluated across separate notebook runs. images/[person_id]/[image_id].png Images of people's faces. If the image ID is 'abc-1' the path is images/abc/1.png. All of the images have been resized to 32x32 pixels. Expect approximately 30,000 images in the hidden dataset. Note that roughly 2% of the images in the dataset have identical perceptual hashes; this is a known limitation of the problem setup. age_class_weights.json The age_group weights used to train the original model. Loading this file yields a dict of the form {age_group: n_occurrences}. original_model.pth The original Resnet-18 Pytorch model checkpoint that you will need to cause to forget certain images. The model was trained to predict the age group associated with each image of a person\u2019s face. It was trained with class weights to mitigate class imbalance and no data augmentation. The original model obtains 99% accuracy on the training set and 96% on the test set and was trained with torch 1.13.1. However, these performance figures come with a substantial caveat: the train/test split was done using the image ID rather than the subject ID. All subjects in the test set also exist in the train set. For a classification task this would be considered leakage but may have different implications in an unlearning task. As the retain/forget splits were done by subject ID, it's possible this approach may exaggerate the impact of unlearning any single example on the underlying model. This is a known limitation of the problem setup and competitors should understand this as one of the risks of the competition."
    },
    {
        "name": "Google \u2013 AI Assistants for Data Tasks with Gemma",
        "url": "https://www.kaggle.com/competitions/data-assistants-with-gemma",
        "overview_text": "Google recently launched Gemma, a new family of open LLMs built from the same research and technology used to create their Gemini models. In this competition, you\u2019re challenged to demonstrate how to use Gemma to accomplish one or more data science oriented tasks.",
        "description_text": "Large Language Models (LLMs) have captured the world's attention and imagination. Often referred to as \"foundation models\", much of their potential lies in their ability to be adapted to accomplish specialized tasks for a seemingly unlimited number of use cases. As the technology rapidly develops, there\u2019s a massive opportunity to uncover the best methods and approaches for adapting LLMs to new and specialized use cases. The goal of this competition is to create a notebook that demonstrates how to use the Gemma LLM to accomplish one of the following data science oriented tasks: By participating in this competition, you'll both be building a useful tool and contributing to the ML field's collective knowledge of how to best sharpen LLMs for real-world needs.",
        "dataset_text": "This competition does not require use of a dataset. The submission_instructions.txt and submission_categories.txt files are included below for reference."
    },
    {
        "name": "Google Analytics Customer Revenue Prediction",
        "url": "https://www.kaggle.com/competitions/ga-customer-revenue-prediction",
        "overview_text": "Overview text not found",
        "description_text": "The 80/20 rule has proven true for many businesses\u2013only a small percentage of customers produce most of the revenue. As such, marketing teams are challenged to make appropriate investments in promotional strategies.  RStudio, the developer of free and open tools for R and enterprise-ready products for teams to scale and share work, has partnered with Google Cloud and Kaggle to demonstrate the business impact that thorough data analysis can have. In this competition, you\u2019re challenged to analyze a Google Merchandise Store (also known as GStore, where Google swag is sold) customer dataset to predict revenue per customer. Hopefully, the outcome will be more actionable operational changes and a better use of marketing budgets for those companies who choose to use data analysis on top of GA data.",
        "dataset_text": "We have now updated the data to work with the new forward-looking problem formulation. Note that in this competition you will be predicting the target for ALL users in the posted test set: test_v2.csv, for their transactions in the future time period of December 1st 2018 through January 31st 2019. You will need to download train_v2.csv and test_v2.csv. These contain the data necessary to make predictions for each fullVisitorId listed in sample_submission_v2.csv. Unfortunately, due to time constraints, the BigQuery version of this data will not be made available immediately. Both train_v2.csv and test_v2.csv contain the columns listed under Data Fields. Each row in the dataset is one visit to the store. Because we are predicting the log of the total revenue per user, be aware that not all rows in test_v2.csv will correspond to a row in the submission, but all unique fullVisitorIds will correspond to a row in the submission. There are multiple columns which contain JSON blobs of varying depth. In one of those JSON columns, totals, the sub-column transactionRevenue contains the revenue information we are trying to predict. This sub-column exists only for the training data. We are predicting the natural log of the sum of all transactions per user. Once the data is updated, as noted above, this will be for all users in test_v2.csv for December 1st, 2018 to January 31st, 2019. For every user in the test set, the target is:\n\ud835\udc66\n\ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc5f\n=\n\u2211\n\ud835\udc56=1\n\ud835\udc5b\n\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc60\ud835\udc4e\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\n\ud835\udc5b\n\ud835\udc62\ud835\udc60\ud835\udc52\n\ud835\udc5f\n\ud835\udc56\nn\n\n\ud835\udc61\ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc52\n\ud835\udc61\n\ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc5f\n=ln(\n\ud835\udc66\n\ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc5f\n+1)\nln\n(\n) Note that the dataset does NOT contain data for December 1st 2018 to January 31st 2019. You must identify the unique fullVisitorIds in the provided test_v2.csv and make predictions for them for those unseen months. sample_submission_v2.csv is composed of all fullVisitorIds for the 5/1/18 to 10/15/18 time period. The public leaderboard submission for this file is based on a separate timeframe than the private leaderboard. The Public LB is being calculated for those visitors during the same timeframe of 5/1/18 to 10/15/18. This is all publicly-available for the target prediction, but intended to provide some means of signal for those who wish to use it in that way. The Private LB is being calculated on the future-looking timeframe of 12/1/18 to 1/31/19 - for those same set of users. Therefore, your submission that is intended for the public LB timeframe will be different from the private LB timeframe, which will be rescored/recalculated on the future timeframe. Knowing this, competitors should be making explicit final submission selections for those submissions which represent what they expect those future-looking predictions to be. These final submission selections for which this competition permits 2, per the rules, can be made under \"My Submissions\" by the final submission deadline. External data is permitted for this competition, per this forum post. This includes the Google Merchandise Store Demo Account. Although the Demo Account contains the predicted variable, final standings will not benefit from access to this external data, because it requires future-looking predictions."
    },
    {
        "name": "Merck Molecular Activity Challenge",
        "url": "https://www.kaggle.com/competitions/MerckActivity",
        "overview_text": "Overview text not found",
        "description_text": "Help enable the development of safe, effective medicines. When developing new medicines it is important to identify molecules that are highly active toward their intended targets but not toward other targets that might cause side effects. The objective of this competition is to identify the best statistical techniques for predicting biological activities of different molecules, both on- and off-target, given numerical descriptors generated from their chemical structures. The challenge is based on 15 molecular activity data sets, each for a biologically relevant target. Each row corresponds to a molecule and contains descriptors derived from that molecule's chemical structure. In addition to the prediction competition, Merck is also hosting a visualization challenge with a $2,000 prize for the most insightful and elegant graphical representations of the data. Prizes total $40,000.",
        "dataset_text": "The Training and Test Sets each consist of 15 biological activity data sets in comma separated value (CSV) format. Each row of data corresponds to a chemical structure represented by molecular descriptors. Training and Test Files The training files are of the form The test files are in the same format with Column 2 removed. Molecule IDs and descriptor names are global to all data sets. Thus some molecules will appear in multiple data sets, as will some descriptors. The challenge is to predict the activity value for each molecule/data set combination in the test set. To keep predictions for molecules unique to each data set, a data set identifier has been prepended to each molecule ID (e.g., \"ACT1_\" or \"ACT8_\"). Data Set Creation For each activity, the training/test set split is done by dates of testing.  That is, the training set consists of compounds assayed by a certain date, and the test set consists of compounds tested after that date. Therefore it is expected that the distribution of descriptors will not necessarily be the same between the training and test sets. Additional Files Also provided is starter code in R for reading the data sets and producing the naive random forest benchmark, and R code for calculating the R-squared metric. The benchmark result is provided as an example submission file."
    },
    {
        "name": "West Nile Virus Prediction",
        "url": "https://www.kaggle.com/competitions/predict-west-nile-virus",
        "overview_text": "Overview text not found",
        "description_text": "West Nile virus is most commonly spread to humans through infected mosquitos. Around 20% of people who become infected with the virus develop symptoms ranging from a persistent fever, to serious neurological illnesses that can result in death.  In 2002, the first human cases of West Nile virus were reported in Chicago. By 2004 the City of Chicago and the Chicago Department of Public Health (CDPH) had established a comprehensive surveillance and control program that is still in effect today. Every week from late spring through the fall, mosquitos in traps across the city are tested for the virus. The results of these tests influence when and where the city will spray airborne pesticides to control adult mosquito populations. Given weather, location, testing, and spraying data, this competition asks you to predict when and where different species of mosquitos will test positive for West Nile virus. A more accurate method of predicting outbreaks of West Nile virus in mosquitos will help the City of Chicago and CPHD more efficiently and effectively allocate resources towards preventing transmission of this potentially deadly virus.  We've jump-started your analysis with some visualizations and starter code in R and Python on Kaggle Scripts. No data download or local environment setup needed!  This competition is sponsored by the Robert Wood Johnson Foundation. Data is provided by the Chicago Department of Public Health.",
        "dataset_text": "Ready to explore the data? Kaggle Scripts is the most frictionless way to get familiar with the competition dataset! No data download needed to start publishing and forking code in R and Python. It's already pre-loaded with our favorite packages and ready for you to start competing! In this competition, you will be analyzing weather data and GIS data and predicting whether or not West Nile virus is present, for a given time, location, and species.  Every year from late-May to early-October, public health workers in Chicago setup mosquito traps scattered across the city. Every week from Monday through Wednesday, these traps collect mosquitos, and the mosquitos are tested for the presence of West Nile virus before the end of the week. The test results include the number of mosquitos, the mosquitos species, and whether or not West Nile virus is present in the cohort.  Main dataset These test results are organized in such a way that when the number of mosquitos exceed 50, they are split into another record (another row in the dataset), such that the number of mosquitos are capped at 50.  The location of the traps are described by the block number and street name. For your convenience, we have mapped these attributes into Longitude and Latitude in the dataset. Please note that these are derived locations. For example, Block=79, and Street= \"W FOSTER AVE\" gives us an approximate address of \"7900 W FOSTER AVE, Chicago, IL\", which translates to (41.974089,-87.824812) on the map. Some traps are \"satellite traps\". These are traps that are set up near (usually within 6 blocks) an established trap to enhance surveillance efforts. Satellite traps are postfixed with letters. For example, T220A is a satellite trap to T220.  Please note that not all the locations are tested at all times. Also, records exist only when a particular species of mosquitos is found at a certain trap at a certain time. In the test set, we ask you for all combinations/permutations of possible predictions and are only scoring the observed ones. Spray Data The City of Chicago also does spraying to kill mosquitos. You are given the GIS data for their spray efforts in 2011 and 2013. Spraying can reduce the number of mosquitos in the area, and therefore might eliminate the appearance of West Nile virus.   Weather Data It is believed that hot and dry conditions are more favorable for West Nile virus than cold and wet. We provide you with the dataset from NOAA of the weather conditions of 2007 to 2014, during the months of the tests.  Station 1: CHICAGO O'HARE INTERNATIONAL AIRPORT Lat: 41.995 Lon: -87.933 Elev: 662 ft. above sea level\nStation 2: CHICAGO MIDWAY INTL ARPT Lat: 41.786 Lon: -87.752 Elev: 612 ft. above sea level Map Data The map files mapdata_copyright_openstreetmap_contributors.rds and mapdata_copyright_openstreetmap_contributors.txt are from Open Streetmap and are primarily provided for use in visualizations (but you are allowed to use them in your models if you wish). Here's an example using mapdata_copyright_openstreetmap_contributors.rds, and here's one using mapdata_copyright_openstreetmap_contributors.txt."
    },
    {
        "name": "Home Depot Product Search Relevance",
        "url": "https://www.kaggle.com/competitions/home-depot-product-search-relevance",
        "overview_text": "Overview text not found",
        "description_text": " Shoppers rely on Home Depot\u2019s product authority to find and buy the latest products and to get timely solutions to their home improvement needs. From installing a new ceiling fan to remodeling an entire kitchen, with the click of a mouse or tap of the screen, customers expect the correct results to their queries \u2013 quickly. Speed, accuracy and delivering a frictionless customer experience are essential. In this competition, Home Depot is asking Kagglers to help them improve their customers' shopping experience by developing a model that can accurately predict the relevance of search results. Search relevancy is an implicit measure Home Depot uses to gauge how quickly they can get customers to the right products. Currently, human raters evaluate the impact of potential changes to their search algorithms, which is a slow and subjective process. By removing or minimizing human input in search relevance evaluation, Home Depot hopes to increase the number of iterations their team can perform on the current search algorithms.",
        "dataset_text": "This data set contains a number of products and real customer search terms from Home Depot's website. The challenge is to predict a relevance score for the provided combinations of search terms and products. To create the ground truth labels, Home Depot has crowdsourced the search/product pairs to multiple human raters. The relevance is a number between 1 (not relevant) to 3 (highly relevant). For example, a search for \"AA battery\" would be considered highly relevant to a pack of size AA batteries (relevance = 3), mildly relevant to a cordless drill battery (relevance = 2), and not relevant to a snow shovel (relevance = 1). Each pair was evaluated by at least three human raters. The provided relevance scores are the average value of the ratings. There are three additional things to know about the ratings: Your task is to predict the relevance for each pair listed in the test set. Note that the test set contains both seen and unseen search terms."
    },
    {
        "name": "Human Protein Atlas Image Classification",
        "url": "https://www.kaggle.com/competitions/human-protein-atlas-image-classification",
        "overview_text": "Overview text not found",
        "description_text": " In this competition, Kagglers will develop models capable of classifying mixed patterns of proteins in microscope images. The Human Protein Atlas will use these models to build a tool integrated with their smart-microscopy system to identify a protein's location(s) from a high-throughput image. Proteins are \u201cthe doers\u201d in the human cell, executing many functions that together enable life. Historically, classification of proteins has been limited to single patterns in one or a few cell types, but in order to fully understand the complexity of the human cell, models must classify mixed patterns across a range of different human cells. Images visualizing proteins in cells are commonly used for biomedical research, and these cells could hold the key for the next breakthrough in medicine. However, thanks to advances in high-throughput microscopy, these images are generated at a far greater pace than what can be manually evaluated. Therefore, the need is greater than ever for automating biomedical image analysis to accelerate the understanding of human cells and disease.   Nature Methods has indicated interest in considering a paper discussing the outcome and approaches of the challenge. The Human Protein Atlas team would like to invite top performing teams to join as co-authors in the writing of this paper. Top performing teams will also be eligible to compete for the special prize. Additional information for both the special prize and co-authoring for Nature Methods will become available through the Discussion posts once the main competition is complete.     The Human Protein Atlas is a Sweden-based initiative aimed at mapping all human proteins in cells, tissues and organs. All the data in the knowledge resource is open access to allow anyone to pursue exploration of the human proteome. In a recent publication, the Human Protein Atlas team has demonstrated the promise of both citizen science and artificial intelligence approaches in describing the location of human proteins in images, however current results are yet to approach expert-level annotations (Sullivan et al, Nature Biotechnology, Oct 2018).",
        "dataset_text": "You will need to download a copy of the images. Due to size, we have provided two versions of the same images. On the data page below, you will find a scaled set of 512x512 PNG files in train.zip and test.zip. Alternatively, if you wish to work with full size original images (a mix of 2048x2048 and 3072x3072 TIFF files) you may download train_full_size.7z and test_full_size.7z from here (warning: these are ~250 GB total). You will also need the training labels from train.csv and the filenames for the test set from sample_submission.csv. The data format is two-fold - first, the labels are provided for each sample in train.csv. The bulk of the data is in the images - train.zip and test.zip. Within each of these is a folder containing four files per sample. Each file represents a different filter on the subcellular protein patterns represented by the sample. The format should be [filename]_[filter color].png for the PNG files, and [filename]_[filter color].tif for the TIFF files. You are predicting protein organelle localization labels for each sample. There are in total 28 different labels present in the dataset. The dataset is acquired in a highly standardized way using one imaging modality (confocal microscopy). However, the dataset comprises 27 different cell types of highly different morphology, which affect the protein patterns of the different organelles. All image samples are represented by four filters (stored as individual files), the protein of interest (green) plus three cellular landmarks: nucleus (blue), microtubules (red), endoplasmic reticulum (yellow). The green filter should hence be used to predict the label, and the other filters are used as references. The labels are represented as integers that map to the following:"
    },
    {
        "name": "Rossmann Store Sales",
        "url": "https://www.kaggle.com/competitions/rossmann-store-sales",
        "overview_text": "Overview text not found",
        "description_text": "Rossmann operates over 3,000 drug stores in 7 European countries. Currently, Rossmann store managers are tasked with predicting their daily sales for up to six weeks in advance. Store sales are influenced by many factors, including promotions, competition, school and state holidays, seasonality, and locality. With thousands of individual managers predicting sales based on their unique circumstances, the accuracy of results can be quite varied. In their first Kaggle competition, Rossmann is challenging you to predict 6 weeks of daily sales for 1,115 stores located across Germany. Reliable sales forecasts enable store managers to create effective staff schedules that increase productivity and motivation. By helping Rossmann create a robust prediction model, you will help store managers stay focused on what\u2019s most important to them: their customers and their teams!   ",
        "dataset_text": "You are provided with historical sales data for 1,115 Rossmann stores. The task is to forecast the \"Sales\" column for the test set. Note that some stores in the dataset were temporarily closed for refurbishment. Most of the fields are self-explanatory. The following are descriptions for those that aren't."
    },
    {
        "name": "Cdiscount\u2019s Image Classification Challenge",
        "url": "https://www.kaggle.com/competitions/cdiscount-image-classification-challenge",
        "overview_text": "Overview text not found",
        "description_text": "Rules Update: The CDiscount team has updated their rules to allow for use of this dataset for research and academic purposes only. To access the data, go to rules and accept the terms to download the data. Cdiscount.com generated nearly 3 billion euros last year, making it France\u2019s largest non-food e-commerce company. While the company already sells everything from TVs to trampolines, the list of products is still rapidly growing. By the end of this year, Cdiscount.com will have over 30 million products up for sale. This is up from 10 million products only 2 years ago. Ensuring that so many products are well classified is a challenging task. Currently, Cdiscount.com applies machine learning algorithms to the text description of the products in order to automatically predict their category. As these methods now seem close to their maximum potential, Cdiscount.com believes that the next quantitative improvement will be driven by the application of data science techniques to images. In this challenge you will be building a model that automatically classifies the products based on their images. As a quick tour of Cdiscount.com's website can confirm, one product can have one or several images. The data set Cdiscount.com is making available is unique and characterized by superlative numbers in several ways:",
        "dataset_text": "At the request of the sponsor, the data has been removed post-competition. BSON, short for Bin\u00adary JSON, is a bin\u00adary-en\u00adcoded seri\u00adal\u00adiz\u00ada\u00adtion of JSON-like doc\u00adu\u00adments, used with MongoDB. This kernel shows how to read and process the BSON files for this competition. Please Note: The train and test files are very large!"
    },
    {
        "name": "Toxic Comment Classification Challenge",
        "url": "https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge",
        "overview_text": "Overview text not found",
        "description_text": " Discussing things you care about can be difficult. The threat of abuse and harassment online means that many people stop expressing themselves and give up on seeking different opinions. Platforms struggle to effectively facilitate conversations, leading many communities to limit or completely shut down user comments. The Conversation AI team, a research initiative founded by Jigsaw and Google (both a part of Alphabet) are working on tools to help improve online conversation. One area of focus is the study of negative online behaviors, like toxic comments (i.e. comments that are rude, disrespectful or otherwise likely to make someone leave a discussion). So far they\u2019ve built a range of publicly available models served through the Perspective API, including toxicity. But the current models still make errors, and they don\u2019t allow users to select which types of toxicity they\u2019re interested in finding (e.g. some platforms may be fine with profanity, but not with other types of toxic content). In this competition, you\u2019re challenged to build a multi-headed model that\u2019s capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate better than Perspective\u2019s current models. You\u2019ll be using a dataset of comments from Wikipedia\u2019s talk page edits. Improvements to the current model will hopefully help online discussion become more productive and respectful. Disclaimer: the dataset for this competition contains text that may be considered profane, vulgar, or offensive.",
        "dataset_text": "You are provided with a large number of Wikipedia comments which have been labeled by human raters for toxic behavior. The types of toxicity are: You must create a model which predicts a probability of each type of toxicity for each comment. The dataset under CC0, with the underlying comment text being governed by Wikipedia's CC-SA-3.0"
    },
    {
        "name": "Acquire Valued Shoppers Challenge",
        "url": "https://www.kaggle.com/competitions/acquire-valued-shoppers-challenge",
        "overview_text": "Overview text not found",
        "description_text": "Consumer brands often offer discounts to attract new shoppers to buy their products. The most valuable customers are those who return after this initial incented purchase.  With enough purchase history, it is possible to predict which shoppers, when presented an offer, will buy a new item. However, identifying the shopper who will become a loyal buyer -- prior to the initial purchase -- is a more challenging task.  The Acquire Valued Shoppers Challenge asks participants to predict which shoppers are most likely to repeat purchase. To aid with algorithmic development, we have provided complete, basket-level, pre-offer shopping history for a large set of shoppers who were targeted for an acquisition campaign. The incentive offered to that shopper and their post-incentive behavior is also provided. This challenge provides almost 350 million rows of completely anonymised transactional data from over 300,000 shoppers. It is one of the largest problems run on Kaggle to date.",
        "dataset_text": "Warning: this is a large data set. The decompressed files require about 22GB of space. This data captures the process of offering incentives (a.k.a. coupons) to a large number of customers and forecasting those who will become loyal to the product. Let's say 100 customers are offered a discount to purchase two bottles of water. Of the 100 customers, 60 choose to redeem the offer. These 60 customers are the focus of this competition. You are asked to predict which of the 60 will return (during or after the promotional period) to purchase the same item again. To create this prediction, you are given a minimum of a year of shopping history prior to each customer's incentive, as well as the purchase histories of many other shoppers (some of whom will have received the same offer). The transaction history contains all items purchased, not just items related to the offer. Only one offer per customer is included in the data. The training set is comprised of offers issued before 2013-05-01. The test set is offers issued on or after 2013-05-01. You are provided four relational files: All of the fields are anonymized and categorized to protect customer and sales information. The specific meanings of the fields will not be provided (so don't bother asking). Part of the challenge of this competition is learning the taxonomy of items in a data-driven way. history\nid - A unique id representing a customer\nchain - An integer representing a store chain\noffer - An id representing a certain offer\nmarket - An id representing a geographical region\nrepeattrips - The number of times the customer made a repeat purchase\nrepeater - A boolean, equal to repeattrips > 0\nofferdate - The date a customer received the offer transactions\nid - see above\nchain - see above\ndept - An aggregate grouping of the Category (e.g. water)\ncategory - The product category (e.g. sparkling water)\ncompany - An id of the company that sells the item\nbrand - An id of the brand to which the item belongs\ndate - The date of purchase\nproductsize - The amount of the product purchase (e.g. 16 oz of water)\nproductmeasure - The units of the product purchase (e.g. ounces)\npurchasequantity - The number of units purchased\npurchaseamount - The dollar amount of the purchase offers\noffer - see above\ncategory - see above\nquantity - The number of units one must purchase to get the discount\ncompany - see above\noffervalue - The dollar value of the offer\nbrand - see above The transactions file can be joined to the history file by (id,chain). The history file can be joined to the offers file by (offer). The transactions file can be joined to the offers file by (category, brand, company). A negative value in productquantity and purchaseamount indicates a return."
    },
    {
        "name": "Driver Telematics Analysis",
        "url": "https://www.kaggle.com/competitions/axa-driver-telematics-analysis",
        "overview_text": "Overview text not found",
        "description_text": " For automobile insurers, telematics represents a growing and valuable way to quantify driver risk. Instead of pricing decisions on vehicle and driver characteristics, telematics gives the opportunity to measure the quantity and quality of a driver's behavior. This can lead to savings for safe or infrequent drivers, and transition the burden to policies that represent increased liability. AXA has provided a dataset of over 50,000 anonymized driver trips. The intent of this competition is to develop an algorithmic signature of driving type. Does a driver drive long trips? Short trips? Highway trips? Back roads? Do they accelerate hard from stops? Do they take turns at high speed? The answers to these questions combine to form an aggregate profile that potentially makes each driver unique. For this competition, Kaggle participants must come up with a \"telematic fingerprint\" capable of distinguishing when a trip was driven by a given driver. The features of this driver fingerprint could help assess risk and form a crucial piece of a larger telematics puzzle.",
        "dataset_text": "Update: this dataset has been removed at the request of the host. You are provided a directory containing a number of folders. Each folder represents a driver. Within each folder are 200 .csv files. Each file represents a driving trip. The trips are recordings of the car's position (in meters) every second and look like the following: In order to protect the privacy of the drivers' location, the trips were centered to start at the origin (0,0), randomly rotated, and short lengths of trip data were removed from the start/end of the trip. A small and random number of false trips (trips that were not driven by the driver of interest) are planted in each driver's folder. These false trips are sourced from drivers not included in the competition data, in order to prevent similarity analysis between the included drivers. You are not given the number of false trips (it varies), nor a labeled training set of true positive trips. You can safely make the assumption that the majority of the trips in each folder do belong to the same driver. The challenge of this competition is to identify trips which are not from the driver of interest, based on their telematic features. You must predict a probability that each trip was taken by the driver of interest."
    },
    {
        "name": "Restaurant Revenue Prediction",
        "url": "https://www.kaggle.com/competitions/restaurant-revenue-prediction",
        "overview_text": "Overview text not found",
        "description_text": " With over 1,200 quick service restaurants across the globe, TFI is the company behind some of the world's most well-known brands: Burger King, Sbarro, Popeyes, Usta Donerci, and Arby\u2019s. They employ over 20,000 people in Europe and Asia and make significant daily investments in developing new restaurant sites. Right now, deciding when and where to open new restaurants is largely a subjective process based on the personal judgement and experience of development teams. This subjective data is difficult to accurately extrapolate across geographies and cultures.  New restaurant sites take large investments of time and capital to get up and running. When the wrong location for a restaurant brand is chosen, the site closes within 18 months and operating losses are incurred.  Finding a mathematical model to increase the effectiveness of investments in new restaurant sites would allow TFI to invest more in other important business areas, like sustainability, innovation, and training for new employees. Using demographic, real estate, and commercial data, this competition challenges you to predict the annual restaurant sales of 100,000 regional locations. TFI would love to hire an expert Kaggler like you to head up their growing data science team in Istanbul or Shanghai. You'd be tackling problems like the one featured in this competition on a global scale. See the job description here >>",
        "dataset_text": "TFI has provided a dataset with 137 restaurants in the training set, and a test set of 100000 restaurants. The data columns include the open date, location, city type, and three categories of obfuscated data: Demographic data, Real estate data, and Commercial data. The revenue column indicates a (transformed) revenue of the restaurant in a given year and is the target of predictive analysis. "
    },
    {
        "name": "Machinery Tube Pricing",
        "url": "https://www.kaggle.com/competitions/machinery-tube-pricing",
        "overview_text": "Overview text not found",
        "description_text": "Construction machines rely on a complex set of tubes to keep the forklift lifting, the loader loading, and the bulldozer from dozing off. Tubes can vary across a number of dimensions, including base materials, number of bends, bend radius, bolt patterns, and end types. Tubes come from a variety manufacturers, each having their own unique pricing model. This competition provides detailed tube, component, and annual volume datasets, and challenges you to predict the price a supplier will quote for a given tube assembly.",
        "dataset_text": "Dataset description not found"
    },
    {
        "name": "Prudential Life Insurance Assessment",
        "url": "https://www.kaggle.com/competitions/prudential-life-insurance-assessment",
        "overview_text": "Overview text not found",
        "description_text": "Picture this. You are a data scientist in a start-up culture with the potential to have a very large impact on the business. Oh, and you are backed up by a company with 140 years' business experience. Curious? Great! You are the kind of person we are looking for. Prudential, one of the largest issuers of life insurance in the USA, is hiring passionate data scientists to join a newly-formed Data Science group solving complex challenges and identifying opportunities. The results have been impressive so far but we want more.  In a one-click shopping world with on-demand\neverything, the life insurance application process is antiquated. Customers provide extensive information to identify risk classification and eligibility, including scheduling medical exams, a process that takes an average of 30 days. The result? People are turned off. That\u2019s why only 40% of U.S. households own individual life insurance. Prudential wants to make it quicker and less labor intensive for new and existing customers to get a quote while maintaining privacy boundaries. By developing a predictive model that accurately classifies risk using a more automated approach, you can greatly impact public perception of the industry. The results will help Prudential better understand the predictive power of the data points in the existing assessment, enabling us to significantly streamline the process.",
        "dataset_text": "In this dataset, you are provided over a hundred variables describing attributes of life insurance applicants. The task is to predict the \"Response\" variable for each Id in the test set. \"Response\" is an ordinal measure of risk that has 8 levels. The following variables are all categorical (nominal): Product_Info_1, Product_Info_2, Product_Info_3, Product_Info_5, Product_Info_6, Product_Info_7, Employment_Info_2, Employment_Info_3, Employment_Info_5, InsuredInfo_1, InsuredInfo_2, InsuredInfo_3, InsuredInfo_4, InsuredInfo_5, InsuredInfo_6, InsuredInfo_7, Insurance_History_1, Insurance_History_2, Insurance_History_3, Insurance_History_4, Insurance_History_7, Insurance_History_8, Insurance_History_9, Family_Hist_1, Medical_History_2, Medical_History_3, Medical_History_4, Medical_History_5, Medical_History_6, Medical_History_7, Medical_History_8, Medical_History_9, Medical_History_11, Medical_History_12, Medical_History_13, Medical_History_14, Medical_History_16, Medical_History_17, Medical_History_18, Medical_History_19, Medical_History_20, Medical_History_21, Medical_History_22, Medical_History_23, Medical_History_25, Medical_History_26, Medical_History_27, Medical_History_28, Medical_History_29, Medical_History_30, Medical_History_31, Medical_History_33, Medical_History_34, Medical_History_35, Medical_History_36, Medical_History_37, Medical_History_38, Medical_History_39, Medical_History_40, Medical_History_41 The following variables are continuous: Product_Info_4, Ins_Age, Ht, Wt, BMI, Employment_Info_1, Employment_Info_4, Employment_Info_6, Insurance_History_5, Family_Hist_2, Family_Hist_3, Family_Hist_4, Family_Hist_5 The following variables are discrete: Medical_History_1, Medical_History_10, Medical_History_15, Medical_History_24, Medical_History_32 Medical_Keyword_1-48 are dummy variables."
    },
    {
        "name": "BNP Paribas Cardif Claims Management",
        "url": "https://www.kaggle.com/competitions/bnp-paribas-cardif-claims-management",
        "overview_text": "Overview text not found",
        "description_text": "As a global specialist in personal insurance, BNP Paribas Cardif serves 90 million clients in 36 countries across Europe, Asia and Latin America. In a world shaped by the emergence of new uses and lifestyles, everything is going faster and faster. When facing unexpected events, customers expect their insurer to support them as soon as possible. However, claims management may require different levels of check before a claim can be approved and a payment can be made. With the new practices and behaviors generated by the digital economy, this process needs adaptation thanks to data science to meet the new needs and expectations of customers.  In this challenge, BNP Paribas Cardif is providing an anonymized database with two categories of claims: Kagglers are challenged to predict the category of a claim based on features available early in the process, helping BNP Paribas Cardif accelerate its claims process and therefore provide a better service to its customers.",
        "dataset_text": "You are provided with an anonymized dataset containing both categorical and numeric variables available when the claims were received by BNP Paribas Cardif. All string type variables are categorical. There are no ordinal variables. The \"target\" column in the train set is the variable to predict. It is equal to 1 for claims suitable for an accelerated approval. The task is to predict a probability (\"PredictedProb\") for each claim in the test set."
    },
    {
        "name": "Bosch Production Line Performance",
        "url": "https://www.kaggle.com/competitions/bosch-production-line-performance",
        "overview_text": "Overview text not found",
        "description_text": "A good chocolate souffl\u00e9 is decadent, delicious, and delicate. But, it's a challenge to prepare. When you pull a disappointingly deflated dessert out of the oven, you instinctively retrace your steps to identify at what point you went wrong. Bosch, one of the world's leading manufacturing companies, has an imperative to ensure that the recipes for the production of its advanced mechanical components are of the highest quality and safety standards. Part of doing so is closely monitoring its parts as they progress through the manufacturing processes.  Because Bosch records data at every step along its assembly lines, they have the ability to apply advanced analytics to improve these manufacturing processes. However, the intricacies of the data and complexities of the production line pose problems for current methods. In this competition, Bosch is challenging Kagglers to predict internal failures using thousands of measurements and tests made for each component along the assembly line. This would enable Bosch to bring quality products at lower costs to the end user.",
        "dataset_text": "The data for this competition represents measurements of parts as they move through Bosch's production lines. Each part has a unique Id. The goal is to predict which parts will fail quality control (represented by a 'Response' = 1). The dataset contains an extremely large number of anonymized features. Features are named according to a convention that tells you the production line, the station on the line, and a feature number. E.g. L3_S36_F3939 is a feature measured on line 3, station 36, and is feature number 3939. On account of the large size of the dataset, we have separated the files by the type of feature they contain: numerical, categorical, and finally, a file with date features. The date features provide a timestamp for when each measurement was taken. Each date column ends in a number that corresponds to the previous feature number. E.g. the value of L0_S0_D1 is the time at which L0_S0_F0 was taken. In addition to being one of the largest datasets (in terms of number of features) ever hosted on Kaggle, the ground truth for this competition is highly imbalanced. Together, these two attributes are expected to make this a challenging problem."
    },
    {
        "name": "Corporaci\u00f3n Favorita Grocery Sales Forecasting",
        "url": "https://www.kaggle.com/competitions/favorita-grocery-sales-forecasting",
        "overview_text": "Overview text not found",
        "description_text": "Brick-and-mortar grocery stores are always in a delicate dance with purchasing and sales forecasting. Predict a little over, and grocers are stuck with overstocked, perishable goods. Guess a little under, and popular items quickly sell out, leaving money on the table and customers fuming. The problem becomes more complex as retailers add new locations with unique needs, new products, ever transitioning seasonal tastes, and unpredictable product marketing. Corporaci\u00f3n Favorita, a large Ecuadorian-based grocery retailer, knows this all too well. They operate hundreds of supermarkets, with over 200,000 different products on their shelves. Corporaci\u00f3n Favorita has challenged the Kaggle community to build a model that more accurately forecasts product sales. They currently rely on subjective forecasting methods with very little data to back them up and very little automation to execute plans. They\u2019re excited to see how machine learning could better ensure they please customers by having just enough of the right products at the right time.",
        "dataset_text": "In this competition, you will be predicting the unit sales for thousands of items sold at different Favorita stores located in Ecuador. The training data includes dates, store and item information, whether that item was being promoted, as well as the unit sales. Additional files include supplementary information that may be useful in building your models."
    },
    {
        "name": "RSNA Pneumonia Detection Challenge",
        "url": "https://www.kaggle.com/competitions/rsna-pneumonia-detection-challenge",
        "overview_text": "Overview text not found",
        "description_text": "In this competition, you\u2019re challenged to build an algorithm to detect a visual signal for pneumonia in medical images. Specifically, your algorithm needs to automatically locate lung opacities on chest radiographs. Here\u2019s the backstory and why solving this problem matters. Pneumonia accounts for over 15% of all deaths of children under 5 years old internationally. In 2015, 920,000 children under the age of 5 died from the disease. In the United States, pneumonia accounts for over 500,000 visits to emergency departments [1] and over 50,000 deaths in 2015 [2], keeping the ailment on the list of top 10 causes of death in the country. While common, accurately diagnosing pneumonia is a tall order. It requires review of a chest radiograph (CXR) by highly trained specialists and confirmation through clinical history, vital signs and laboratory exams. Pneumonia usually manifests as an area or areas of increased opacity [3] on CXR. However, the diagnosis of pneumonia on CXR is complicated because of a number of other conditions in the lungs such as fluid overload (pulmonary edema), bleeding, volume loss (atelectasis or collapse), lung cancer, or post-radiation or surgical changes. Outside of the lungs, fluid in the pleural space (pleural effusion) also appears as increased opacity on CXR. When available, comparison of CXRs of the patient taken at different time points and correlation with clinical symptoms and history are helpful in making the diagnosis. CXRs are the most commonly performed diagnostic imaging study. A number of factors such as positioning of the patient and depth of inspiration can alter the appearance of the CXR [4], complicating interpretation further. In addition, clinicians are faced with reading high volumes of images every shift. To improve the efficiency and reach of diagnostic services, the Radiological Society of North America (RSNA\u00ae) has reached out to Kaggle\u2019s machine learning community and collaborated with the US National Institutes of Health, The Society of Thoracic Radiology, and MD.ai to develop a rich dataset for this challenge.  The RSNA is an international society of radiologists, medical physicists and other medical professionals with more than 54,000 members from 146 countries across the globe. They see the potential for ML to automate initial detection (imaging screening) of potential pneumonia cases in order to prioritize and expedite their review. Challenge participants may be invited to present their AI models and methodologies during an award ceremony at the RSNA Annual Meeting which will be held in Chicago, Illinois, USA, from November 25-30, 2018. Thank you to the National Institutes of Health Clinical Center for publicly providing the Chest X-Ray dataset [5]. Also, a big thank you to the competition organizers!",
        "dataset_text": "Note that new files are available to download! The training set now contains both the train and test set from stage 1. The test set is comprised of new, unseen images. The metric and file formats remain the same, but you'll now be making predictions using the updated train and test sets. We have an FAQ about two-stage competitions that provides some context for how this all works. Please give it a read. Good luck! This is a two-stage challenge. You will need the images for the current stage - provided as stage_2_train_images.zip and stage_2_test_images.zip. You will also need the training data - stage_2_train_labels.csv - and the sample submission stage_2_sample_submission.csv, which provides the IDs for the test set, as well as a sample of what your submission should look like. The file stage_2_detailed_class_info.csv contains detailed information about the positive and negative classes in the training set, and may be used to build more nuanced models. The training data is provided as a set of patientIds and bounding boxes. Bounding boxes are defined as follows:\nx-min y-min width height There is also a binary target column, Target, indicating pneumonia or non-pneumonia. There may be multiple rows per patientId. All provided images are in DICOM format. In this challenge competitors are predicting whether pneumonia exists in a given image. They do so by predicting bounding boxes around areas of the lung. Samples without bounding boxes are negative and contain no definitive evidence of pneumonia. Samples with bounding boxes indicate evidence of pneumonia. When making predictions, competitors should predict as many bounding boxes as they feel are necessary, in the format:\nconfidence x-min y-min width height There should be only ONE predicted row per image. This row may include multiple bounding boxes. A properly formatted row may look like any of the following. For patientIds with no predicted pneumonia / bounding boxes:\n0004cfab-14fd-4e49-80ba-63a80b6bddd6, For patientIds with a single predicted bounding box:\n0004cfab-14fd-4e49-80ba-63a80b6bddd6,0.5 0 0 100 100 For patientIds with multiple predicted bounding boxes:\n0004cfab-14fd-4e49-80ba-63a80b6bddd6,0.5 0 0 100 100 0.5 0 0 100 100, etc."
    },
    {
        "name": "Predicting Molecular Properties",
        "url": "https://www.kaggle.com/competitions/champs-scalar-coupling",
        "overview_text": "Overview text not found",
        "description_text": "Think you can use your data science smarts to make big predictions at a molecular level? This challenge aims to predict interactions between atoms. Imaging technologies like MRI enable us to see and understand the molecular composition of tissues. Nuclear Magnetic Resonance (NMR) is a closely related technology which uses the same principles to understand the structure and dynamics of proteins and molecules. Researchers around the world conduct NMR experiments to further understanding of the structure and dynamics of molecules, across areas like environmental science, pharmaceutical science, and materials science. This competition is hosted by members of the CHemistry and Mathematics in Phase Space (CHAMPS) at the University of Bristol, Cardiff University, Imperial College and the University of Leeds. Winning teams will have an opportunity to partner with this multi-university research program on an academic publication Your Challenge In this competition, you will develop an algorithm that can predict the magnetic interaction between two atoms in a molecule (i.e., the scalar coupling constant). Once the competition finishes, CHAMPS would like to invite the top teams to present their work, discuss the details of their models, and work with them to write a joint research publication which discusses an open-source implementation of the solution. About Scalar Coupling Using NMR to gain insight into a molecule\u2019s structure and dynamics depends on the ability to accurately predict so-called \u201cscalar couplings\u201d. These are effectively the magnetic interactions between a pair of atoms. The strength of this magnetic interaction depends on intervening electrons and chemical bonds that make up a molecule\u2019s three-dimensional structure. Using state-of-the-art methods from quantum mechanics, it is possible to accurately calculate scalar coupling constants given only a 3D molecular structure as input. However, these quantum mechanics calculations are extremely expensive (days or weeks per molecule), and therefore have limited applicability in day-to-day workflows. A fast and reliable method to predict these interactions will allow medicinal chemists to gain structural insights faster and cheaper, enabling scientists to understand how the 3D chemical structure of a molecule affects its properties and behavior. Ultimately, such tools will enable researchers to make progress in a range of important problems, like designing molecules to carry out specific cellular tasks, or designing better drug molecules to fight disease. Join the CHAMPS Scalar Coupling challenge to apply predictive analytics to chemistry and chemical biology.",
        "dataset_text": "In this competition, you will be predicting the scalar_coupling_constant between atom pairs in molecules, given the two atom types (e.g., C and H), the coupling type (e.g., 2JHC), and any features you are able to create from the molecule structure (xyz) files. For this competition, you will not be predicting all the atom pairs in each molecule rather, you will only need to predict the pairs that are explicitly listed in the train and test files. For example, some molecules contain Fluorine (F), but you will not be predicting the scalar coupling constant for any pair that includes F. The training and test splits are by molecule, so that no molecule in the training data is found in the test data. NOTE: additional data is provided for the molecules in Train only!"
    },
    {
        "name": "SIIM-ACR Pneumothorax Segmentation",
        "url": "https://www.kaggle.com/competitions/siim-acr-pneumothorax-segmentation",
        "overview_text": "Overview text not found",
        "description_text": "Imagine suddenly gasping for air, helplessly breathless for no apparent reason. Could it be a collapsed lung? In the future, your entry in this competition could predict the answer. Pneumothorax can be caused by a blunt chest injury, damage from underlying lung disease, or most horrifying\u2014it may occur for no obvious reason at all. On some occasions, a collapsed lung can be a life-threatening event. Pneumothorax is usually diagnosed by a radiologist on a chest x-ray, and can sometimes be very difficult to confirm. An accurate AI algorithm to detect pneumothorax would be useful in a lot of clinical scenarios. AI could be used to triage chest radiographs for priority interpretation, or to provide a more confident diagnosis for non-radiologists. The Society for Imaging Informatics in Medicine (SIIM) is the leading healthcare organization for those interested in the current and future use of informatics in medical imaging. Their mission is to advance medical imaging informatics across the enterprise through education, research, and innovation in a multi-disciplinary community. Today, they need your help. In this competition, you\u2019ll develop a model to classify (and if present, segment) pneumothorax from a set of chest radiographic images. If successful, you could aid in the early recognition of pneumothoraces and save lives. If you\u2019re up for the challenge, take a deep breath, and get started now. Note: As specified on the Data Page, the dataset must be retrieved from Cloud Healthcare. Review this tutorial (or in pdf format) for instructions on how to do so. SIIM Machine Learning Committee Co-Chairs, Steven G. Langer, PhD, CIIP and George Shih, MD, MS for tirelessly leading this effort and making the challenge possible in such a short period of time. SIIM Machine Learning Committee Members for their dedication in annotating the dataset, helping to define the most useful metrics and running tests to prepare the challenge for launch. SIIM Hackathon Committee, especially Mohannad Hussain, for their crucial technical support with data conversion.  American College of Radiology (ACR), @RadiologyACR: For Co-hosting the challenge and Co-sponsoring the Prizes  Society of Thoracic Radiology (STR), @thoracicrad: For their unparalleled expertise in adjudicating the dataset  MD.ai: For providing the annotation tool and helping with the first layer of annotations",
        "dataset_text": "Stage 2 Note: the stage 1 files (if needed) should be downloaded using the special downloading instructions. The stage 2 files must be downloaded directly from Kaggle. See below. Note: this is a two-stage competition with special downloading instructions. Please read carefully and ask any questions you might have! The stage 2 training set consists of the stage 1 training set and the stage 1 test set combined. If you need to download their images, please follow these instructions for both train AND test, using this tutorial page or in .pdf format here. The annotations for the stage 2 training set should be downloaded below - the filename is stage_2_train.csv. The stage 2 test images can be downloaded directly from this page - contained in stage_2_images.zip. You may also need the sample submission (which contains all of the test IDs for stage 2) to aid you in making predictions. The data is comprised of images in DICOM format and annotations in the form of image IDs and run-length-encoded (RLE) masks. Some of the images contain instances of pneumothorax (collapsed lung), which are indicated by encoded binary masks in the annotations. Some training images have multiple annotations. Images without pneumothorax have a mask value of -1. We are attempting to a) predict the existence of pneumothorax in our test images and b) indicate the location and extent of the condition using masks. Your model should create binary masks and encode them using RLE. Note that we are using a relative form of RLE (meaning that pixel locations are measured from the end of the previous run) as indicated below: Sample code is available for download that may help with encoding and decoding this form of RLE.\nEach test image may only have one mask submitted for it. It should combine all predicted masks for that image."
    },
    {
        "name": "Mechanisms of Action (MoA) Prediction",
        "url": "https://www.kaggle.com/competitions/lish-moa",
        "overview_text": "Overview text not found",
        "description_text": "The Connectivity Map, a project within the Broad Institute of MIT and Harvard, the Laboratory for Innovation Science at Harvard (LISH), and the NIH Common Funds Library of Integrated Network-Based Cellular Signatures (LINCS), present this challenge with the goal of advancing drug development through improvements to MoA prediction algorithms. What is the Mechanism of Action (MoA) of a drug? And why is it important? In the past, scientists derived drugs from natural products or were inspired by traditional remedies. Very common drugs, such as paracetamol, known in the US as acetaminophen, were put into clinical use decades before the biological mechanisms driving their pharmacological activities were understood. Today, with the advent of more powerful technologies, drug discovery has changed from the serendipitous approaches of the past to a more targeted model based on an understanding of the underlying biological mechanism of a disease. In this new framework, scientists seek to identify a protein target associated with a disease and develop a molecule that can modulate that protein target. As a shorthand to describe the biological activity of a given molecule, scientists assign a label referred to as mechanism-of-action or MoA for short. How do we determine the MoAs of a new drug? One approach is to treat a sample of human cells with the drug and then analyze the cellular responses with algorithms that search for similarity to known patterns in large genomic databases, such as libraries of gene expression or cell viability patterns of drugs with known MoAs. In this competition, you will have access to a unique dataset that combines gene expression and cell viability data. The data is based on a new technology that measures simultaneously (within the same samples) human cells\u2019 responses to drugs in a pool of 100 different cell types (thus solving the problem of identifying ex-ante, which cell types are better suited for a given drug). In addition, you will have access to MoA annotations for more than 5,000 drugs in this dataset. As is customary, the dataset has been split into testing and training subsets. Hence, your task is to use the training dataset to develop an algorithm that automatically labels each case in the test set as one or more MoA classes. Note that since drugs can have multiple MoA annotations, the task is formally a multi-label classification problem. How to evaluate the accuracy of a solution? Based on the MoA annotations, the accuracy of solutions will be evaluated on the average value of the logarithmic loss function applied to each drug-MoA annotation pair. If successful, you\u2019ll help to develop an algorithm to predict a compound\u2019s MoA given its cellular signature, thus helping scientists advance the drug discovery process.   ",
        "dataset_text": "In this competition, you will be predicting multiple targets of the Mechanism of Action (MoA) response(s) of different samples (sig_id), given various inputs such as gene expression data and cell viability data. Two notes:"
    },
    {
        "name": "Lyft Motion Prediction for Autonomous Vehicles",
        "url": "https://www.kaggle.com/competitions/lyft-motion-prediction-autonomous-vehicles",
        "overview_text": "Overview text not found",
        "description_text": " Autonomous vehicles (AVs) are expected to dramatically redefine the future of transportation. However, there are still significant engineering challenges to be solved before one can fully realize the benefits of self-driving cars. One such challenge is building models that reliably predict the movement of traffic agents around the AV, such as cars, cyclists, and pedestrians. The ridesharing company Lyft started Level 5 to take on the self-driving challenge and build a full self-driving system (they\u2019re hiring!). Their previous competition tasked participants with identifying 3D objects, an important step prior to detecting their movement. Now, they\u2019re challenging you to predict the motion of these traffic agents. In this competition, you\u2019ll apply your data science skills to build motion prediction models for self-driving vehicles. You'll have access to the largest Prediction Dataset ever released to train and test your models. Your knowledge of machine learning will then be required to predict how cars, cyclists,and pedestrians move in the AV's environment. Lyft\u2019s mission is to improve people\u2019s lives with the world\u2019s best transportation. They believe in a future where self-driving cars make transportation safer, environment-friendly and more accessible for everyone. Their goal is to accelerate development across the industry by sharing data with researchers. As a result of your participation, you can have a hand in propelling the industry forward and helping people around the world benefit from self-driving cars sooner.",
        "dataset_text": "The Lyft Motion Prediction for Autonomous Vehicles competition is fairly unique, data-wise. In it, a very large amount of data is provided, which can be used in many different ways. Reading the data is also complex - please refer to Lyft's L5Kit module and sample notebooks to properly load the data and use it for training. Further Kaggle-specific sample notebooks will follow shortly. Note also that this competition requires that submissions be made from kernels, and that internet must be turned off in your submission kernels. For your convenience, Lyft's l5kit module is provided via a utility script called kaggle_l5kit. Just attach it to your kernel, and the latest version of l5kit and all dependencies will be available. You can compete with just train.zarr and test.zarr, the other files are optional but will likely be helpful. Please refer to the sample notebooks for help on how to load and iterate over the datasets. Note: for full details, please refer to the data format page in L5Kit The data is packaged in .zarr files. These are loaded using the zarr Python module, and are also loaded natively by l5kit. Each .zarr file contains a set of: We are predicting the motion of the objects in a given scene. For test, you will have 99 frames of objects moving around will be asked to predict their location in the next 50."
    },
    {
        "name": "SIIM-ISIC Melanoma Classification",
        "url": "https://www.kaggle.com/competitions/siim-isic-melanoma-classification",
        "overview_text": "Overview text not found",
        "description_text": "Skin cancer is the most prevalent type of cancer. Melanoma, specifically, is responsible for 75% of skin cancer deaths, despite being the least common skin cancer. The American Cancer Society estimates over 100,000 new melanoma cases will be diagnosed in 2020. It's also expected that almost 7,000 people will die from the disease. As with other cancers, early and accurate detection\u2014potentially aided by data science\u2014can make treatment more effective. Currently, dermatologists evaluate every one of a patient's moles to identify outlier lesions or \u201cugly ducklings\u201d that are most likely to be melanoma. Existing AI approaches have not adequately considered this clinical frame of reference. Dermatologists could enhance their diagnostic accuracy if detection algorithms take into account \u201ccontextual\u201d images within the same patient to determine which images represent a melanoma. If successful, classifiers would be more accurate and could better support dermatological clinic work. As the leading healthcare organization for informatics in medical imaging, the Society for Imaging Informatics in Medicine (SIIM)'s mission is to advance medical imaging informatics through education, research, and innovation in a multi-disciplinary community. SIIM is joined by the International Skin Imaging Collaboration (ISIC), an international effort to improve melanoma diagnosis. The ISIC Archive contains the largest publicly available collection of quality-controlled dermoscopic images of skin lesions. In this competition, you\u2019ll identify melanoma in images of skin lesions. In particular, you\u2019ll use images within the same patient and determine which are likely to represent a melanoma. Using patient-level contextual information may help the development of image analysis tools, which could better support clinical dermatologists. Melanoma is a deadly disease, but if caught early, most melanomas can be cured with minor surgery. Image analysis tools that automate the diagnosis of melanoma will improve dermatologists' diagnostic accuracy. Better detection of melanoma has the opportunity to positively impact millions of people.",
        "dataset_text": "The images are provided in DICOM format. This can be accessed using commonly-available libraries like pydicom, and contains both image and metadata. It is a commonly used medical imaging data format. Images are also provided in JPEG and TFRecord format (in the jpeg and tfrecords directories, respectively). Images in TFRecord format have been resized to a uniform 1024x1024. Metadata is also provided outside of the DICOM format, in CSV files. See the Columns section for a description. You are predicting a binary target for each image. Your model should predict the probability (floating point) between 0.0 and 1.0 that the lesion in the image is malignant (the target). In the training data, train.csv, the value 0 denotes benign, and 1 indicates malignant. Please cite the dataset under CC BY-NC 4.0 with the following attribution:"
    },
    {
        "name": "RSNA STR Pulmonary Embolism Detection",
        "url": "https://www.kaggle.com/competitions/rsna-str-pulmonary-embolism-detection",
        "overview_text": "Overview text not found",
        "description_text": " If every breath is strained and painful, it could be a serious and potentially life-threatening condition. A pulmonary embolism (PE) is caused by an artery blockage in the lung. It is time consuming to confirm a PE and prone to overdiagnosis. Machine learning could help to more accurately identify PE cases, which would make management and treatment more effective for patients. Currently, CT pulmonary angiography (CTPA), is the most common type of medical imaging to evaluate patients with suspected PE. These CT scans consist of hundreds of images that require detailed review to identify clots within the pulmonary arteries. As the use of imaging continues to grow, constraints of radiologists\u2019 time may contribute to delayed diagnosis. The Radiological Society of North America (RSNA\u00ae) has teamed up with the Society of Thoracic Radiology (STR) to help improve the use of machine learning in the diagnosis of PE. In this competition, you\u2019ll detect and classify PE cases. In particular, you'll use chest CTPA images (grouped together as studies) and your data science skills to enable more accurate identification of PE. If successful, you'll help reduce human delays and errors in detection and treatment. With 60,000-100,000 PE deaths annually in the United States, it is among the most fatal cardiovascular diseases. Timely and accurate diagnosis will help these patients receive better care and may also improve outcomes. The Radiological Society of North America (RSNA\u00ae) is an international society of radiologists, medical physicists, and other medical professionals with more than 53,400 members worldwide. RSNA hosts the world\u2019s premier radiology forum and publishes two top peer-reviewed journals: Radiology, the highest-impact scientific journal in the field, and RadioGraphics, the only journal dedicated to continuing education in radiology. The Society of Thoracic Radiology (STR) was founded in 1982. The STR is dedicated to advancing cardiothoracic imaging in clinical application, education, and research in radiology and allied disciplines. Continuing professional development opportunities provided by the STR include educational and scientific meetings, mentorship programs, grant support and award opportunities, our society journal, Journal of Thoracic Imaging, and global collaboration activities. A full set of acknowledgments can be found on this page.",
        "dataset_text": "You may access and use these de-identified imaging datasets and annotations (\u201cthe data\u201d) for non-commercial purposes only, including academic research and education, as long as you agree to abide by the following provisions: In this competition, we are predicting the existence and characteristics of pulmonary embolisms. Please see the Evaluation page for further details about the predictions themselves. Note also that the private test set is approximately 3x larger than the public test set (230GB vs. 70GB), so ensure that your kernels have enough time to finish their re-run. The training set includes 7279 studies, the public set 650, and the private set has 1517. You will need the training and test images, as well as train.csv and test.csv. The images are grouped in directories by study and series. They are in DICOM format, and contain additional metadata that may be relevant to the competition. Each image has a unique identifier - SOPInstanceUID. The location for each image is given by: <StudyInstanceUID>/<SeriesInstanceUID>/<SOPInstanceUID>.dcm. The data provided by the host for this competition and made available below is the RSNA-STR PE CT (RSPECT) dataset. Use of the dataset for non-commercial and/or academic purposes is permitted with citation. train.csv contains the three UIDs noted above, and a number of labels. Some are targets which require predictions, and some are informational, which will also be noted below in Data fields. test.csv contains only the three UIDs. In this competition we are predicting a number of labels, at both the image and study level. Note that some labels are logically mutually exclusive. Please see the Evaluation page for additional details. The following image is a flowchart outlining the relationships between labels. Note that there are four labels in the training set that are purely informational and require no predictions. They are QA Contrast, QA Motion, True filling defect not PE, and Flow artifact, and are not scored, but are meant to be used as helpers. Also note that Acute PE is not an explicit label, but is implied by the lack of Chronic PE or Acute and Chronic PE. Note that your predictions must adhere to the expected label hierarchy defined in this diagram, and the host will verify that prospective winners have not made conflicting label predictions, as detailed on the Prizes page. The requirements which submissions will be held to are specified by the host in this post, and the code that will be used to check compliance with these requirements is available in this notebook. "
    },
    {
        "name": "Shopee - Price Match Guarantee",
        "url": "https://www.kaggle.com/competitions/shopee-product-matching",
        "overview_text": "Overview text not found",
        "description_text": "Do you scan online retailers in search of the best deals? You're joined by the many savvy shoppers who don't like paying extra for the same product depending on where they shop. Retail companies use a variety of methods to assure customers that their products are the cheapest. Among them is product matching, which allows a company to offer products at rates that are competitive to the same product sold by another retailer. To perform these matches automatically requires a thorough machine learning approach, which is where your data science skills could help. Two different images of similar wares may represent the same product or two completely different items. Retailers want to avoid misrepresentations and other issues that could come from conflating two dissimilar products. Currently, a combination of deep learning and traditional machine learning analyzes image and text information to compare similarity. But major differences in images, titles, and product descriptions prevent these methods from being entirely effective. Shopee is the leading e-commerce platform in Southeast Asia and Taiwan. Customers appreciate its easy, secure, and fast online shopping experience tailored to their region. The company also provides strong payment and logistical support along with a 'Lowest Price Guaranteed' feature on thousands of Shopee's listed products. In this competition, you\u2019ll apply your machine learning skills to build a model that predicts which items are the same products. The applications go far beyond Shopee or other retailers. Your contributions to product matching could support more accurate product categorization and uncover marketplace spam. Customers will benefit from more accurate listings of the same or similar products as they shop. Perhaps most importantly, this will aid you and your fellow shoppers in your hunt for the very best deals.",
        "dataset_text": "Finding near-duplicates in large datasets is an important problem for many online businesses. In Shopee's case, everyday users can upload their own images and write their own product descriptions, adding an extra layer of challenge. Your task is to identify which products have been posted repeatedly. The differences between related products may be subtle while photos of identical products may be wildly different! As this is a code competition, only the first few rows/images of the test set are published; the remainder are only available to your notebook when it is submitted. Expect to find roughly 70,000 images in the hidden test set. The few test rows and images that are provided are intended to illustrate the hidden test set format and folder structure. [train/test].csv - the training set metadata. Each row contains the data for a single posting. Multiple postings might have the exact same image ID, but with different titles or vice versa. [train/test]images - the images associated with the postings. sample_submission.csv - a sample submission file in the correct format."
    },
    {
        "name": "RSNA-MICCAI Brain Tumor Radiogenomic Classification",
        "url": "https://www.kaggle.com/competitions/rsna-miccai-brain-tumor-radiogenomic-classification",
        "overview_text": "Overview text not found",
        "description_text": "A malignant tumor in the brain is a life-threatening condition. Known as glioblastoma, it's both the most common form of brain cancer in adults and the one with the worst prognosis, with median survival being less than a year. The presence of a specific genetic sequence in the tumor known as MGMT promoter methylation has been shown to be a favorable prognostic factor and a strong predictor of responsiveness to chemotherapy.  Currently, genetic analysis of cancer requires surgery to extract a tissue sample. Then it can take several weeks to determine the genetic characterization of the tumor. Depending upon the results and type of initial therapy chosen, a subsequent surgery may be necessary. If an accurate method to predict the genetics of the cancer through imaging (i.e., radiogenomics) alone could be developed, this would potentially minimize the number of surgeries and refine the type of therapy required. The Radiological Society of North America (RSNA) has teamed up with the Medical Image Computing and Computer Assisted Intervention Society (the MICCAI Society) to improve diagnosis and treatment planning for patients with glioblastoma. In this competition you will predict the genetic subtype of glioblastoma using MRI (magnetic resonance imaging) scans to train and test your model to detect for the presence of MGMT promoter methylation. If successful, you'll help brain cancer patients receive less invasive diagnoses and treatments. The introduction of new and customized treatment strategies before surgery has the potential to improve the management, survival, and prospects of patients with brain cancer. The Radiological Society of North America (RSNA\u00ae) is a non-profit organization that represents 31 radiologic subspecialties from 145 countries around the world. RSNA promotes excellence in patient care and health care delivery through education, research and technological innovation. RSNA provides high-quality educational resources, publishes five top peer-reviewed journals, hosts the world\u2019s largest radiology conference and is dedicated to building the future of the profession through the RSNA Research & Education (R&E) Foundation, which has funded $66 million in grants since its inception. RSNA also supports and facilitates artificial intelligence (AI) research in medical imaging by sponsoring an ongoing series of AI challenge competitions. The Medical Image Computing and Computer Assisted Intervention Society (the MICCAI Society) is dedicated to the promotion, preservation and facilitation of research, education and practice in the field of medical image computing and computer assisted medical interventions including biomedical imaging and medical robotics. The Society achieves this aim through the organization and operation of annual high quality international conferences, workshops, tutorials and publications that promote and foster the exchange and dissemination of advanced knowledge, expertise and experience in the field produced by leading institutions and outstanding scientists, physicians and educators around the world. A full set of acknowledgments can be found on this page. ",
        "dataset_text": "The competition data is defined by three cohorts: Training, Validation (Public), and Testing (Private). The \u201cTraining\u201d and the \u201cValidation\u201d cohorts are provided to the participants, whereas the \u201cTesting\u201d cohort is kept hidden at all times, during and after the competition. These 3 cohorts are structured as follows: Each independent case has a dedicated folder identified by a five-digit number. Within each of these \u201ccase\u201d folders, there are four sub-folders, each of them corresponding to each of the structural multi-parametric MRI (mpMRI) scans, in DICOM format. The exact mpMRI scans included are: Exact folder structure: If you reference or use the dataset in any form, include the following citation:"
    },
    {
        "name": "RSNA 2022 Cervical Spine Fracture Detection",
        "url": "https://www.kaggle.com/competitions/rsna-2022-cervical-spine-fracture-detection",
        "overview_text": "Overview text not found",
        "description_text": "Over 1.5 million spine fractures occur annually in the United States alone resulting in over 17,730 spinal cord injuries annually. The most common site of spine fracture is the cervical spine. There has been a rise in the incidence of spinal fractures in the elderly and in this population, fractures can be more difficult to detect on imaging due to superimposed degenerative disease and osteoporosis. Imaging diagnosis of adult spine fractures is now almost exclusively performed with computed tomography (CT) instead of radiographs (x-rays). Quickly detecting and determining the location of any vertebral fractures is essential to prevent neurologic deterioration and paralysis after trauma. RSNA has teamed with the American Society of Neuroradiology (ASNR) and the American Society of Spine Radiology (ASSR) to conduct an AI challenge competition exploring whether artificial intelligence can be used to aid in the detection and localization of cervical spine fractures. To create the ground truth dataset, the challenge planning task force collected imaging data sourced from twelve sites on six continents, including approximately 3,000 CT studies. Spine radiology specialists from the ASNR and ASSR provided expert image level annotations these studies to indicate the presence, vertebral level and location of any cervical spine fractures. In this challenge competition, you will try to develop machine learning models that match the radiologists' performance in detecting and localizing fractures to the seven vertebrae that comprise the cervical spine. Winners will be recognized at an event during the RSNA 2022 annual meeting. For more information on the challenge, contact RSNA Informatics staff at informatics@rsna.org. A full set of acknowledgments can be found on this page.",
        "dataset_text": "The goal of this competition is to identify fractures in CT scans of the cervical spine (neck) at both the level of a single vertebrae and the entire patient. Quickly detecting and determining the location of any vertebral fractures is essential to prevent neurologic deterioration and paralysis after trauma. This competition uses a hidden test. When your submitted notebook is scored the actual test data (including a full length sample submission) will be made available to your notebook. train.csv Metadata for the train test set. test.csv Metadata for the test set prediction structure. Only the first few rows of the test set are available for download. [train/test]_images/[StudyInstanceUID]/[slice_number].dcm The image data, organized with one folder per scan. Expect to see roughly 1,500 scans in the hidden test set.\nEach image is in the dicom file format. The DICOM image files are \u2264 1 mm slice thickness, axial orientation, and bone kernel. Note that some of the DICOM files are JPEG compressed. You may require additional resources to read the pixel array of these files, such as GDCM and pylibjpeg. sample_submission.csv A valid sample submission. train_bounding_boxes.csv Bounding boxes for a subset of the training set. segmentations/ Pixel level annotations for a subset of the training set. This data is provided in the nifti file format. A portion of the imaging datasets have been segmented automatically using a 3D UNET model, and radiologists modified and approved the segmentations. The provided segmentation labels have values of 1 to 7 for C1 to C7 (seven cervical vertebrae) and 8 to 19 for T1 to T12 (twelve thoracic vertebrae are located in the center of your upper and middle back), and 0 for everything else. As we focused on the cervical spine, all scans have C1 to C7 labels but not all thoracic labels. Please be aware that the NIFTI files consist of segmentation in the sagittal plane, while the DICOM files are in the axial plane. Please use the NIFTI header information to determine the appropriate orientation such that the DICOM images and segmentation match. Otherwise, you run the risk of having the segmentations flipped in the Z axis and mirrored in the X axis."
    },
    {
        "name": "OTTO \u2013 Multi-Objective Recommender System",
        "url": "https://www.kaggle.com/competitions/otto-recommender-system",
        "overview_text": "Overview text not found",
        "description_text": "The goal of this competition is to predict e-commerce clicks, cart additions, and orders. You'll build a multi-objective recommender system based on previous events in a user session. Your work will help improve the shopping experience for everyone involved. Customers will receive more tailored recommendations while online retailers may increase their sales. Online shoppers have their pick of millions of products from large retailers. While such variety may be impressive, having so many options to explore can be overwhelming, resulting in shoppers leaving with empty carts. This neither benefits shoppers seeking to make a purchase nor retailers that missed out on sales. This is one reason online retailers rely on recommender systems to guide shoppers to products that best match their interests and motivations. Using data science to enhance retailers' ability to predict which products each customer actually wants to see, add to their cart, and order at any given moment of their visit in real-time could improve your customer experience the next time you shop online with your favorite retailer. Current recommender systems consist of various models with different approaches, ranging from simple matrix factorization to a transformer-type deep neural network. However, no single model exists that can simultaneously optimize multiple objectives. In this competition, you\u2019ll build a single entry to predict click-through, add-to-cart, and conversion rates based on previous same-session events. With more than 10 million products from over 19,000 brands, OTTO is the largest German online shop. OTTO is a member of the Hamburg-based, multi-national Otto Group, which also subsidizes Crate & Barrel (USA) and 3 Suisses (France). Your work will help online retailers select more relevant items from a vast range to recommend to their customers based on their real-time behavior. Improving recommendations will ensure navigating through seemingly endless options is more effortless and engaging for shoppers.",
        "dataset_text": "The goal of this competition is to predict e-commerce clicks, cart additions, and orders. You'll build a multi-objective recommender system based on previous events in a user session. The training data contains full e-commerce session information. For each session in the test data, your task it to predict the aid values for each session type thats occur after the last timestamp ts in the test session. In other words, the test data contains sessions truncated by timestamp, and you are to predict what occurs after the point of truncation. For additional background, please see the published OTTO Recommender Systems Dataset GitHub."
    },
    {
        "name": "Google AI Open Images - Object Detection Track",
        "url": "https://www.kaggle.com/competitions/google-ai-open-images-object-detection-track",
        "overview_text": "Overview text not found",
        "description_text": "Google AI (Google\u2019s AI research arm, tasked with advancing AI for everyone) is challenging you to build an algorithm that detects objects automatically using an absolutely massive training dataset \u2015 one with more varied and complex bounding-box annotations and object classes than ever before. Here's the background. Computers are getting better and better at vision. But in a few critical ways, they still can't match a human\u2019s intuitive perception. For example, what do you see when you look at this photo? Most of us would answer, \u201ca sandy beach, the ocean, a few people walking, some trees, grass, and buildings\u2026a woman walking her dog right there! Oh yeah, and there is a man holding a plastic cup.\u201d Can a computer provide as precise an image description? Google AI wants to further push the capabilities of computer vision. We hope that providing very large training set will stimulate research into more sophisticated object and relationship detection models that will exceed current state-of-the-art performance. The results of this Challenge will be presented at a workshop at the European Conference on Computer Vision 2018. With this in mind, to spur advances in analyzing and understanding images, Google AI has publicly released the Open Images dataset. Open Images follows the tradition of PASCAL VOC, ImageNet and COCO, now at an unprecedented scale. The Open Images Challenge is based on Open Images dataset. The training set of the Challenge contains: In this track of the Challenge, you are asked to build the best performing algorithm for automatically detecting objects. Please refer to the Open Images Challenge page for additional details on the dataset. In addition to this Object Detection track, the Challenge also includes a Visual Relationship Detection track to detect pairs of objects in particular relations, e.g. \"woman playing guitar,\" \"beer on table,\" \"dog inside car\", \"man holding coffee\", etc. The Visual Relationship Detection track is available here. Example annotations. Left: Mark Paul Gosselaar plays the guitar by Rhys A. Right: the house by anita kluska. Both images used under CC BY 2.0 license.",
        "dataset_text": "The train and validation sets of images and their ground truth (bounding boxes and labels) should be downloaded from Open Images Challenge page . Please note that the test images used in this competition is independent from previously released part of Open Images V4. The images can be downloaded from: Note: The images are the same as in the Visual Relationship Track so you do not need to re-download them. You should expect 99,999 images in total."
    },
    {
        "name": "New York City Taxi Trip Duration",
        "url": "https://www.kaggle.com/competitions/nyc-taxi-trip-duration",
        "overview_text": "Overview text not found",
        "description_text": " In this competition, Kaggle is challenging you to build a model that predicts the total ride duration of taxi trips in New York City. Your primary dataset is one released by the NYC Taxi and Limousine Commission, which includes pickup time, geo-coordinates, number of passengers, and several other variables. Longtime Kagglers will recognize that this competition objective is similar to the ECML/PKDD trip time challenge we hosted in 2015. But, this challenge comes with a twist. Instead of awarding prizes to the top finishers on the leaderboard, this playground competition was created to reward collaboration and collective learning. We are encouraging you (with cash prizes!) to publish additional training data that other participants can use for their predictions. We also have designated bi-weekly and final prizes to reward authors of kernels that are particularly insightful or valuable to the community.",
        "dataset_text": "The competition dataset is based on the 2016 NYC Yellow Cab trip record data made available in Big Query on Google Cloud Platform. The data was originally published by the NYC Taxi and Limousine Commission (TLC). The data was sampled and cleaned for the purposes of this playground competition. Based on individual trip attributes, participants should predict the duration of each trip in the test set. Disclaimer: The decision was made to not remove dropoff coordinates from the dataset order to provide an expanded set of variables to use in Kernels."
    },
    {
        "name": "2019 Kaggle Machine Learning & Data Science Survey",
        "url": "https://www.kaggle.com/competitions/kaggle-survey-2019",
        "overview_text": "Overview text not found",
        "description_text": "Welcome to Kaggle's third annual Machine Learning and Data Science Survey \u2015 and our second-ever survey data challenge. You can read our executive summary here. This year, as in 2017 and 2018, we set out to conduct an industry-wide survey that presents a truly comprehensive view of the state of data science and machine learning. The survey was live for three weeks in October, and after cleaning the data we finished with 19,717 responses! There's a lot to explore here. The results include raw numbers about who is working with data, what\u2019s happening with machine learning in different industries, and the best ways for new data scientists to break into the field. We've published the data in as raw a format as possible without compromising anonymization, which makes it an unusual example of a survey dataset. This year Kaggle is launching the second annual Data Science Survey Challenge, where we will be awarding a prize pool of $30,000 to notebook authors who tell a rich story about a subset of the data science and machine learning community. In our third year running this survey, we were once again awed by the global, diverse, and dynamic nature of the data science and machine learning industry. This survey data EDA provides an overview of the industry on an aggregate scale, but it also leaves us wanting to know more about the many specific communities comprised within the survey. For that reason, we\u2019re inviting the Kaggle community to dive deep into the survey datasets and help us tell the diverse stories of data scientists from around the world. The challenge objective: tell a data story about a subset of the data science community represented in this survey, through a combination of both narrative text and data exploration. A \u201cstory\u201d could be defined any number of ways, and that\u2019s deliberate. The challenge is to deeply explore (through data) the impact, priorities, or concerns of a specific group of data science and machine learning practitioners. That group can be defined in the macro (for example: anyone who does most of their coding in Python) or the micro (for example: female data science students studying machine learning in masters programs). This is an opportunity to be creative and tell the story of a community you identify with or are passionate about! Submissions will be evaluated on the following: To be valid, a submission must be contained in one notebook, made public on or before the submission deadline. Participants are free to use any datasets in addition to the Kaggle Data Science survey, but those datasets must also be publicly available on Kaggle by the deadline for a submission to be valid. To make a submission, complete the submission form. Only one submission will be judged per participant, so if you make multiple submissions we will review the last (most recent) entry. No submission is necessary for the Weekly Notebook Award. To be eligible, a notebook must be public and use the 2019 Data Science Survey as a data source. Submission deadline: 11:59PM UTC, December 2nd, 2019. Data has been released under a CC 2.0 license: https://creativecommons.org/licenses/by/2.0/",
        "dataset_text": "Dataset description not found"
    },
    {
        "name": "2020 Kaggle Machine Learning & Data Science Survey",
        "url": "https://www.kaggle.com/competitions/kaggle-survey-2020",
        "overview_text": "Overview text not found",
        "description_text": "Welcome to Kaggle's annual Machine Learning and Data Science Survey competition! You can read our executive summary here. This year, as in 2017, 2018, and 2019 we set out to conduct an industry-wide survey that presents a truly comprehensive view of the state of data science and machine learning. The survey was live for 3.5 weeks in October, and after cleaning the data we finished with 20,036 responses! There's a lot to explore here. The results include raw numbers about who is working with data, what\u2019s happening with machine learning in different industries, and the best ways for new data scientists to break into the field. We've published the data in as raw a format as possible without compromising anonymization, which makes it an unusual example of a survey dataset. This year Kaggle is once again launching an annual Data Science Survey Challenge, where we will be awarding a prize pool of $30,000 to notebook authors who tell a rich story about a subset of the data science and machine learning community. In our fourth year running this survey, we were once again awed by the global, diverse, and dynamic nature of the data science and machine learning industry. This survey data EDA provides an overview of the industry on an aggregate scale, but it also leaves us wanting to know more about the many specific communities comprised within the survey. For that reason, we\u2019re inviting the Kaggle community to dive deep into the survey datasets and help us tell the diverse stories of data scientists from around the world. The challenge objective: tell a data story about a subset of the data science community represented in this survey, through a combination of both narrative text and data exploration. A \u201cstory\u201d could be defined any number of ways, and that\u2019s deliberate. The challenge is to deeply explore (through data) the impact, priorities, or concerns of a specific group of data science and machine learning practitioners. That group can be defined in the macro (for example: anyone who does most of their coding in Python) or the micro (for example: female data science students studying machine learning in masters programs). This is an opportunity to be creative and tell the story of a community you identify with or are passionate about! Submissions will be evaluated on the following: To be valid, a submission must be contained in one notebook, made public on or before the submission deadline. Participants are free to use any datasets in addition to the Kaggle Data Science survey, but those datasets must also be publicly available on Kaggle by the deadline for a submission to be valid. To make a submission, complete the submission form. Only one submission will be judged per participant, so if you make multiple submissions we will review the last (most recent) entry. No submission is necessary for the Notebook Award. To be eligible, a notebook must be public and use the 2020 Data Science Survey as a data source. Submission deadline: 11:59PM UTC, January 6th, 2021.",
        "dataset_text": "kaggle_survey_2020_responses.csv: 39+ questions and 20,036 responses kaggle_survey_2020_answer_choices.pdf: list of answer choices for every question"
    },
    {
        "name": "2021 Kaggle Machine Learning & Data Science Survey",
        "url": "https://www.kaggle.com/competitions/kaggle-survey-2021",
        "overview_text": "Overview text not found",
        "description_text": "Welcome to Kaggle's annual Machine Learning and Data Science Survey competition! You can read our executive summary here. This year, as in 2017, 2018, 2019, and 2020 we set out to conduct an industry-wide survey that presents a truly comprehensive view of the state of data science and machine learning. The survey was live from 09/01/2021 to 10/04/2021, and after cleaning the data we finished with 25,973 responses! There's a lot to explore here. The results include raw numbers about who is working with data, what\u2019s happening with machine learning in different industries, and the best ways for new data scientists to break into the field. We've published the data in as raw a format as possible without compromising anonymization, which makes it an unusual example of a survey dataset. This year Kaggle is once again launching an annual Data Science Survey Challenge, where we will be awarding a prize pool of $30,000 to notebook authors who tell a rich story about a subset of the data science and machine learning community. In our fifth year running this survey, we were once again awed by the global, diverse, and dynamic nature of the data science and machine learning industry. This survey data EDA provides an overview of the industry on an aggregate scale, but it also leaves us wanting to know more about the many specific communities comprised within the survey. For that reason, we\u2019re inviting the Kaggle community to dive deep into the survey datasets and help us tell the diverse stories of data scientists from around the world. The challenge objective: tell a data story about a subset of the data science community represented in this survey, through a combination of both narrative text and data exploration. A \u201cstory\u201d could be defined any number of ways, and that\u2019s deliberate. The challenge is to deeply explore (through data) the impact, priorities, or concerns of a specific group of data science and machine learning practitioners. That group can be defined in the macro (for example: anyone who does most of their coding in Python) or the micro (for example: female data science students studying machine learning in masters programs). This is an opportunity to be creative and tell the story of a community you identify with or are passionate about! Submissions will be evaluated on the following: To be valid, a submission must be contained in one notebook, made public on or before the submission deadline. Participants are free to use any datasets in addition to the Kaggle Data Science survey, but those datasets must also be publicly available on Kaggle by the deadline for a submission to be valid. You can make your submission by filling out the submission form.",
        "dataset_text": "kaggle_survey_2021_responses.csv: 42+ questions and 25,973 responses kaggle_survey_2021_answer_choices.pdf: list of answer choices for every question kaggle_survey_2021_methodology.pdf: a description of how the survey was conducted"
    },
    {
        "name": "2022 Kaggle Machine Learning & Data Science Survey",
        "url": "https://www.kaggle.com/competitions/kaggle-survey-2022",
        "overview_text": "Overview text not found",
        "description_text": "Welcome to Kaggle's annual Machine Learning and Data Science Survey competition! You can read our executive summary here. This year, as in 2017, 2018, 2019, 2020, and 2021, we set out to conduct an industry-wide survey that presents a truly comprehensive view of the state of data science and machine learning. The survey was live from 09/16/2022 to 10/16/2022, and after cleaning the data we finished with 23,997 responses! There's a lot to explore here. The results include raw numbers about who is working with data, what\u2019s happening with machine learning in different industries, and the best ways for new data scientists to break into the field. We've published all of the data rather than only the aggregated survey results, which makes it an unusual example of a survey dataset, as it allows analysts to investigate the data on their own. This year Kaggle is once again launching an annual Data Science Survey Challenge, where we will be awarding a prize pool of $30,000 to notebook authors who tell a rich story about a subset of the data science and machine learning community. In our sixth year running this survey, we were once again awed by the global, diverse, and dynamic nature of the data science and machine learning industry. This survey data EDA provides an overview of the industry on an aggregate scale, but it also leaves us wanting to know more about the many specific communities comprised within the survey, and how they have changed from year over year. For that reason, we\u2019re inviting the Kaggle community to dive deep into the survey datasets and help us tell the diverse stories of data scientists from around the world. The challenge objective: tell a data story about a subset of the data science community represented in this survey, through a combination of both narrative text and data exploration. A \u201cstory\u201d could be defined any number of ways, and that\u2019s deliberate. The challenge is to deeply explore (through data) the impact, priorities, or concerns of a specific group of data science and machine learning practitioners. That group can be defined in the macro (for example: anyone who does most of their coding in Python) or the micro (for example: female data science students studying machine learning in masters programs). This is an opportunity to be creative and tell the story of a community you identify with or are passionate about! Submissions will be evaluated on the following: To be valid, a submission must be contained in one notebook, made public on or before the submission deadline. Participants are free to use any datasets in addition to the Kaggle Data Science survey, but those datasets must also be publicly available on Kaggle by the deadline for a submission to be valid. You can make your submission by filling out the submission form.",
        "dataset_text": "kaggle_survey_2022_responses.csv: 43 questions and 23,997 responses kaggle_survey_2022_answer_choices.pdf: list of answer choices for every question kaggle_survey_2022_methodology.pdf: a description of how the survey was conducted"
    },
    {
        "name": "Humpback Whale Identification",
        "url": "https://www.kaggle.com/competitions/humpback-whale-identification",
        "overview_text": "Overview text not found",
        "description_text": " After centuries of intense whaling, recovering whale populations still have a hard time adapting to warming oceans and struggle to compete every day with the industrial fishing industry for food. To aid whale conservation efforts, scientists use photo surveillance systems to monitor ocean activity. They use the shape of whales\u2019 tails and unique markings found in footage to identify what species of whale they\u2019re analyzing and meticulously log whale pod dynamics and movements. For the past 40 years, most of this work has been done manually by individual scientists, leaving a huge trove of data untapped and underutilized. In this competition, you\u2019re challenged to build an algorithm to identify individual whales in images. You\u2019ll analyze Happywhale\u2019s database of over 25,000 images, gathered from research institutions and public contributors. By contributing, you\u2019ll help to open rich fields of understanding for marine mammal population dynamics around the globe. Note, this competition is similar in nature to this competition with an expanded and updated dataset. We'd like to thank Happywhale for providing this data and problem. Happywhale is a platform that uses image process algorithms to let anyone to submit their whale photo and have it automatically identified.",
        "dataset_text": "This training data contains thousands of images of humpback whale flukes. Individual whales have been identified by researchers and given an Id. The challenge is to predict the whale Id of images in the test set. What makes this such a challenge is that there are only a few examples for each of 3,000+ whale Ids."
    },
    {
        "name": "Google QUEST Q&A Labeling",
        "url": "https://www.kaggle.com/competitions/google-quest-challenge",
        "overview_text": "Overview text not found",
        "description_text": "Computers are really good at answering questions with single, verifiable answers. But, humans are often still better at answering questions about opinions, recommendations, or personal experiences. Humans are better at addressing subjective questions that require a deeper, multidimensional understanding of context - something computers aren't trained to do well\u2026yet.. Questions can take many forms - some have multi-sentence elaborations, others may be simple curiosity or a fully developed problem. They can have multiple intents, or seek advice and opinions. Some may be helpful and others interesting. Some are simple right or wrong.  Unfortunately, it\u2019s hard to build better subjective question-answering algorithms because of a lack of data and predictive models. That\u2019s why the CrowdSource team at Google Research, a group dedicated to advancing NLP and other types of ML science via crowdsourcing, has collected data on a number of these quality scoring aspects. In this competition, you\u2019re challenged to use this new dataset to build predictive algorithms for different subjective aspects of question-answering. The question-answer pairs were gathered from nearly 70 different websites, in a \"common-sense\" fashion. Our raters received minimal guidance and training, and relied largely on their subjective interpretation of the prompts. As such, each prompt was crafted in the most intuitive fashion so that raters could simply use their common-sense to complete the task. By lessening our dependency on complicated and opaque rating guidelines, we hope to increase the re-use value of this data set. What you see is what you get! Demonstrating these subjective labels can be predicted reliably can shine a new light on this research area. Results from this competition will inform the way future intelligent Q&A systems will get built, hopefully contributing to them becoming more human-like.",
        "dataset_text": "The data for this competition includes questions and answers from various StackExchange properties. Your task is to predict target values of 30 labels for each question-answer pair. The list of 30 target labels are the same as the column names in the sample_submission.csv file. Target labels with the prefix question_ relate to the question_title and/or question_body features in the data. Target labels with the prefix answer_ relate to the answer feature. Each row contains a single question and a single answer to that question, along with additional features. The training data contains rows with some duplicated questions (but with different answers). The test data does not contain any duplicated questions. This is not a binary prediction challenge. Target labels are aggregated from multiple raters, and can have continuous values in the range [0,1]. Therefore, predictions must also be in that range. Since this is a synchronous re-run competition, you only have access to the Public test set. For planning purposes, the re-run test set is no larger than 10,000 rows, and less than 8 Mb uncompressed. Additional information about the labels and collection method will be provided by the competition sponsor in the forum."
    },
    {
        "name": "The 2nd YouTube-8M Video Understanding Challenge",
        "url": "https://www.kaggle.com/competitions/youtube8m-2018",
        "overview_text": "Overview text not found",
        "description_text": " The world is generating and consuming an enormous amount of video content. Currently on YouTube, people watch over 1 billion hours of video every single day. To spur advances in analyzing and understanding video, Google AI has publicly released a large-scale video dataset that consists of millions of YouTube video features and associated labels from a diverse vocabulary of 3,700+ visual entities called the YouTube-8M Dataset. Last year, we successfully hosted Google Cloud & YouTube-8M Video Understanding Challenge, with 742 participating teams with 946 individual competitors from 60 countries. This competition is the second Kaggle competition based on YouTube 8M dataset, and is focused on learning video representation under budget constraints. For a lot of video tasks where there are a large number of classes, like recommending new videos or automatic video classification, compact models need to meet memory and computational requirements. This is true even if working in cloud computational environments. Also, compact models make it possible to have limited-memory or catalog indexes on devices in order to do personalized and privacy-preserving computation on user\u2019s personal mobile phones. In this competition, you\u2019re challenged to produce a compact video classification model. Your model size must not exceed 1 GB (this is strictly enforced, through model upload). We encourage participants to train a model that most efficiently uses this budget, rather than ensembles of lots of models. This competition is being hosted by Google AI (previously known as Google Research) as a part of the European Conference on Computer Vision (ECCV) 2018 selected workshop session. Please refer to the YouTube 8M Large-Scale Video Understanding Workshop Page for details about the workshop.",
        "dataset_text": "In this competition, you will predict the labels of a YouTube video. We provide you extracted frame-level and video-level features. The feature data and detailed feature information can be found on the YouTube-8M dataset webpage.  The training dataset in this competition contain videos and labels that are publicly available on YouTube, while the test data is not publicly available. The test data also has anonymized video IDs to ensure the fairness of the competition."
    },
    {
        "name": "ASHRAE - Great Energy Predictor III",
        "url": "https://www.kaggle.com/competitions/ashrae-energy-prediction",
        "overview_text": "Overview text not found",
        "description_text": "Q: How much does it cost to cool a skyscraper in the summer?\nA: A lot! And not just in dollars, but in environmental impact. Thankfully, significant investments are being made to improve building efficiencies to reduce costs and emissions. The question is, are the improvements working? That\u2019s where you come in. Under pay-for-performance financing, the building owner makes payments based on the difference between their real energy consumption and what they would have used without any retrofits. The latter values have to come from a model. Current methods of estimation are fragmented and do not scale well. Some assume a specific meter type or don\u2019t work with different building types. In this competition, you\u2019ll develop accurate models of metered building energy usage in the following areas: chilled water, electric, hot water, and steam meters. The data comes from over 1,000 buildings over a three-year timeframe. With better estimates of these energy-saving investments, large scale investors and financial institutions will be more inclined to invest in this area to enable progress in building efficiencies. About the Host Founded in 1894, ASHRAE serves to advance the arts and sciences of heating, ventilation, air conditioning refrigeration and their allied fields. ASHRAE members represent building system design and industrial process professionals around the world. With over 54,000 members serving in 132 countries, ASHRAE supports research, standards writing, publishing and continuing education - shaping tomorrow\u2019s built environment today. Banner photo by Federico Beccari on Unsplash",
        "dataset_text": "Assessing the value of energy efficiency improvements can be challenging as there's no way to truly know how much energy a building would have used without the improvements. The best we can do is to build counterfactual models. Once a building is overhauled the new (lower) energy consumption is compared against modeled values for the original building to calculate the savings from the retrofit. More accurate models could support better market incentives and enable lower cost financing. This competition challenges you to build these counterfactual models across four energy types based on historic usage rates and observed weather. The dataset includes three years of hourly meter readings from over one thousand buildings at several different sites around the world. Weather data from a meteorological station as close as possible to the site. The submission files use row numbers for ID codes in order to save space on the file uploads. test.csv has no feature data; it exists so you can get your predictions into the correct order. A valid sample submission."
    },
    {
        "name": "Quick, Draw! Doodle Recognition Challenge",
        "url": "https://www.kaggle.com/competitions/quickdraw-doodle-recognition",
        "overview_text": "Overview text not found",
        "description_text": " \"Quick, Draw!\" was released as an experimental game to educate the public in a playful way about how AI works. The game prompts users to draw an image depicting a certain category, such as \u201dbanana,\u201d \u201ctable,\u201d etc. The game generated more than 1B drawings, of which a subset was publicly released as the basis for this competition\u2019s training set. That subset contains 50M drawings encompassing 340 label categories. Sounds fun, right? Here's the challenge: since the training data comes from the game itself, drawings can be incomplete or may not match the label. You\u2019ll need to build a recognizer that can effectively learn from this noisy data and perform well on a manually-labeled test set from a different distribution. Your task is to build a better classifier for the existing Quick, Draw! dataset. By advancing models on this dataset, Kagglers can improve pattern recognition solutions more broadly. This will have an immediate impact on handwriting recognition and its robust applications in areas including OCR (Optical Character Recognition), ASR (Automatic Speech Recognition) & NLP (Natural Language Processing).",
        "dataset_text": "The Quick Draw Dataset is a collection of millions of drawings across 300+ categories, contributed by players of Quick, Draw! The drawings were captured as timestamped vectors, tagged with metadata including what the player was asked to draw and in which country the player was located. Two versions of the data are given. The raw data is the exact input recorded from the user drawing, while the simplified version removes unnecessary points from the vector information. (For example, a straight line may have been recorded with 8 points, but since you only need 2 points to uniquely identify a line, 6 points can be dropped.) The simplified files are much smaller and provide effectively the same information. For this competition, you may use the raw files, the simplified files, or both. You can find out more details about the drawing format on the quickdraw-dataset github page. Your models should predict the correct \"word\" of the drawing. IMPORTANT: Some \"words\" are actually more than one word! The training data aligns to the Quick Draw dataset that that was previously released, and uses spaces to delimit multi-word labels. The Kaggle metric for this competition requires labels with no spaces, so you will need to adjust your label predictions to replace spaces with underscores. For example, \"roller coaster\" should be predicted as \"roller_coaster\". You may predict up to 3 guesses per drawing. See the Evaluation page for the correct format."
    },
    {
        "name": "PLAsTiCC Astronomical Classification",
        "url": "https://www.kaggle.com/competitions/PLAsTiCC-2018",
        "overview_text": "Overview text not found",
        "description_text": " Help some of the world's leading astronomers grasp the deepest properties of the universe. The human eye has been the arbiter for the classification of astronomical sources in the night sky for hundreds of years. But a new facility -- the Large Synoptic Survey Telescope (LSST) -- is about to revolutionize the field, discovering 10 to 100 times more astronomical sources that vary in the night sky than we've ever known. Some of these sources will be completely unprecedented! The Photometric LSST Astronomical Time-Series Classification Challenge (PLAsTiCC) asks Kagglers to help prepare to classify the data from this new survey. Competitors will classify astronomical sources that vary with time into different classes, scaling from a small training set to a very large test set of the type the LSST will discover. More background information is available here.   PLAsTiCC is funded through LSST Corporation Grant Award # 2017-03 and administered by the University of Toronto. Financial support for LSST comes from the National Science Foundation (NSF) through Cooperative Agreement No. 1258333, the Department of Energy (DOE) Office of Science under Contract No. DE-AC02-76SF00515, and private funding raised by the LSST Corporation. The NSF-funded LSST Project Office for construction was established as an operating center under management of the Association of Universities for Research in Astronomy (AURA). The DOE-funded effort to build the LSST camera is managed by the SLAC National Accelerator Laboratory (SLAC). The National Science Foundation (NSF) is an independent federal agency created by Congress in 1950 to promote the progress of science. NSF supports basic research and people to create knowledge that transforms the future. Photo Credit: M. Park/Inigo Films/LSST/AURA/NSF",
        "dataset_text": "A few caveats about the light-curve data are as follows:"
    },
    {
        "name": "VSB Power Line Fault Detection",
        "url": "https://www.kaggle.com/competitions/vsb-power-line-fault-detection",
        "overview_text": "Overview text not found",
        "description_text": " Medium voltage overhead power lines run for hundreds of miles to supply power to cities. These great distances make it expensive to manually inspect the lines for damage that doesn't immediately lead to a power outage, such as a tree branch hitting the line or a flaw in the insulator. These modes of damage lead to a phenomenon known as partial discharge \u2014 an electrical discharge which does not bridge the electrodes between an insulation system completely. Partial discharges slowly damage the power line, so left unrepaired they will eventually lead to a power outage or start a fire. Your challenge is to detect partial discharge patterns in signals acquired from these power lines with a new meter designed at the ENET Centre at V\u0160B. Effective classifiers using this data will make it possible to continuously monitor power lines for faults. ENET Centre researches and develops renewable energy resources with the goal of reducing or eliminating harmful environmental impacts. Their efforts focus on developing technology solutions around transportation and processing of energy raw materials. By developing a solution to detect partial discharge you\u2019ll help reduce maintenance costs, and prevent power outages.",
        "dataset_text": "Faults in electric transmission lines can lead to a destructive phenomenon called partial discharge.\nIf left alone, partial discharges can damage equipment to the point that it stops functioning entirely. Your challenge is to detect partial discharges so that repairs can be made before any lasting harm occurs. Each signal contains 800,000 measurements of a power line's voltage, taken over 20 milliseconds. As the underlying electric grid operates at 50 Hz, this means each signal covers a single complete grid cycle. The grid itself operates on a 3-phase power scheme, and all three phases are measured simultaneously."
    },
    {
        "name": "PetFinder.my Adoption Prediction",
        "url": "https://www.kaggle.com/competitions/petfinder-adoption-prediction",
        "overview_text": "Overview text not found",
        "description_text": " Millions of stray animals suffer on the streets or are euthanized in shelters every day around the world. If homes can be found for them, many precious lives can be saved \u2014 and more happy families created. PetFinder.my has been Malaysia\u2019s leading animal welfare platform since 2008, with a database of more than 150,000 animals. PetFinder collaborates closely with animal lovers, media, corporations, and global organizations to improve animal welfare. Animal adoption rates are strongly correlated to the metadata associated with their online profiles, such as descriptive text and photo characteristics. As one example, PetFinder is currently experimenting with a simple AI tool called the Cuteness Meter, which ranks how cute a pet is based on qualities present in their photos. In this competition you will be developing algorithms to predict the adoptability of pets - specifically, how quickly is a pet adopted? If successful, they will be adapted into AI tools that will guide shelters and rescuers around the world on improving their pet profiles' appeal, reducing animal suffering and euthanization. Top participants may be invited to collaborate on implementing their solutions into AI tools for assessing and improving pet adoption performance, which will benefit global animal welfare. Be aware that this is being run as a Kernels Only Competition, requiring that all submissions be made via a Kernel output. Photo by Krista Mangulsone on Unsplash",
        "dataset_text": "In this competition you will predict the speed at which a pet is adopted, based on the pet\u2019s listing on PetFinder. Sometimes a profile represents a group of pets. In this case, the speed of adoption is determined by the speed at which all of the pets are adopted. The data included text, tabular, and image data. See below for details.\nThis is a Kernels-only competition. At the end of the competition, test data will be replaced in their entirety with new data of approximately the same size, and your kernels will be rerun on the new data. Contestants are required to predict this value. The value is determined by how quickly, if at all, a pet is adopted. The values are determined in the following way:\n0 - Pet was adopted on the same day as it was listed.\n1 - Pet was adopted between 1 and 7 days (1st week) after being listed.\n2 - Pet was adopted between 8 and 30 days (1st month) after being listed.\n3 - Pet was adopted between 31 and 90 days (2nd & 3rd month) after being listed.\n4 - No adoption after 100 days of being listed. (There are no pets in this dataset that waited between 90 and 100 days). For pets that have photos, they will be named in the format of PetID-ImageNumber.jpg. Image 1 is the profile (default) photo set for the pet. For privacy purposes, faces, phone numbers and emails have been masked. We have run the images through Google's Vision API, providing analysis on Face Annotation, Label Annotation, Text Annotation and Image Properties. You may optionally utilize this supplementary information for your image analysis. File name format is PetID-ImageNumber.json. Some properties will not exist in JSON file if not present, i.e. Face Annotation. Text Annotation has been simplified to just 1 entry of the entire text description (instead of the detailed JSON result broken down by individual characters and words). Phone numbers and emails are already anonymized in Text Annotation. Google Vision API reference:\nhttps://cloud.google.com/vision/docs/reference/rest/v1/images/annotate We have run each pet profile's description through Google's Natural Language API, providing analysis on sentiment and key entities. You may optionally utilize this supplementary information for your pet description analysis. There are some descriptions that the API could not analyze. As such, there are fewer sentiment files than there are rows in the dataset. File name format is PetID.json. Google Natural Language API reference:\nhttps://cloud.google.com/natural-language/docs/basics In the second stage of the competition, we will re-run your selected Kernels. The following files will be swapped with new data: In stage 2, all data will be replaced with approximately the same amount of different data. The stage 1 test data will not be available when kernels are rerun in stage 2."
    },
    {
        "name": "Traveling Santa 2018 - Prime Paths",
        "url": "https://www.kaggle.com/competitions/traveling-santa-2018-prime-paths",
        "overview_text": "Overview text not found",
        "description_text": "Rudolph the red-nosed reindeer\nHad some very tired hooves\nBut he had a job to finish\nCould he do it with the shortest moves? All of the other reindeer\nUsed to laugh and mock his code\nThey always said poor Rudolph\nCouldn't handle the workload Then one foggy Christmas Eve\nSanta came to say\nI see you've taken number theory\nPlease make this night a bit less dreary? Then how the reindeer loved him\nand each enrolled in an AI degree\nRudolph the red-nosed reindeer\nWe get to go to bed early! Rudolph has always believed in working smarter, not harder. And what better way to earn the respect of Comet and Blitzen than showing the initiative to improve Santa's annual route for delivering toys on Christmas Eve? This year, Rudolph believes he can motivate the overworked Reindeer team by wisely choosing the order in which they visit the houses on Santa's list. The houses in prime cities always leave carrots for the Reindeers alongside the usual cookies and milk. These carrots are just the sustenance the Reindeers need to keep pace. In fact, Rudolph has found that if the Reindeer team doesn't originate from a prime city exactly every 10th step, it takes the 10% longer than it normally would to make their next destination! Can you help Rudolph solve the Traveling Santa problem subject to his carrot constraint? His team--and Santa--are counting on you! Reindeer Photo: Norman Tsui\nStocking Photo: Wesley Tingey",
        "dataset_text": "You are provided a list of cities and their coordinates in cities.csv. You must create the shortest possible path that visits all the cities. Your submission file is simply the ordered list in which you visit each city. Paths have the following constraints:"
    },
    {
        "name": "Quora Insincere Questions Classification",
        "url": "https://www.kaggle.com/competitions/quora-insincere-questions-classification",
        "overview_text": "Overview text not found",
        "description_text": " An existential problem for any major website today is how to handle toxic and divisive content. Quora wants to tackle this problem head-on to keep their platform a place where users can feel safe sharing their knowledge with the world. Quora is a platform that empowers people to learn from each other. On Quora, people can ask questions and connect with others who contribute unique insights and quality answers. A key challenge is to weed out insincere questions -- those founded upon false premises, or that intend to make a statement rather than look for helpful answers. In this competition, Kagglers will develop models that identify and flag insincere questions. To date, Quora has employed both machine learning and manual review to address this problem. With your help, they can develop more scalable methods to detect toxic and misleading content. Here's your chance to combat online trolls at scale. Help Quora uphold their policy of \u201cBe Nice, Be Respectful\u201d and continue to be a place for sharing and growing the world\u2019s knowledge. Be aware that this is being run as a Kernels Only Competition, requiring that all submissions be made via a Kernel output. Please read the Kernels FAQ and the data page very carefully to fully understand how this is designed.",
        "dataset_text": "In this competition you will be predicting whether a question asked on Quora is sincere or not. An insincere question is defined as a question intended to make a statement rather than look for helpful answers. Some characteristics that can signify that a question is insincere: The training data includes the question that was asked, and whether it was identified as insincere (target = 1). The ground-truth labels contain some amount of noise: they are not guaranteed to be perfect. Note that the distribution of questions in the dataset should not be taken to be representative of the distribution of questions asked on Quora. This is, in part, because of the combination of sampling procedures and sanitization measures that have been applied to the final dataset. This is a Kernels-only competition. The files in this Data section are downloadable for reference in Stage 1. Stage 2 files will only be available in Kernels and not available for download. In the second stage of the competition, we will re-run your selected Kernels. The following files will be swapped with new data: External data sources are not allowed for this competition. We are, though, providing a number of word embeddings along with the dataset that can be used in the models. These are as follows:"
    },
    {
        "name": "Google Cloud & NCAA\u00ae ML Competition 2019-Men's",
        "url": "https://www.kaggle.com/competitions/mens-machine-learning-competition-2019",
        "overview_text": "Overview text not found",
        "description_text": "As a result of the continued collaboration between Google Cloud and the NCAA, the sixth annual Kaggle-backed March Madness competition is underway! Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. Kagglers will join the millions of fans who attempt to forecast the outcomes of March Madness during this year's NCAA Division I Men\u2019s and Women\u2019s Basketball Championships. But unlike most fans, you will pick your bracket using a combination of NCAA\u2019s historical data and your computing power, while the ground truth unfolds on national television.  In the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset. In the second stage, competitors will forecast outcomes of all possible matchups in the 2019 NCAA Division I Men\u2019s and Women\u2019s Basketball Championships. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2019 results. As the official public cloud provider of the NCAA, Google is proud to provide a competition to help participants strengthen their knowledge of basketball, statistics, data modeling, and cloud technology. As part of its journey to the cloud, the NCAA has migrated 80+ years of historical and play-by-play data, from 90 championships and 24 sports, to Google Cloud Platform (GCP). The NCAA has tapped into decades of historical basketball data using BigQuery, Cloud Spanner, Datalab, Cloud Machine Learning and Cloud Dataflow, to power the analysis of team and player performance. The mission of the NCAA has long been about serving the needs of schools, their teams and students. Google Cloud is proud to support that mission by helping the NCAA use data and machine learning to better engage with its millions of fans, 500,000 student-athletes, and more than 19,000 teams. Game on! This page is for the NCAA Division I Men's tournament. Check out the NCAA Division I Women's tournament here.",
        "dataset_text": "Note for Stage 2: All of the data files have been updated through the end of the current regular season. You can find everything you need in these four files: 1) Stage2DataFiles.zip (this contains the same type of information as the DataFiles.zip did for Stage 1, but it now includes 2019 data as well)\n\n2) MasseyOrdinals_thru_2019_day_128.zip (this contains the same type of information as MasseyOrdinals.zip did for Stage 1, but it now includes 2019 data as well). For the absolute latest version of the Massey Ordinals, see the Discussion thread \"Massey Ordinals Day 133 Thread\".\n\n3) PlayByPlay_2019.zip (this contains the same type of information as PlayByPlay_2018.zip, etc., but it is play-by-play for the current 2019 season games)\n\n4) SampleSubmissionStage2.csv (this has the proper number of rows, and the proper teams for the 2019 tourney only, which is all you predict in Stage 2)  You can just disregard the Stage 1 files and the Prelim files at this point - they are completely superseded by the above release. The only exceptions are that the play-by-play data for earlier years (PlayByPlay_2010, PlayByPlay_2011, \u2026, PlayByPlay_2018) is still useful, and there will be ongoing releases of the latest Massey Ordinals as they become available. ------------ Each season there are thousands of NCAA basketball games played between Division I men's teams, culminating in March Madness\u00ae, the 68-team national championship that starts in the middle of March. We have provided a large amount of historical data about college basketball games and teams, going back many years. Armed with this historical data, you can explore it and develop your own distinctive ways of predicting March Madness\u00ae game outcomes. You can even evaluate and compare different approaches by seeing which of them would have done best at predicting tournament games from the past. If you are unfamiliar with the format and intricacies of the NCAA\u00ae tournament, we encourage reading the wikipedia page before diving into the data.  The data description and schema may seem daunting at first, but is not as complicated as it appears. As a reminder, you are encouraged to incorporate your own sources of data. We have provided extensive historical data to jump-start the modeling process, and this data is self-consistent (for instance, dates and team ID's are always treated the same way). Nevertheless, you may also be able to make productive use of external data. If you head down this path, please be forewarned that many sources have their own distinctive way of identifying the names of teams, and this can make it challenging to link up with our data. The TeamSpellings file, which is listed in the bottom section below, may help you map external team references into our own Team ID structure, and you may also need to understand exactly how dates work in our data. Please also note that we have standardized the spelling of column names and some filenames, so if you are re-using code from previous instances of this contest, you may need to adjust for this. For example, we are universally referencing Team ID columns with a spelling of \"TeamID\" rather than \"team_id\". We extend our gratitude to Kenneth Massey for providing much of the historical data. Special Acknowledgment to Jeff Sonas of Sonas Consulting for his support in assembling the dataset for this competition. Stage 1 - You should submit predicted probabilities for every possible matchup in the past 5 NCAA\u00ae tournaments (2014-2018). Stage 2 - You should submit predicted probabilities for every possible matchup before the 2019 tournament begins. Refer to the Timeline page for specific dates. In both stages, the sample submission will tell you which games to predict. Below we describe the format and fields of the contest data files. The data will likely be refreshed once in late February while Stage 1 of the competition is running. Many of the files are only complete through the end of last season. At the start of Stage 2, we will provide updates to these files to incorporate data from the current season. This section provides everything you need to build a simple prediction model and submit predictions. Special note about \"Season\" numbers: the college basketball season lasts from early November until the national championship tournament that starts in the middle of March. For instance, this year the first men\u2019s Division I games were played on November 6th, 2018 and the men\u2019s national championship game will be played on April 8th, 2019. Because a basketball season spans two calendar years like this, it can be confusing to refer to the year of the season. By convention, when we identify a particular season, we will reference the year that the season ends in, not the year that it starts in. So for instance, the current season will be identified in our data as the 2019 season, not the 2018 season or the 2018-19 season or the 2018-2019 season, though you may see any of these in everyday use outside of our data. Data Section 1 file: Teams.csv This file identifies the different college teams present in the dataset. Each school is uniquely identified by a 4 digit id number. You will not see games present for all teams in all seasons, because the games listing is only for matchups where both teams are Division-I teams. There are 353 teams currently in Division-I, and an overall total of 366 teams in our team listing (each year, some teams might start being Division-I programs, and others might stop being Division-I programs). This year there are two teams that are new to Division I: Cal Baptist (TeamID=1465) and North Alabama (TeamID=1466), and so you will not see any historical data for these teams prior to the current season. Data Section 1 file: Seasons.csv This file identifies the different seasons included in the historical data, along with certain season-level properties. Data Section 1 file: NCAATourneySeeds.csv This file identifies the seeds for all teams in each NCAA\u00ae tournament, for all seasons of historical data. Thus, there are between 64-68 rows for each year, depending on whether there were any play-in games and how many there were. In recent years the structure has settled at 68 total teams, with four \"play-in\" games leading to the final field of 64 teams entering Round 1 on Thursday of the first week. We will not know the seeds of the respective tournament teams, or even exactly which 68 teams it will be, until Selection Sunday on March 17, 2019. Data Section 1 file: RegularSeasonCompactResults.csv This file identifies the game-by-game results for many seasons of historical data, starting with the 1985 season (the first year the NCAA\u00ae had a 64-team tournament). For each season, the file includes all games played from daynum 0 through 132. It is important to realize that the \"Regular Season\" games are simply defined to be all games played on DayNum=132 or earlier (DayNum=132 is Selection Sunday, and there are always a few conference tournament finals actually played early in the day on Selection Sunday itself). Thus a game played on or before Selection Sunday will show up here whether it was a pre-season tournament, a non-conference game, a regular conference game, a conference tournament game, or whatever. Data Section 1 file: NCAATourneyCompactResults.csv This file identifies the game-by-game NCAA\u00ae tournament results for all seasons of historical data. The data is formatted exactly like the RegularSeasonCompactResults data. Note that these games also include the play-in games (which always occurred on day 134/135) for those years that had play-in games. Thus each season you will see between 63 and 67 games listed, depending on how many play-in games there were. Because of the consistent structure of the NCAA\u00ae tournament schedule, you can actually tell what round a game was, depending on the exact DayNum. Thus: Special note: Each year, there are also going to be other games that happened after Selection Sunday, which are not part of the NCAA\u00ae Tournament. This includes tournaments like the postseason NIT, the CBI, the CIT, and the Vegas 16. Such games are not listed in the Regular Season or the NCAA\u00ae Tourney files; they can be found in the \"Secondary Tourney\" data files within Data Section 6. Although they would not be games you would ever be predicting directly for the NCAA\u00ae tournament, and they would not be games you would have data from at the time of predicting NCAA\u00ae tournament outcomes, you may nevertheless wish to make use of these games for model optimization, depending on your methodology. The more games that you can test your predictions against, the better your optimized model might eventually become, depending on how applicable all those games are. A similar argument might be advanced in favor of optimizing your predictions against conference tournament games, which might be viewed as fairly similar to NCAA\u00ae tournament games.  Data Section 1 file: SampleSubmissionStage1.csv This file illustrates the submission file format for Stage 1. It is the simplest possible submission: a 50% winning percentage is predicted for each possible matchup. A submission file lists every possible matchup between tournament teams for one or more years. During Stage 1, you are asked to make predictions for all possible matchups from the past five NCAA\u00ae tournaments (seasons 2014, 2015, 2016, 2017, 2018). In Stage 2, you will be asked to make predictions for all possible matchups from the current NCAA\u00ae tournament (season 2019). When there are 68 teams in the tournament, there are 68*67/2=2,278 predictions to make for that year, so a Stage 1 submission file will have 2,278*5=11,390 data rows. Example #1: You want to make a prediction for Duke (TeamID=1181) against Arizona (TeamID=1112) in the 2012 tournament, with Duke given a 53% chance to win and Arizona given a 47% chance to win. In this case, Arizona has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Arizona's perspective (47%): 2012_1112_1181,0.47 Example #2: You want to make a prediction for Duke (TeamID=1181) against North Carolina (TeamID=1314) in the 2012 tournament, with Duke given a 51.6% chance to win and North Carolina given a 48.4% chance to win. In this case, Duke has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Duke's perspective (51.6%): 2012_1181_1314,0.516 This section provides game-by-game stats at a team level (free throws attempted, defensive rebounds, turnovers, etc.) for all regular season, conference tournament, and NCAA\u00ae tournament games since the 2002-03 season.  Team Box Scores are provided in \"Detailed Results\" files rather than \"Compact Results\" files. However, the two files are strongly related. In a Detailed Results file, the first eight columns (Season, DayNum, WTeamID, WScore, LTeamID, LScore, WLoc, and NumOT) are exactly the same as a Compact Results file. However, in a Detailed Results file, there are many additional columns. The column names should be self-explanatory to basketball fans (as above, \"W\" or \"L\" refers to the winning or losing team): (and then the same set of stats from the perspective of the losing team: LFGM is the number of field goals made by the losing team, and so on up to LPF). Note: by convention, \"field goals made\" (either WFGM or LFGM) refers to the total number of fields goals made by a team, a combination of both two-point field goals and three-point field goals. And \"three point field goals made\" (either WFGM3 or LFGM3) is just the three-point fields goals made, of course. So if you want to know specifically about two-point field goals, you have to subtract one from the other (e.g., WFGM - WFGM3). And the total number of points scored is most simply expressed as 2*FGM + FGM3 + FTM. Data Section 2 file: RegularSeasonDetailedResults.csv This file provides team-level box scores for many regular seasons of historical data, starting with the 2003 season. All games listed in the RegularSeasonCompactResults file since the 2003 season should exactly be present in the RegularSeasonDetailedResults file. Data Section 2 file: NCAATourneyDetailedResults.csv This file provides team-level box scores for many NCAA\u00ae tournaments, starting with the 2003 season. All games listed in the NCAATourneyCompactResults file since the 2003 season should exactly be present in the NCAATourneyDetailedResults file. This section provides city locations of all regular season, conference tournament, and NCAA\u00ae tournament games since the 2009-10 season Data Section 3 file: Cities.csv This file provides a master list of cities that have been locations for games played. Data Section 3 file: GameCities.csv This file identifies all games, starting with the 2010 season, along with the city that the game was played in. Games from the regular season, the NCAA\u00ae tourney, and other post-season tournaments, are all listed together. This section provides weekly team rankings for dozens of top rating systems - Pomeroy, Sagarin, RPI, ESPN, etc., since the 2002-2003 season Data Section 4 file: MasseyOrdinals.zip containing MasseyOrdinals.csv This zip file contains a large CSV file, listing out rankings (e.g. #1, #2, #3, ..., #N) of teams going back to the 2002-2003 season, under a large number of different ranking system methodologies. The information was gathered by Kenneth Massey and provided on his College Basketball Ranking Composite page. Note that a rating system is more precise than a ranking system, because a rating system can provide insight about the strength gap between two adjacently-ranked teams. A ranking system will just tell you who is #1 or who is #2, but a rating system might tell you whether the gap between #1 and #2 is large or small. Nevertheless, it can be hard to compare two different rating systems that are expressed in different scales, so it can be very useful to express all the systems in terms of their ordinal ranking (1, 2, 3, ..., N) of teams. Disclaimer: you ought to be careful about your methodology when using or evaluating these ranking systems. They are presented on a weekly basis, and given a consistent date on the Massey Composite page that typically is a Sunday; that is how the ranking systems can be compared against each other on this page. However, these systems each follow their own timeline and some systems may be released on a Sunday and others on a Saturday or Monday or even Tuesday. You should remember that if a ranking is released on a Tuesday, and was calculated based on games played through Monday, it will make the system look unusually good at predicting if you use that system to forecast the very games played on Monday that already inform the rankings. To avoid this methodological trap, we have typically used a conservative RankingDayNum of Wednesday to represent the rankings that were released at approximately the end of the weekend, a few days before, even though those rankings are represented on the composite page as being on a Sunday. For some of the older years, a more precise timestamp was known for each ranking system that allowed a more precise assignment of a RankingDayNum. By convention, the final pre-tournament rankings are always expressed as RankingDayNum=133, even though sometimes the rankings for individual systems are not released until Tuesday (DayNum=134) or even Wednesday or Thursday. If you decide to use some rankings from these Massey Ordinals to inform your predictions, be forewarned that we have no control over when they are released, and not all systems may turn out to be available in time to make pre-tournament predictions by our submission deadline. In such a situation, you may wish to use the rankings from DayNum=128 or you may need to dig into the details of the actual source of the rankings, by following the respective links on the Massey Composite Page. We may also be able to provide partial releases of the final pre-tournament Massey Ordinals on the forums, so that as systems come in on Monday or Tuesday you can use them right away. This section provides play-by-play event logs for more than 99.5% of each year's regular season, NCAA\u00ae tournament, and secondary tournament games since the 2014-15 season - including plays by individual players. This year we are transitioning to a different play-by-play source, which includes data since the 2014-2015 season rather than since the 2009-2010 season (that's what we had previously for men's data). However, we are now able to provide play-by-play for both men's and women's data, and there is locational play-by-play detail starting with games from the 2018-2019 season. This includes an X/Y location (ranging from 0 to 100 in each dimension) on the court for each shot attempt, turnover, and foul for many games, as well as an overall categorization of the area on the court that the shot or turnover or foul occurred in (inside left wing, outside right wing, under the basket, etc.) Some games in these recent seasons still lack the locational detail. The data from last year (2019 season) matches what you can expect for the current year (2020 season) as we approach the postseason. Despite the 99.5% coverage, there are still a few games missing annually, and we will try to bring those in as well, if possible. Data Section 5 file: MEvents2015.csv, MEvents2016.csv, MEvent2017.csv, MEvents2018.csv, MEvents2019.csv Each MEvents file lists the play-by-play event logs for more than 99.5% of games from that season. Each event is assigned to either a team or a single one of the team's players. Thus if a basket is made by one player and an assist is credited to a second player, that would show up as two separate records. The players are listed by PlayerID within the MPlayers.csv file. Event Types and Subtypes:   Data Section 5 file: MPlayers.csv Note: there are data collection errors within the events, in that they don't necessarily add up to the final stats for the game. In addition, the player name spellings may vary over the course of a season or career for the same player, which would lead to the same player showing up with different PlayerID values. Nevertheless, this was the highest quality data we could manage for play-by-play with the near-complete set of games. This section contains additional supporting information, including coaches, conference affiliations, alternative team name spellings, bracket structure, game results for NIT and other postseason tournaments Data Section 6 file: TeamCoaches.csv This file indicates the head coach for each team in each season, including a start/finish range of DayNums to indicate a mid-season coaching change. For scenarios where a team had the same head coach the entire season, they will be listed with a DayNum range of 0 to 154 for that season. For head coaches whose term lasted many seasons, there will be many rows listed, most of which have a DayNum range of 0 to 154 for the corresponding season. Data Section 6 file: Conferences.csv This file indicates the Division I conferences that have existed over the years since 1985. Each conference is listed with an abbreviation and a longer name. There has been no attempt to link up conferences who merged with other conferences, or whose names changed over time. Thus you will see, for instance, a \"Pacific-10\" conference up through the 2011 season, and then a \"Pacific-12\" conference starting in the 2012 season, and these look like different conferences in the data, even though it was still mostly the same teams. Data Section 6 file: TeamConferences.csv This file indicates the conference affiliations for each team during each season. Some conferences have changed their names from year to year, and/or changed which teams are part of the conference. This file tracks this information historically. Data Section 6 file: ConferenceTourneyGames.csv This file indicates which games were part of each year's post-season conference tournaments (all of which finished on Selection Sunday or earlier), starting from the 2001 season. Many of these conference tournament games are held on neutral sites, and many of the games are played by tournament-caliber teams just a few days before the NCAA\u00ae tournament. Thus these games could be considered as very similar to NCAA\u00ae tournament games, and (depending on your methodology) may be of use in optimizing your predictions. However, this is NOT a new listing of games; these games are already present within the RegularSeasonCompactResults and RegularSeasonDetailedResults files. So this file simply helps you to identify which of the \"regular season\" games since the 2001 season were actually conference tournament games, in case that is useful information. Data Section 6 file: SecondaryTourneyTeams.csv This file identifies the teams that participated in post-season tournaments other than the NCAA\u00ae Tournament (such events would run in parallel with the NCAA\u00ae Tournament). These are teams that were not invited to the NCAA\u00ae Tournament and instead were invited to some other tournament, of which the NIT is the most prominent tournament, but there have also been the CBI, CIT, and Vegas 16 (V16) at various points in recent years. Depending on your methodology, you might find it useful to have these additional game results, above and beyond what is available from the NCAA\u00ae Tournament results. Many of these teams, especially in the NIT, were \"bubble\" teams of comparable strength to several NCAA\u00ae Tournament invitees, and so these games may be of use in model optimization for predicting NCAA\u00ae Tournament results. Also note that this information could be determined just from inspecting the Secondary Tourney Compact Results file, but is presented in this file as well, for your convenience. Data Section 6 file: SecondaryTourneyCompactResults.csv This file indicates the final scores for the tournament games of \"secondary\" post-season tournaments: the NIT, CBI, CIT, and Vegas 16. The detailed results (team box scores) have not been assembled for these games. For the most part, this file is exactly like other Compact Results listings, although it also has a column for Secondary Tourney. Data Section 6 file: TeamSpellings.csv This file indicates alternative spellings of many team names. It is intended for use in associating external spellings against our own TeamID numbers, thereby helping to relate the external data properly with our datasets. Over the years we have identified various external spellings of different team names (as an example, for Ball State we have seen \"ball st\", and \"ball st.\", and \"ball state\", and \"ball-st\", and \"ball-state\"). Other teams have had more significant changes to their names over the years; for example, \"Texas Pan-American\" and \"Texas-Rio Grande Valley\" are actually the same school. The current list is obviously not exhaustive, and we encourage participants to identify additional mappings and upload extended versions of this file to the forums. Data Section 6 file: NCAATourneySlots This file identifies the mechanism by which teams are paired against each other, depending upon their seeds, as the tournament proceeds through its rounds. It can be of use in identifying, for a given historical game, what round it occurred in, and what the seeds/slots were for the two teams (the meaning of \"slots\" is described below). Because of the existence of play-in games for particular seed numbers, the pairings have small differences from year to year. You may need to know these specifics if you are trying to represent/simulate the exact workings of the tournament bracket. Data Section 6 file: NCAATourneySeedRoundSlots.csv This file helps to represent the bracket structure in any given year. No matter where the play-in seeds are located, we can always know, for a given tournament seed, exactly what bracket slot they would be playing in, on each possible game round, and what the possible DayNum values would be for that round. Thus, if we know when a historical game was played, and what the team's seed was, we can identify the slot for that game. This can be useful in representing or simulating the tournament bracket structure."
    },
    {
        "name": "Google Cloud & NCAA\u00ae ML Competition 2019-Women's",
        "url": "https://www.kaggle.com/competitions/womens-machine-learning-competition-2019",
        "overview_text": "Overview text not found",
        "description_text": "As a result of the continued collaboration between Google Cloud and the NCAA\u00ae, the sixth annual Kaggle-backed March Madness competition is underway! Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. Kagglers will join the millions of fans who attempt to forecast the outcomes of March Madness during this year's NCAA Division I Men\u2019s and Women\u2019s Basketball Championships. But unlike most fans, you will pick your bracket using a combination of NCAA\u2019s historical data and your computing power, while the ground truth unfolds on national television.  In the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset. In the second stage, competitors will forecast outcomes of all possible matchups in the 2019 NCAA Division I Men\u2019s and Women\u2019s Basketball Championships. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2019 results. As the official public cloud provider of the NCAA, Google Cloud is proud to provide a competition to help participants strengthen their knowledge of basketball, statistics, data modeling, and cloud technology. As part of its journey to the cloud, the NCAA has migrated 80+ years of historical and play-by-play data, from 90 championships and 24 sports, to Google Cloud Platform (GCP). The NCAA has tapped into decades of historical basketball data using BigQuery, Cloud Spanner, Datalab, Cloud Machine Learning and Cloud Dataflow, to power the analysis of team and player performance. The mission of the NCAA has long been about serving the needs of schools, their teams and students. Google Cloud is proud to support that mission by helping the NCAA use data and machine learning to better engage with its millions of fans, 500,000 student-athletes and more than 19,000 teams. Game on! This page is for the NCAA Division I Women's tournament. Check out the NCAA Division I Men's tournament here.",
        "dataset_text": "Note for Stage 2: All of the data files have been updated through the end of the current regular season. You can find everything you need in these two files: 1) Stage2WDataFiles.zip (this contains the same type of information as the WDataFiles.zip did for Stage 1, but it now includes 2019 data as well)\n\n2) WSampleSubmissionStage2.csv (this has the proper number of rows, and the proper teams for the 2019 tourney only, which is all you predict in Stage 2) Each season there are thousands of NCAA\u00ae basketball games played between Division I women's teams, culminating in March Madness, the 64-team national championship that starts in the middle of March. We have provided a large amount of historical data about college basketball games and teams, going back many years. Armed with this historical data, you can explore it and develop your own distinctive ways of predicting March Madness\u00ae game outcomes. You can even evaluate and compare different approaches by seeing which of them would have done best at predicting tournament games from the past. If you are unfamiliar with the format and intricacies of the tournament, we encourage reading the wikipedia page before diving into the data. The data description and schema may seem daunting at first, but is not as complicated as it appears. As a reminder, you are encouraged to incorporate your own sources of data. We have provided extensive historical data to jump-start the modeling process, and this data is self-consistent (for instance, dates and team ID's are always treated the same way). Nevertheless, you may also be able to make productive use of external data. If you head down this path, please be forewarned that many sources have their own distinctive way of identifying the names of teams, and this can make it challenging to link up with our data. The WTeamSpellings file, which is listed in the bottom section below, may help you map external team references into our own Team ID structure, and you may also need to understand exactly how dates work in our data. Please also note that we have standardized the spelling of column names and some filenames, and we have assigned a \"W\" prefix to all files pertinent to women's college basketball, so if you are re-using code from the men's contest, you may need to adjust for this. We extend our gratitude to Kenneth Massey for providing much of the historical data. Special Acknowledgment to Jeff Sonas of Sonas Consulting for his support in assembling the dataset for this competition. Stage 1 - You should submit predicted probabilities for every possible matchup in the past 5 NCAA\u00ae tournaments (2014-2018). Stage 2 - You should submit predicted probabilities for every possible matchup before the 2019 tournament begins. Refer to the Timeline page for specific dates. In both stages, the sample submission will tell you which games to predict. Below we describe the format and fields of the contest data files. The data will likely be refreshed once in late February while Stage 1 of the competition is running. Many of the files are only complete through the end of last season. At the start of Stage 2, we will provide updates to these files to incorporate data from the current season. This section provides everything you need to build a simple prediction model and submit predictions. Special note about \"Season\" numbers: the college basketball season lasts from early November until the national championship tournament that starts in the middle of March. For instance, this year the first women\u2019s Division I games were played on November 8th, 2018 and the women\u2019s national championship game will be played on April 7th, 2019. Because a basketball season spans two calendar years like this, it can be confusing to refer to the year of the season. By convention, when we identify a particular season, we will reference the year that the season ends in, not the year that it starts in. So for instance, the current season will be identified in our data as the 2019 season, not the 2018 season or the 2018-19 season or the 2018-2019 season, though you may see any of these in everyday use outside of our data. Data Section 1 file: WTeams.csv This file identifies the different college teams present in the dataset. Each school is uniquely identified by a 4 digit id number. You will not see games present for all teams in all seasons, because the games listing is only for matchups where both teams are Division-I teams. There are 353 teams currently in Division-I, and an overall total of 366 teams in our team listing (each year, some teams might start being Division-I programs, and others might stop being Division-I programs). This year there are two teams that are new to Division I: Cal Baptist (TeamID=3465) and North Alabama (TeamID=3466), and so you will not see any historical data for these teams prior to the current season. Data Section 1 file: WSeasons.csv This file identifies the different seasons included in the historical data, along with certain season-level properties. The game dates in this dataset are expressed in relative terms, as the number of days since the start of the regular season, and aligned for each season so that day number #133 is the Monday right before the tournament, when team selections are made. During any given season, day number zero is defined to be exactly 19 weeks earlier than Selection Monday, so Day #0 is a Monday in late October or early November such that Day #132 is Selection Sunday (for the men's tournament) and Day #133 is Selection Monday (for the women's tournament). This doesn't necessarily mean that the regular season will always start exactly on Day #0; in fact, during the past decade, regular season games typically start being played on a Friday that is either Day #4 or Day #11, but further back there was more variety. Data Section 1 file: WNCAATourneySeeds.csv This file identifies the seeds for all teams in each NCAA\u00ae tournament, for all seasons of historical data. Thus, there are exactly 64 rows for each year, since there are no play-in teams in the women's tournament. We will not know the seeds of the respective tournament teams, or even exactly which 64 teams it will be, until Selection Monday on March 18, 2019. Data Section 1 file: WRegularSeasonCompactResults.csv This file identifies the game-by-game results for many seasons of historical data, starting with the 1998 season. For each season, the file includes all games played from daynum 0 through 132. It is important to realize that the \"Regular Season\" games are simply defined to be all games played on DayNum=132 or earlier (DayNum=133 is Selection Monday). Thus a game played before Selection Monday will show up here whether it was a pre-season tournament, a non-conference game, a regular conference game, a conference tournament game, or whatever. Data Section 1 file: WNCAATourneyCompactResults.csv This file identifies the game-by-game NCAA\u00ae tournament results for all seasons of historical data. The data is formatted exactly like the WRegularSeasonCompactResults data. Each season you will see 63 games listed, since there are no women's play-in games. Although the scheduling of the men's tournament rounds has been consistent for many years, there has been more variety in the scheduling of the women's rounds. There have been four different schedules over the course of the past 20 years for the women's tournament, as follows: 2017 season through 2019 season: 2015 season and 2016 season: 2003 season through 2014 season: 1998 season through 2002 season: Data Section 1 file: WSampleSubmissionStage1.csv This file illustrates the submission file format for Stage 1. It is the simplest possible submission: a 50% winning percentage is predicted for each possible matchup. A submission file lists every possible matchup between tournament teams for one or more years. During Stage 1, you are asked to make predictions for all possible matchups from the past five NCAA\u00ae tournaments (seasons 2014, 2015, 2016, 2017, and 2018). In Stage 2, you will be asked to make predictions for all possible matchups from the current NCAA\u00ae tournament (season 2019). Since there are 64 teams in the tournament, there are 64*63/2=2,016 predictions to make for that year, so a Stage 1 submission file will have 2,016*5=10,080 data rows. Example #1: You want to make a prediction for Duke (TeamID=3181) against Arizona (TeamID=3112) in the 2005 tournament, with Duke given a 53% chance to win and Arizona given a 47% chance to win. In this case, Arizona has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Arizona's perspective (47%): 2005_3112_3181,0.47 Example #2: You want to make a prediction for Duke (TeamID=3181) against North Carolina (TeamID=3314) in the 2005 tournament, with Duke given a 51.6% chance to win and North Carolina given a 48.4% chance to win. In this case, Duke has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Duke's perspective (51.6%): 2005_3181_3314,0.516 This section provides game-by-game stats at a team level (free throws attempted, defensive rebounds, turnovers, etc.) for all regular season, conference tournament, and NCAA\u00ae tournament games since the 2009-10 season.  Team Box Scores are provided in \"Detailed Results\" files rather than \"Compact Results\" files. However, the two files are strongly related. In a Detailed Results file, the first eight columns (Season, DayNum, WTeamID, WScore, LTeamID, LScore, WLoc, and NumOT) are exactly the same as a Compact Results file. However, in a Detailed Results file, there are many additional columns. The column names should be self-explanatory to basketball fans (as above, \"W\" or \"L\" refers to the winning or losing team): (and then the same set of stats from the perspective of the losing team: LFGM is the number of field goals made by the losing team, and so on up to LPF). Note: by convention, \"field goals made\" (either WFGM or LFGM) refers to the total number of fields goals made by a team, a combination of both two-point field goals and three-point field goals. And \"three point field goals made\" (either WFGM3 or LFGM3) is just the three-point fields goals made, of course. So if you want to know specifically about two-point field goals, you have to subtract one from the other (e.g., WFGM - WFGM3). And the total number of points scored is most simply expressed as 2*FGM + FGM3 + FTM. Data Section 2 file: WRegularSeasonDetailedResults.csv This file provides team-level box scores for many regular seasons of historical data, starting with the 2010 season. All games listed in the WRegularSeasonCompactResults file since the 2010 season should exactly be present in the WRegularSeasonDetailedResults file. Data Section 2 file: WNCAATourneyDetailedResults.csv This file provides team-level box scores for many NCAA\u00ae tournaments, starting with the 2010 season. All games listed in the WNCAATourneyCompactResults file since the 2010 season should exactly be present in the WNCAATourneyDetailedResults file. This section provides city locations of all regular season, conference tournament, and NCAA\u00ae tournament games since the 2009-10 season Data Section 3 file: WCities.csv This file provides a master list of cities that have been locations for games played. Data Section 3 file: WGameCities.csv This file identifies all games, starting with the 2010 season, along with the city that the game was played in. Games from the regular season and the NCAA\u00ae tourney are all listed together. This section contains additional supporting information, including alternative team name spellings and representations of bracket structure Data Section 4 file: WTeamSpellings.csv This file indicates alternative spellings of many team names. It is intended for use in associating external spellings against our own TeamID numbers, thereby helping to relate the external data properly with our datasets. Over the years we have identified various external spellings of different team names (as an example, for Ball State we have seen \"ball st\", and \"ball st.\", and \"ball state\", and \"ball-st\", and \"ball-state\"). Other teams have had more significant changes to their names over the years; for example, \"Texas Pan-American\" and \"Texas-Rio Grande Valley\" are actually the same school. The current list is obviously not exhaustive, and we encourage participants to identify additional mappings and upload extended versions of this file to the forums. Data Section 4 file: WNCAATourneySlots This file identifies the mechanism by which teams are paired against each other, depending upon their seeds, as the tournament proceeds through its rounds. It can be of use in identifying, for a given historical game, what round it occurred in, and what the seeds/slots were for the two teams (the meaning of \"slots\" is described below). You may need to know these specifics if you are trying to represent/simulate the exact workings of the tournament bracket. Unlike the analogous file on the men's side, it is not necessary to provide a Season within this file, because the women's tournament has never had play-in-games and so the 64-team women's bracket has always had the same structure each season."
    },
    {
        "name": "RSNA Intracranial Hemorrhage Detection",
        "url": "https://www.kaggle.com/competitions/rsna-intracranial-hemorrhage-detection",
        "overview_text": "Overview text not found",
        "description_text": "Intracranial hemorrhage, bleeding that occurs inside the cranium, is a serious health problem requiring rapid and often intensive medical treatment. For example, intracranial hemorrhages account for approximately 10% of strokes in the U.S., where stroke is the fifth-leading cause of death. Identifying the location and type of any hemorrhage present is a critical step in treating the patient. Diagnosis requires an urgent procedure. When a patient shows acute neurological symptoms such as severe headache or loss of consciousness, highly trained specialists review medical images of the patient\u2019s cranium to look for the presence, location and type of hemorrhage. The process is complicated and often time consuming. In this competition, your challenge is to build an algorithm to detect acute intracranial hemorrhage and its subtypes. You\u2019ll develop your solution using a rich image dataset provided by the Radiological Society of North America (RSNA\u00ae) in collaboration with members of the American Society of Neuroradiology and MD.ai. If successful, you\u2019ll help the medical community identify the presence, location and type of hemorrhage in order to quickly and effectively treat affected patients. Challenge participants may be invited to present their AI models and methodologies during an award ceremony at the RSNA Annual Meeting which will be held in Chicago, Illinois, USA, from December 1-6, 2019. Four research institutions provided large volumes of de-identified CT studies that were assembled to create the challenge dataset: Stanford University, Thomas Jefferson University, Unity Health Toronto and Universidade Federal de S\u00e3o Paulo (UNIFESP), The American Society of Neuroradiology (ASNR) organized a cadre of more than 60 volunteers to label over 25,000 exams for the challenge dataset. ASNR is the world\u2019s leading organization for the future of neuroradiology representing more than 5,300 radiologists, researchers, interventionalists, and imaging scientists. MD.ai provided tooling and support for the data annotation process. The RSNA is an international society of radiologists, medical physicists and other medical professionals with more than 54,000 members from 146 countries across the globe. They see the potential for AI to assist in detection and classification of hemorrhages in order to prioritize and expedite their clinical work. A full set of acknowledgments can be found on this page.",
        "dataset_text": "This is a two-stage challenge. You will need the images for the current stage - provided as stage_2_test.zip. You will also need the training data - stage_2_train.csv - and the sample submission stage_2_sample_submission.csv, which provides the IDs for the test set, as well as a sample of what your submission should look like. Note: The timeline page outlines the two-stage format and deadlines. Stage 2 data is now available in accordance with this timeline. Also review two-stage FAQs for more details. The training data is provided as a set of image Ids and multiple labels, one for each of five sub-types of hemorrhage, plus an additional label for any, which should always be true if any of the sub-type labels is true. There is also a target column, Label, indicating the probability of whether that type of hemorrhage exists in the indicated image. There will be 6 rows per image Id. The label indicated by a particular row will look like [Image Id]_[Sub-type Name], as follows: All provided images are in DICOM format. DICOM images contain associated metadata. This will include PatientID, StudyInstanceUID, SeriesInstanceUID, and other features. In this challenge competitors are predicting whether a hemorrhage exists in a given image, and what type it is."
    },
    {
        "name": "Lyft 3D Object Detection for Autonomous Vehicles",
        "url": "https://www.kaggle.com/competitions/3d-object-detection-for-autonomous-vehicles",
        "overview_text": "Overview text not found",
        "description_text": " Self-driving technology presents a rare opportunity to improve the quality of life in many of our communities. Avoidable collisions, single-occupant commuters, and vehicle emissions are choking cities, while infrastructure strains under rapid urban growth. Autonomous vehicles are expected to redefine transportation and unlock a myriad of societal, environmental, and economic benefits. You can apply your data analysis skills in this competition to advance the state of self-driving technology. Lyft, whose mission is to improve people\u2019s lives with the world\u2019s best transportation, is investing in the future of self-driving vehicles. Level 5, their self-driving division, is working on a fleet of autonomous vehicles, and currently has a team of 450+ across Palo Alto, London, and Munich working to build a leading self-driving system (they\u2019re hiring!). Their goal is to democratize access to self-driving technology for hundreds of millions of Lyft passengers. From a technical standpoint, however, the bar to unlock technical research and development on higher-level autonomy functions like perception, prediction, and planning is extremely high. This implies technical R&D on self-driving cars has traditionally been inaccessible to the broader research community. This dataset aims to democratize access to such data, and foster innovation in higher-level autonomy functions for everyone, everywhere. By conducting a competition, we hope to encourage the research community to focus on hard problems in this space\u2014namely, 3D object detection over semantic maps. In this competition, you will build and optimize algorithms based on a large-scale dataset. This dataset features the raw sensor camera inputs as perceived by a fleet of multiple, high-end, autonomous vehicles in a restricted geographic area. If successful, you\u2019ll make a significant contribution towards stimulating further development in autonomous vehicles and empowering communities around the world.",
        "dataset_text": "You will need the LIDAR, image, map and data files for both train and test (test_images.zip, test_lidar.zip, etc.). You may also need the train.csv, which includes the sample annotations in the form expected for submissions. The sample_submission.csv file contains all of the sample Ids for the test set. The data files (test_data.zip, train_data.zip) are in JSON format. The data comes in the form of many interlocking tables and formats. The JSON files all contain single tables with identifying tokens that can be used to join with other files / tables. The images and lidar files all correspond to a sample in sample_data.json, and the sample_token from sample_data.json is the primary identifier used for the train and test samples. The annotations in train.csv are in the following format:\ncenter_x center_y center_z width length height yaw class_name For the test samples, we're predicting the bounding volumes and classes of all of the objects in a given scene. For example, in sample_token 97ce3ab08ccbc0baae0267cbf8d4da947e1f11ae1dbcb80c3f4408784cd9170c, we might be predicting the presence of a car (with confidence 1.0) and a bus (with confidence 0.5). The prediction would look like this: With all predicted objects on the same line. Note that confidence values are inserted prior to center_x center_y center_z width length height yaw class_name."
    },
    {
        "name": "Santa's Workshop Tour 2019",
        "url": "https://www.kaggle.com/competitions/santa-workshop-tour-2019",
        "overview_text": "Overview text not found",
        "description_text": "Hammers ring, are you listenin\u2019\nIn the shop, toys are glistenin\u2019\nShould they see the sights?\nThere might be a fight\u2026\nWalkin\u2019 \u2018round the Workshop Wonderland  Families said, they want to see it\nSanta said, he\u2019d guarantee it\nThey pick a date\nBut they may have to wait\nWalkin\u2019 \u2018round the Workshop Wonderland We told Santa that he was a madman\nHe just wants to make sure they all smile\nHe\u2019ll say \u201cAre you flexible?\u201c, They\u2019ll say \u201cYeah man,\nBut can you help us make it worth our while?\u201d \u201cGive them food, or sweater\nthe more they wait, the gifts get better\u201d\nPlease help us rank\nOr we\u2019ll break the bank!\nWalkin\u2019 \u2019round the Workshop Wonderland Santa has exciting news! For 100 days before Christmas, he opened up tours to his workshop. Because demand was so strong, and because Santa wanted to make things as fair as possible, he let each of the 5,000 families that will visit the workshop choose a list of dates they'd like to attend the workshop. Now that all the families have sent Santa their preferences, he's realized it's impossible for everyone to get their top picks, so he's decided to provide extra perks for families that don't get their preferences. In addition, Santa's accounting department has told him that, depending on how families are scheduled, there may be some unexpected and hefty costs incurred. Santa needs the help of the Kaggle community to optimize which day each family is assigned to attend the workshop in order to minimize any extra expenses that would cut into next years toy budget! Can you help Santa out? Banner/Listing Photo by Nathan Lemon on Unsplash\nDescription Photo by Markus Spiske on Unsplash",
        "dataset_text": "Your task is to schedule the families to Santa's Workshop in a way that minimizes the penalty cost to Santa (as described on the Evaluation page). Each family has listed their top 10 preferences for the dates they'd like to attend Santa's workshop tour. Dates are integer values representing the days before Christmas, e.g., the value 1 represents Dec 24, the value 2 represents Dec 23, etc. Each family also has a number of people attending, n_people. Every family must be scheduled for one and only one assigned_day."
    },
    {
        "name": "The Hunt for Prohibited Content",
        "url": "https://www.kaggle.com/competitions/avito-prohibited-content",
        "overview_text": "Overview text not found",
        "description_text": " Avito.ru is the largest general classified website in Russia that helps connect buyers with sellers across all Russian territories. There are more than 22 million active ads on Avito and each day a huge number of ads are added or modified. The efficiency of Avito depends heavily on the content quality -- when buyers can quickly find relevant content, sellers can sell their items in hours. The larger and more popular Avito becomes the more attractive it becomes to sell illicit items or services. Some items that people try to sell are completely illegal while others might seem allowable but are still prohibited by our rules. This is why all new or modified ads are thoroughly moderated by our team of human moderators. The moderators can remove the ad if it conflicts with the Russian legislation or with the internal rules of AVITO.ru. However, with our growth it becomes more and more challenging to thoroughly moderate all ads. This is where machine learning comes into play. The objective of this challenge is to create a predictive model that will learn from moderators' answers how to classify if an ad contains illicit content or not.",
        "dataset_text": "Data for this competition consists mainly of Russian text. All files are encoded in UTF-8 and are in tab separated format (.tsv). To help you transform Russian text into a set of features we have prepared intoductory code, where we recommend which modules in Python to use. Also note that uncompressed training and test data together take ~4GB of space. Training and Test data sets consist of individual ads that have either been blocked for illicit content or that have never been blocked. All ads that participate in this competition have already been closed. External data is allowed in this competition with approval. To gain approval for a data set/source, please post your request on this forum thread. To have more understanding about each field please see commented picture from Ad details page on Avito.ru "
    },
    {
        "name": "Liberty Mutual Group - Fire Peril Loss Cost",
        "url": "https://www.kaggle.com/competitions/liberty-mutual-fire-peril",
        "overview_text": "Overview text not found",
        "description_text": " A Fortune 100 company, Liberty Mutual Insurance has provided a wide range of insurance products and services designed to meet our customers' ever-changing needs for over 100 years. Within the business insurance industry, fire losses account for a significant portion of total property losses. High severity and low frequency, fire losses are inherently volatile, which makes modeling them difficult. In this challenge, your task is to predict the target, a transformed ratio of loss to total insured value, using the provided information. This will enable more accurate identification of each policyholder\u2019s risk exposure and the ability to tailor the insurance coverage for their specific operation. Because we seek to tap innovation both inside and outside the company, certain eligible Liberty Mutual employees are encouraged to participate in this challenge for development purposes. Refer to the competition rules for the full details.",
        "dataset_text": "This data represents almost a million insurance records and the task is to predict a transformed ratio of loss to total insured value (called \"target\" within the data set). The provided features contain policy characteristics, information on crime rate, geodemographics, and weather. The train and test sets are split randomly. For each id in the test set, you must predict the target using the provided features. id : A unique identifier of the data set target : The transformed ratio of loss to total insured value dummy : Nuisance variable used to control the model, but not working as a predictor var1 \u2013 var17 : A set of normalized variables representing policy characteristics (note: var11 is the weight used in the weighted gini score calculation) crimeVar1 \u2013 crimeVar9: A set of normalized Crime Rate variables geodemVar1 \u2013 geodemVar37 : A set of normalized geodemographic variables weatherVar1 \u2013 weatherVar236 : A set of normalized weather station variables Data Type * : Level \"Z\" in these variable represents a missing value. Missing values elsewhere in the data are denoted with NA +: Levels for var4 are in a hierarchical structure. The letter represents higher level and the number following the letter represents lower level nested within the higher level."
    },
    {
        "name": "Liberty Mutual Group: Property Inspection Prediction",
        "url": "https://www.kaggle.com/competitions/liberty-mutual-group-property-inspection-prediction",
        "overview_text": "Overview text not found",
        "description_text": "A Fortune 100 company, Liberty Mutual Insurance has provided a wide range of insurance products and services designed to meet their customers' ever-changing needs for over 100 years. To ensure that Liberty Mutual\u2019s portfolio of home insurance policies aligns with their business goals, many newly insured properties receive a home inspection. These inspections review the condition of key attributes of the property, including things like the foundation, roof, windows and siding. The results of an inspection help Liberty Mutual determine if the property is one they want to insure. In this challenge, your task is to predict a transformed count of hazards or pre-existing damages using a dataset of property information. This will enable Liberty Mutual to more accurately identify high risk homes that require additional examination to confirm their insurability.  Liberty Mutual is interested in hiring predictive modelers like you to work on one of many growing analytics teams within our company. As a member of Liberty Mutual\u2019s advanced analytics community, you will have the opportunity to apply sophisticated, cutting-edge techniques, similar to those used in this competition, to large data sets in departments such as Actuarial, Product, Claims, Marketing, Distribution, Human Resources, and Finance. Click to view available positions. Because we seek to tap innovation both inside and outside the company, certain eligible Liberty Mutual employees are encouraged to participate in this challenge for development purposes. Refer to the competition rules for the full details.",
        "dataset_text": "See, fork, and run a random forest benchmark model through Kaggle Scripts Each row in the dataset corresponds to a property that was inspected and given a hazard score (\"Hazard\"). You can think of the hazard score as a continuous number that represents the condition of the property as determined by the inspection. Some inspection hazards are major and contribute more to the total score, while some are minor and contribute less. The total score for a property is the sum of the individual hazards. The aim of the competition is to forecast the hazard score based on anonymized variables which are available before an inspection is ordered."
    },
    {
        "name": "Expedia Hotel Recommendations",
        "url": "https://www.kaggle.com/competitions/expedia-hotel-recommendations",
        "overview_text": "Overview text not found",
        "description_text": "Planning your dream vacation, or even a weekend escape, can be an overwhelming affair. With hundreds, even thousands, of hotels to choose from at every destination, it's difficult to know which will suit your personal preferences. Should you go with an old standby with those pillow mints you like, or risk a new hotel with a trendy pool bar?   Expedia wants to take the proverbial rabbit hole out of hotel search by providing personalized hotel recommendations to their users. This is no small task for a site with hundreds of millions of visitors every month! Currently, Expedia uses search parameters to adjust their hotel recommendations, but there aren't enough customer specific data to personalize them for each user. In this competition, Expedia is challenging Kagglers to contextualize customer data and predict the likelihood a user will stay at 100 different hotel groups. The data in this competition is a random selection from Expedia and is not representative of the overall statistics. ",
        "dataset_text": "Expedia has provided you logs of customer behavior. These include what customers searched for, how they interacted with search results (click/book), whether or not the search result was a travel package. The data in this competition is a random selection from Expedia and is not representative of the overall statistics. Expedia is interested in predicting which hotel group a user is going to book. Expedia has in-house algorithms to form hotel clusters, where similar hotels for a search (based on historical price, customer star ratings, geographical locations relative to city center, etc) are grouped together. These hotel clusters serve as good identifiers to which types of hotels people are going to book, while avoiding outliers such as new hotels that don't have historical data. Your goal of this competition is to predict the booking outcome (hotel cluster) for a user event, based on their search and other attributes associated with that user event. The train and test datasets are split based on time: training data from 2013 and 2014, while test data are from 2015. The public/private leaderboard data are split base on time as well. Training data includes all the users in the logs, including both click events and booking events. Test data only includes booking events.  destinations.csv data consists of features extracted from hotel reviews text.  Note that some srch_destination_id's in the train/test files don't exist in the destinations.csv file. This is because some hotels are new and don't have enough features in the latent space. Your algorithm should be able to handle this missing information. train/test.csv destinations.csv"
    },
    {
        "name": "Grupo Bimbo Inventory Demand",
        "url": "https://www.kaggle.com/competitions/grupo-bimbo-inventory-demand",
        "overview_text": "Overview text not found",
        "description_text": "Planning a celebration is a balancing act of preparing just enough food to go around without being stuck eating the same leftovers for the next week. The key is anticipating how many guests will come. Grupo Bimbo must weigh similar considerations as it strives to meet daily consumer demand for fresh bakery products on the shelves of over 1 million stores along its 45,000 routes across Mexico.  Currently, daily inventory calculations are performed by direct delivery sales employees who must single-handedly predict the forces of supply, demand, and hunger based on their personal experiences with each store. With some breads carrying a one week shelf life, the acceptable margin for error is small. In this competition, Grupo Bimbo invites Kagglers to develop a model to accurately forecast inventory demand based on historical sales data. Doing so will make sure consumers of its over 100 bakery products aren\u2019t staring at empty shelves, while also reducing the amount spent on refunds to store owners with surplus product unfit for sale.",
        "dataset_text": "In this competition, you will forecast the demand of a product for a given week, at a particular store. The dataset you are given consists of 9 weeks of sales transactions in Mexico. Every week, there are delivery trucks that deliver products to the vendors. Each transaction consists of sales and returns. Returns are the products that are unsold and expired. The demand for a product in a certain week is defined as the sales this week subtracted by the return next week. The train and test dataset are split based on time, as well as the public and private leaderboard dataset split. Things to note:"
    },
    {
        "name": "TalkingData Mobile User Demographics",
        "url": "https://www.kaggle.com/competitions/talkingdata-mobile-user-demographics",
        "overview_text": "Overview text not found",
        "description_text": " Nothing is more comforting than being greeted by your favorite drink just as you walk through the door of the corner caf\u00e9. While a thoughtful barista knows you take a macchiato every Wednesday morning at 8:15, it\u2019s much more difficult in a digital space for your preferred brands to personalize your experience. TalkingData, China\u2019s largest third-party mobile data platform, understands that everyday choices and behaviors paint a picture of who we are and what we value. Currently, TalkingData is seeking to leverage behavioral data from more than 70% of the 500 million mobile devices active daily in China to help its clients better understand and interact with their audiences. In this competition, Kagglers are challenged to build a model predicting users\u2019 demographic characteristics based on their app usage, geolocation, and mobile device properties. Doing so will help millions of developers and brand advertisers around the world pursue data-driven marketing efforts which are relevant to their users and catered to their preferences. ",
        "dataset_text": "In this competition, you are going to predict the demographics of a user (gender and age) based on their app download and usage behaviors.  The Data is collected from TalkingData SDK integrated within mobile apps TalkingData serves under the service term between TalkingData and mobile app developers. Full recognition and consent from individual user of those apps have been obtained, and appropriate anonymization have been performed to protect privacy. Due to confidentiality, we won't provide details on how the gender and age data was obtained. Please treat them as accurate ground truth for prediction.  The data schema can be represented in the following chart:"
    },
    {
        "name": "Outbrain Click Prediction",
        "url": "https://www.kaggle.com/competitions/outbrain-click-prediction",
        "overview_text": "Overview text not found",
        "description_text": "The internet is a stimulating treasure trove of possibility. Every day we stumble on news stories relevant to our communities or experience the serendipity of finding an article covering our next travel destination. Outbrain, the web\u2019s leading content discovery platform, delivers these moments while we surf our favorite sites.  Currently, Outbrain pairs relevant content with curious readers in about 250 billion personalized recommendations every month across many thousands of sites. In this competition, Kagglers are challenged to predict which pieces of content its global base of users are likely to click on. Improving Outbrain\u2019s recommendation algorithm will mean more users uncover stories that satisfy their individual tastes.",
        "dataset_text": "Data Use Update: The Competition Sponsor has updated the permitted use of this competition's dataset. You may access and use the Competition Data for any purpose, whether commercial or non-commercial, including for participating in the Competition and on Kaggle.com forums, and for academic research and education.  The dataset for this challenge contains a sample of users\u2019 page views and clicks, as observed on multiple publisher sites in the United States between 14-June-2016 and 28-June-2016. Each viewed page or clicked recommendation is further accompanied by some semantic attributes of those documents. For full details, see data specifications below. The dataset contains numerous sets of content recommendations served to a specific user in a specific context. Each context (i.e. a set of recommendations) is given a display_id. In each such set, the user has clicked on at least one recommendation. The identities of the clicked recommendations in the test set are not revealed. Your task is to rank the recommendations in each group by decreasing predicted likelihood of being clicked. As a warning, this is a very large relational dataset. While most of the tables are small enough to fit in memory, the page views log (page_views.csv) is over 2 billion rows and 100GB uncompressed. We have also uploaded a sample version of this file with the first 10,000,000 rows. The MD5 checksum of page_views.csv.zip is 3742c116bab4030e0a7ea1c0be623bd9. Each user in the dataset is represented by a unique id (uuid). A person can view a document (document_id), which is simply a web page with content (e.g.  a news article). On each document, a set of ads (ad_id) are displayed. Each ad belongs to a campaign (campaign_id) run by an advertiser (advertiser_id). You are also provided metadata about the document, such as which entities are mentioned, a taxonomy of categories, the topics mentioned, and the publisher. page_views.csv is a the log of users visiting documents. To save disk space, the timestamps in the entire dataset are relative to the first time in the dataset. If you wish to recover the actual epoch time of the visit, add 1465876799998 to the timestamp. clicks_train.csv is the training set, showing which of a set of ads was clicked. clicks_test.csv is the same as clicks_train.csv, except it does not have the clicked ad. This is the file you should use to predict. Each display_id has only one clicked ad. Note that test set contains display_ids from the entire dataset timeframe. Additionally, the public/private sampling for the competition is uniformly random, not based on time. These sampling choices were intentional, in spite of the possibility that participants can look ahead in time. sample_submission.csv shows the correct submission format. events.csv provides information on the display_id context. It covers both the train and test set. promoted_content.csv provides details on the ads. documents_meta.csv provides details on the documents. documents_topics.csv, documents_entities.csv, and documents_categories.csv all provide information about the content in a document, as well as Outbrain's confidence in each respective relationship. For example, an entity_id can represent a person, organization, or location. The rows in documents_entities.csv give the confidence that the given entity was referred to in the document.  Outbrain is releasing 2 Billion page views and 16,900,000 clicks of 700 Million unique users, across 560 sites. The data is anonymized. Please remember that participants are prohibited from de-anonymizing or reverse engineering data or combining the data with other publicly available information. Outbrain does not collect or hold PII (personally identifiable information), and the user identifiers we are releasing here are obscured. To protect its publisher partners, Outbrain is not releasing URLs of viewed or clicked stories, but rather anonymized document and site identifiers. The task at hand is click prediction, and by downloading the dataset, participants agree to use the data for that task alone, and will not attempt to reverse engineer the mapping from document, site, and user identifiers to URLs, site names or actual users."
    },
    {
        "name": "Sberbank Russian Housing Market",
        "url": "https://www.kaggle.com/competitions/sberbank-russian-housing-market",
        "overview_text": "Overview text not found",
        "description_text": "Housing costs demand a significant investment from both consumers and developers. And when it comes to planning a budget\u2014whether personal or corporate\u2014the last thing anyone needs is uncertainty about one of their biggets expenses. Sberbank, Russia\u2019s oldest and largest bank, helps their customers by making predictions about realty prices so renters, developers, and lenders are more confident when they sign a lease or purchase a building. Although the housing market is relatively stable in Russia, the country\u2019s volatile economy makes forecasting prices as a function of apartment characteristics a unique challenge. Complex interactions between housing features such as number of bedrooms and location are enough to make pricing predictions complicated. Adding an unstable economy to the mix means Sberbank and their customers need more than simple regression models in their arsenal. In this competition, Sberbank is challenging Kagglers to develop algorithms which use a broad spectrum of features to predict realty prices. Competitors will rely on a rich dataset that includes housing data and macroeconomic patterns. An accurate forecasting model will allow Sberbank to provide more certainty to their customers in an uncertain economy.",
        "dataset_text": "The aim of this competition is to predict the sale price of each property. The target variable is called price_doc in train.csv. The training data is from August 2011 to June 2015, and the test set is from July 2015 to May 2016. The dataset also includes information about overall conditions in Russia's economy and finance sector, so you can focus on generating accurate price forecasts for individual properties, without needing to second-guess what the business cycle will do. Update: please see the pinned discussion thread for some optional extra data, resolving an issue with some GIS features."
    },
    {
        "name": "Instacart Market Basket Analysis",
        "url": "https://www.kaggle.com/competitions/instacart-market-basket-analysis",
        "overview_text": "Overview text not found",
        "description_text": " Whether you shop from meticulously planned grocery lists or let whimsy guide your grazing, our unique food rituals define who we are. Instacart, a grocery ordering and delivery app, aims to make it easy to fill your refrigerator and pantry with your personal favorites and staples when you need them. After selecting products through the Instacart app, personal shoppers review your order and do the in-store shopping and delivery for you. Instacart\u2019s data science team plays a big part in providing this delightful shopping experience. Currently they use transactional data to develop models that predict which products a user will buy again, try for the first time, or add to their cart next during a session. Recently, Instacart open sourced this data - see their blog post on 3 Million Instacart Orders, Open Sourced. In this competition, Instacart is challenging the Kaggle community to use this anonymized data on customer orders over time to predict which previously purchased products will be in a user\u2019s next order. They\u2019re not only looking for the best model, Instacart\u2019s also looking for machine learning engineers to grow their team. Winners of this competition will receive both a cash prize and a fast track through the recruiting process. For more information about exciting opportunities at Instacart, check out their careers page here or e-mail their recruiting team directly at ml.jobs@instacart.com.",
        "dataset_text": "The dataset for this competition is a relational set of files describing customers' orders over time. The goal of the competition is to predict which products will be in a user's next order. The dataset is anonymized and contains a sample of over 3 million grocery orders from more than 200,000 Instacart users. For each user, we provide between 4 and 100 of their orders, with the sequence of products purchased in each order. We also provide the week and hour of day the order was placed, and a relative measure of time between orders. For more information, see the blog post accompanying its public release. Each entity (customer, product, order, aisle, etc.) has an associated unique id. Most of the files and variable names should be self-explanatory. These files specify which products were purchased in each order. order_products__prior.csv contains previous order contents for all customers. 'reordered' indicates that the customer has a previous order that contains the product. Note that some orders will have no reordered items. You may predict an explicit 'None' value for orders with no reordered items. See the evaluation page for full details.\norder_id,product_id,add_to_cart_order,reordered\n1,49302,1,1\n1,11109,2,1\n1,10246,3,0\n\u2026 This file tells to which set (prior, train, test) an order belongs. You are predicting reordered items only for the test set orders. 'order_dow' is the day of week.\norder_id,user_id,eval_set,order_number,order_dow,order_hour_of_day,days_since_prior_order\n2539329,1,prior,1,2,08,\n2398795,1,prior,2,3,07,15.0\n473747,1,prior,3,3,12,21.0\n\u2026 order_id,products\n17,39276\n34,39276\n137,39276\n\u2026"
    },
    {
        "name": "Peking University/Baidu - Autonomous Driving",
        "url": "https://www.kaggle.com/competitions/pku-autonomous-driving",
        "overview_text": "Overview text not found",
        "description_text": "Who do you think hates traffic more - humans or self-driving cars? The position of nearby automobiles is a key question for autonomous vehicles \u2015 and it's at the heart of our newest challenge. Self-driving cars have come a long way in recent years, but they're still not flawless. Consumers and lawmakers remain wary of adoption, in part because of doubts about vehicles\u2019 ability to accurately perceive objects in traffic. Baidu's Robotics and Autonomous Driving Lab (RAL), along with Peking University, hopes to close the gap once and for all with this challenge. They\u2019re providing Kagglers with more than 60,000 labeled 3D car instances from 5,277 real-world images, based on industry-grade CAD car models. Your challenge: develop an algorithm to estimate the absolute pose of vehicles (6 degrees of freedom) from a single image in a real-world traffic environment. Succeed and you'll help improve computer vision. That, in turn, will bring autonomous vehicles a big step closer to widespread adoption, so they can help reduce the environmental impact of our growing societies. Please cite the following paper when using the dataset:\nApolloCar3D: A Large 3D Car Instance Understanding Benchmark for Autonomous Driving\n@inproceedings{song2019apollocar3d,\ntitle={Apollocar3d: A large 3d car instance understanding benchmark for autonomous driving},\nauthor={Song, Xibin and Wang, Peng and Zhou, Dingfu and Zhu, Rui and Guan, Chenye and Dai, Yuchao and Su, Hao and Li, Hongdong and Yang, Ruigang},\nbooktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\npages={5452--5462},\nyear={2019}\n}",
        "dataset_text": "This dataset contains photos of streets, taken from the roof of a car. We're attempting to predict the position and orientation of all un-masked cars in the test images. You should also provide a confidence score indicating how sure you are of your prediction. Note that rotation values are angles expressed in radians, relative to the camera. The primary data is images of cars and related pose information. The pose information is formatted as strings, as follows: model type, yaw, pitch, roll, x, y, z A concrete example with two cars in the photo: 5 0.5 0.5 0.5 0.0 0.0 0.0 32 0.25 0.25 0.25 0.5 0.4 0.7 Submissions (per sample_submission.csv) are very similar, with the addition of a confidence score, and the removal of the model type. You are not required to predict the model type of the vehicle in question. ID, PredictionString ID_1d7bc9b31,0.5 0.5 0.5 0.0 0.0 0.0 1.0 indicating that this prediction has a confidence score of 1.0. Some cars in the images are not of interest (too far away, etc.). Binary masks are provided to allow competitors to remove them from consideration. 3D models of all cars of interest are available for download as pickle files - they can be compared against cars in images, used as references for rotation, etc. The pickles were created in Python 2. For Python 3 users, the following code will load a given model:"
    },
    {
        "name": "Prostate cANcer graDe Assessment (PANDA) Challenge",
        "url": "https://www.kaggle.com/competitions/prostate-cancer-grade-assessment",
        "overview_text": "Overview text not found",
        "description_text": "With more than 1 million new diagnoses reported every year, prostate cancer (PCa) is the second most common cancer among males worldwide that results in more than 350,000 deaths annually. The key to decreasing mortality is developing more precise diagnostics. Diagnosis of PCa is based on the grading of prostate tissue biopsies. These tissue samples are examined by a pathologist and scored according to the Gleason grading system. In this challenge, you will develop models for detecting PCa on images of prostate tissue samples, and estimate severity of the disease using the most extensive multi-center dataset on Gleason grading yet available. The grading process consists of finding and classifying cancer tissue into so-called Gleason patterns (3, 4, or 5) based on the architectural growth patterns of the tumor (Fig. 1). After the biopsy is assigned a Gleason score, it is converted into an ISUP grade on a 1-5 scale. The Gleason grading system is the most important prognostic marker for PCa, and the ISUP grade has a crucial role when deciding how a patient should be treated. There is both a risk of missing cancers and a large risk of overgrading resulting in unnecessary treatment. However, the system suffers from significant inter-observer variability between pathologists, limiting its usefulness for individual patients. This variability in ratings could lead to unnecessary treatment, or worse, missing a severe diagnosis. Automated deep learning systems have shown some promise in accurately grading PCa. Recent research, including two studies independently conducted by the groups hosting this challenge, have shown that these systems can achieve pathologist-level performance. However, these systems/results were not tested with multi-center datasets at scale. Your work here will improve on these efforts using the most extensive multi-center dataset on Gleason grading yet. The training set consists of around 11,000 whole-slide images of digitized H&E-stained biopsies originating from two centers. This is the largest public whole-slide image dataset available, roughly 8 times the size of the CAMELYON17 challenge, one of the largest digital pathology datasets and best known challenges in the field. Furthermore, in contrast to previous challenges, we are making full diagnostic biopsy images available. Using a sizable multi-center test set, graded by expert uro-pathologists, we will evaluate challenge submissions on their applicability to improve this critical diagnostic function.  Figure 1: An illustration of the Gleason grading process for an example biopsy containing prostate cancer. The most common (blue outline, Gleason pattern 3) and second most common (red outline, Gleason pattern 4) cancer growth patterns present in the biopsy dictate the Gleason score (3+4 for this biopsy), which in turn is converted into an ISUP grade (2 for this biopsy) following guidelines of the International Society of Urological Pathology. Biopsies not containing cancer are represented by an ISUP grade of 0 in this challenge. Radboud University Medical Center and Karolinska Institute have teamed up to organize this competition in collaboration with colleagues from Tampere University. The Computational Pathology Group (CPG) of the Radboud University Medical Center is a research group that develops computer algorithms to aid clinicians. Karolinska Institute\u2019s Department of Medical Epidemiology and Biostatistics (MEB) includes an interdisciplinary research group to improve the diagnostics and treatment of prostate cancer. Together, they hope to further their existing research to make a significant impact on the healthcare of prostate cancer patients. Challenge organizer team: Wouter Bulten, Geert Litjens, Hans Pinckaers, Peter Str\u00f6m, Martin Eklund, Lars Egevad, Henrik Gr\u00f6nberg, Kimmo Kartasalo, Pekka Ruusuvuori, Tomi H\u00e4kkinen, Sohier Dane, Maggie Demkin.  The PANDA workshop at MICCAI 2020 is sponsored by ContextVision, Ibex and Google.  The paper on the PANDA challenge has been published as Open Access in Nature Medicine. In the paper, we took a deep dive into the solutions, tested the methods to see if they generalize well to unseen data, and performed a comparison with pathologists. You can read the full paper and all results here: https://www.nature.com/articles/s41591-021-01620-2 With the paper's publication, the embargo on the data is now lifted (see forum post). If you want, you can now use the dataset for further scientific work and publish your results on the dataset. If you do so, please take the license (CC BY-SA-NC 4.0) into account (non-commercial) and make sure you cite the PANDA paper. The test sets will not be made public at this time, to allow further late submissions to be used for benchmarking algorithms. We are looking forward to seeing new scientific projects coming out of this dataset!",
        "dataset_text": "Your challenge in this competition is to classify the severity of prostate cancer from microscopy scans of prostate biopsy samples. There are two unusual twists to this problem relative to most competitions: [train/test].csv [train/test]_images: The images. Each is a large multi-level tiff file. You can expect roughly 1,000 images in the hidden test set. Note that slightly different procedures were in place for the images used in the test set than the training set. Some of the training set images have stray pen marks on them, but the test set slides are free of pen marks. train_label_masks: Segmentation masks showing which parts of the image led to the ISUP grade. Not all training images have label masks, and there may be false positives or false negatives in the label masks for a variety of reasons. These masks are provided to assist with the development of strategies for selecting the most useful subsamples of the images. The mask values depend on the data provider: Radboud: Prostate glands are individually labelled. Valid values are:\n\n0: background (non tissue) or unknown\n\n1: stroma (connective tissue, non-epithelium tissue)\n\n2: healthy (benign) epithelium\n\n3: cancerous epithelium (Gleason 3)\n\n4: cancerous epithelium (Gleason 4)\n\n5: cancerous epithelium (Gleason 5) Karolinska: Regions are labelled. Valid values are:\n\n0: background (non tissue) or unknown\n\n1: benign tissue (stroma and epithelium combined)\n\n2: cancerous tissue (stroma and epithelium combined) sample_submission.csv: A valid submission file. This is a notebooks-only competition; the downloadable test.csv and sample_submission.csv have been truncated. The full versions will be available to your submitted notebooks."
    },
    {
        "name": "Human Protein Atlas - Single Cell Classification",
        "url": "https://www.kaggle.com/competitions/hpa-single-cell-image-classification",
        "overview_text": "Overview text not found",
        "description_text": "There are billions of humans on this earth, and each of us is made up of trillions of cells. Just like every individual is unique, even genetically identical twins, scientists observe differences between the genetically identical cells in our bodies. Differences in the location of proteins can give rise to such cellular heterogeneity. Proteins play essential roles in virtually all cellular processes. Often, many different proteins come together at a specific location to perform a task, and the exact outcome of this task depends on which proteins are present. As you can imagine, different subcellular distributions of one protein can give rise to great functional heterogeneity between cells. Finding such differences, and figuring out how and why they occur, is important for understanding how cells function, how diseases develop, and ultimately how to develop better treatments for those diseases. To see more, start with less. That may seem counterintuitive, but the study of a single cell enables the discovery of mechanisms too difficult to see with multi-cell research. The importance of studying single cells is reflected in the ongoing revolution in biology centered around technologies for single cell analysis. Microscopy offers an opportunity to study differences in protein localizations within a population of cells. Current machine learning models for classifying protein localization patterns in microscope images gives a summary of the entire population of cells. However, the single-cell revolution in biology demands models that can precisely classify patterns in each individual cell in the image. The Human Protein Atlas is an initiative based in Sweden that is aimed at mapping proteins in all human cells, tissues, and organs. The data in the Human Protein Atlas database is freely accessible to scientists all around the world that allows them to explore the cellular makeup of the human body. Solving the single-cell image classification challenge will help us characterize single-cell heterogeneity in our large collection of images by generating more accurate annotations of the subcellular localizations for thousands of human proteins in individual cells. Thanks to you, we will be able to more accurately model the spatial organization of the human cell and provide new open-access cellular data to the scientific community, which may accelerate our growing understanding of how human cells functions and how diseases develop. This is a weakly supervised multi-label classification problem and a code competition. Given images of cells from our microscopes and labels of protein location assigned together for all cells in the image, Kagglers will develop models capable of segmenting and classifying each individual cell with precise labels. If successful, you'll contribute to the revolution of single-cell biology! The scientific journal Nature Methods is interested in considering a paper discussing the outcome and approaches of the challenge. The Human Protein Atlas team, led by Professor Emma Lundberg, would like to invite top performing teams to join as co-authors in writing this paper. Please follow the discussion forum for more details on how you can help.",
        "dataset_text": "On the data page below, you will find a set of full size original images (a mix of 1728x1728, 2048x2048 and 3072x3072 PNG files) in train.zip and test.zip. (Please note that since this is a code competition, part of test data will be hidden) You will also need the image level labels from train.csv and the filenames for the test set from sample_submission.csv.\nAs many Kagglers made use of all public images in HPA for previous classification challenge, we made the public HPA images available to download as instructed in this notebook. Note also that there are TFRecords available if competitors would like to use TPUs. The 16-bit version of the training images are available here. Additional training images are available here. The training image-level labels are provided for each sample in train.csv.\nThe bulk of the data for images - train.zip. Each sample consists of four files. Each file represents a different filter on the subcellular protein patterns represented by the sample. The format should be [filename]_[filter color].png for the PNG files. Colors are red for microtubule channels, blue for nuclei channels, yellow for Endoplasmic Reticulum (ER) channels, and green for protein of interest. You are predicting protein organelle localization labels for each cell in the image. Border cells are included when there is enough information to decide on the labels. There are in total 19 different labels present in the dataset (18 labels for specific locations, and label 18 for negative and unspecific signal). The dataset is acquired in a highly standardized way using one imaging modality (confocal microscopy). However, the dataset comprises 17 different cell types of highly different morphology, which affect the protein patterns of the different organelles. All image samples are represented by four filters (stored as individual files), the protein of interest (green) plus three cellular landmarks: nucleus (blue), microtubules (red), endoplasmic reticulum (yellow). The green filter should hence be used to predict the label, and the other filters are used as references.\nThe labels are represented as integers that map to the following: The labels you will get for training are image level labels while the task is to predict cell level labels.\nThat is to say, each training image contains a number of cells that have collectively been labeled as described above and the prediction task is to look at images of the same type and predict the labels of each individual cell within those images. As the training labels are a collective label for all the cells in an image, it means that each labeled pattern can be seen in the image but not necessarily that each cell within the image expresses the pattern. This imprecise labeling is what we refer to as weak. During the challenge you will both need to segment the cells in the images and predict the labels of those segmented cells."
    },
    {
        "name": "Santa 2021 - The Merry Movie Montage",
        "url": "https://www.kaggle.com/competitions/santa-2021",
        "overview_text": "Overview text not found",
        "description_text": " You\u2019ve gotta watch out\nYou probably will cry\nYou\u2019ll smile throughout\nI'm telling you why\nSantaTV\u2019s coming to town So give them a list,\nThey\u2019re watching it thrice\nThey\u2019re gonna find out what\u2019s sugar, what\u2019s spice.\nSantaTV\u2019s is coming to town The elves have lots of movies\nLet\u2019s hope they stay awake\nCause Christmas season\u2019s coming soon\nAnd there\u2019s toys for them to make! People seem to be getting in the Christmas spirit earlier and earlier each year. Decorations appear for sale in stores in the fall, Christmas songs are on the radio in October\u2026 The Elves at the North Pole are starting to recognize this, and need to work as fast as possible to launch their latest holiday offering: SantaTV+! A 24/7 streaming television channel where it\u2019s \u201cAlways Christmas, All the Time.\u201d To debut their new station, they\u2019ve decided to kick things off with a made-for-television Christmas movie marathon! They\u2019re excited for the premiere of such movies as \ud83c\udf85, \ud83e\udd36, \ud83e\udd8c, \ud83e\udddd, \ud83c\udf84, \ud83c\udf81, and \ud83c\udf80! But elves know that just as important as the movie themselves is the order they\u2019ll be aired. So the elves have decided the best way to figure out which order is best is to watch all the movies in every possible combination to see which feels the most Christmas-y. Your job is to help the elves by giving them the shortest viewing schedules that shows them every combination of movies so they can get SantaTV+ live as soon as possible! The elves have formed three movie-watching teams to lighten the load, so every combination must be seen by at least one of their groups. But they\u2019re also pretty sure they want to kick off the movie marathon with the \ud83c\udf85 and \ud83e\udd36 movies back-to-back, so be sure that each group has all the combinations that start with those. And finally, the elves have agreed to two sugar breaks, so you\u2019re allowed to give each group up to two \ud83c\udf1f wildcards, which will play all the movies at once while they\u2019re snacking, which will help speed things along. They can\u2019t launch SantaTV+ until all the groups have finished watching - so help give them the most efficient schedule to see every Christmas movie combination, and help them get back to making toys! Photos by Erwan Hesry and Diljaz TM on Unsplash.",
        "dataset_text": "Your task in this competition is to create three schedules that contain all permutations of the seven symbols \ud83c\udf85, \ud83e\udd36, \ud83e\udd8c, \ud83e\udddd, \ud83c\udf84, \ud83c\udf81, and \ud83c\udf80, subject to the conditions described on the Evaluation page. None of the files included here is necessary to solve the problem, but are provided for your convenience."
    },
    {
        "name": "U.S. Patent Phrase to Phrase Matching",
        "url": "https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching",
        "overview_text": "Overview text not found",
        "description_text": "Can you extract meaning from a large, text-based dataset derived from inventions? Here's your chance to do so. The U.S. Patent and Trademark Office (USPTO) offers one of the largest repositories of scientific, technical, and commercial information in the world through its Open Data Portal. Patents are a form of intellectual property granted in exchange for the public disclosure of new and useful inventions. Because patents undergo an intensive vetting process prior to grant, and because the history of U.S. innovation spans over two centuries and 11 million patents, the U.S. patent archives stand as a rare combination of data volume, quality, and diversity. In this competition, you will train your models on a novel semantic similarity dataset to extract relevant information by matching key phrases in patent documents. Determining the semantic similarity between phrases is critically important during the patent search and examination process to determine if an invention has been described before. For example, if one invention claims \"television set\" and a prior publication describes \"TV set\", a model would ideally recognize these are the same and assist a patent attorney or examiner in retrieving relevant documents. This extends beyond paraphrase identification; if one invention claims a \"strong material\" and another uses \"steel\", that may also be a match. What counts as a \"strong material\" varies per domain (it may be steel in one domain and ripstop fabric in another, but you wouldn't want your parachute made of steel). We have included the Cooperative Patent Classification as the technical domain context as an additional feature to help you disambiguate these situations. Can you build a model to match phrases in order to extract contextual information, thereby helping the patent community connect the dots between millions of patent documents? ",
        "dataset_text": "In this dataset, you are presented pairs of phrases (an anchor and a target phrase) and asked to rate how similar they are on a scale from 0 (not at all similar) to 1 (identical in meaning). This challenge differs from a standard semantic similarity task in that similarity has been scored here within a patent's context, specifically its CPC classification (version 2021.05), which indicates the subject to which the patent relates. For example, while the phrases \"bird\" and \"Cape Cod\" may have low semantic similarity in normal language, the likeness of their meaning is much closer if considered in the context of \"house\". This is a code competition, in which you will submit code that will be run against an unseen test set. The unseen test set contains approximately 12k pairs of phrases. A small public test set has been provided for testing purposes, but is not used in scoring. Information on the meaning of CPC codes may be found on the USPTO website. The CPC version 2021.05 can be found on the CPC archive website. The scores are in the 0-1 range with increments of 0.25 with the following meanings:"
    },
    {
        "name": "March Machine Learning Mania 2022 - Men\u2019s",
        "url": "https://www.kaggle.com/competitions/mens-march-mania-2022",
        "overview_text": "Overview text not found",
        "description_text": "Description text not found",
        "dataset_text": "Each season there are thousands of NCAA basketball games played between Division I men's teams, culminating in March Madness\u00ae, the 68-team national championship that starts in the middle of March. We have provided a large amount of historical data about college basketball games and teams, going back many years. Armed with this historical data, you can explore it and develop your own distinctive ways of predicting March Madness\u00ae game outcomes. You can even evaluate and compare different approaches by seeing which of them would have done best at predicting tournament games from the past. If you are unfamiliar with the format and intricacies of the NCAA\u00ae tournament, we encourage reading the wikipedia page before diving into the data.  The data description and schema may seem daunting at first, but is not as complicated as it appears. As a reminder, you are encouraged to incorporate your own sources of data. We have provided extensive historical data to jump-start the modeling process, and this data is self-consistent (for instance, dates and team ID's are always treated the same way). Nevertheless, you may also be able to make productive use of external data. If you head down this path, please be forewarned that many sources have their own distinctive way of identifying the names of teams, and this can make it challenging to link up with our data. The MTeamSpellings file, which is listed in the bottom section below, may help you map external team references into our own Team ID structure, and you may also need to understand exactly how dates work in our data. We extend our gratitude to Kenneth Massey for providing much of the historical data. Special Acknowledgment to Jeff Sonas of Sonas Consulting for his support in assembling the dataset for this competition. Stage 1 - You should submit predicted probabilities for every possible matchup in the past 5 NCAA\u00ae tournaments (2016-2019 and 2021). Note that there was no tournament held in 2020. Stage 2 - You should submit predicted probabilities for every possible matchup before the 2022 tournament begins. Refer to the Timeline page for specific dates. In both stages, the sample submission will tell you which games to predict. Below we describe the format and fields of the contest data files. All of the files are complete through February 7th of the current season. At the start of Stage 2, we will provide updates to these files to incorporate data from the remaining weeks of the current season. This section provides everything you need to build a simple prediction model and submit predictions. Special note about \"Season\" numbers: the college basketball season lasts from early November until the national championship tournament that starts in the middle of March. For instance, this year the first men\u2019s Division I games were played on November 9th, 2021 and the men\u2019s national championship game will be played on April 4th, 2022. Because a basketball season spans two calendar years like this, it can be confusing to refer to the year of the season. By convention, when we identify a particular season, we will reference the year that the season ends in, not the year that it starts in. So for instance, the current season will be identified in our data as the 2022 season, not the 2021 season or the 2021-22 season or the 2021-2022 season, though you may see any of these in everyday use outside of our data. Data Section 1 file: MTeams.csv This file identifies the different college teams present in the dataset. Each school is uniquely identified by a 4 digit id number. You will not see games present for all teams in all seasons, because the games listing is only for matchups where both teams are Division-I teams. There are 358 teams currently in Division-I, and an overall total of 372 teams in our team listing (each year, some teams might start being Division-I programs, and others might stop being Division-I programs).   Data Section 1 file: MSeasons.csv This file identifies the different seasons included in the historical data, along with certain season-level properties. Data Section 1 file: MNCAATourneySeeds.csv This file identifies the seeds for all teams in each NCAA\u00ae tournament, for all seasons of historical data. Thus, there are between 64-68 rows for each year, depending on whether there were any play-in games and how many there were. In recent years the structure has settled at 68 total teams, with four \"play-in\" games leading to the final field of 64 teams entering Round 1 on Thursday of the first week (by definition, that is DayNum=136 each season). We will not know the seeds of the respective tournament teams, or even exactly which 68 teams it will be, until Selection Sunday on March 13, 2022 (DayNum=132). Data Section 1 file: MRegularSeasonCompactResults.csv This file identifies the game-by-game results for many seasons of historical data, starting with the 1985 season (the first year the NCAA\u00ae had a 64-team tournament). For each season, the file includes all games played from DayNum 0 through 132. It is important to realize that the \"Regular Season\" games are simply defined to be all games played on DayNum=132 or earlier (DayNum=132 is Selection Sunday, and there are always a few conference tournament finals actually played early in the day on Selection Sunday itself). Thus a game played on or before Selection Sunday will show up here whether it was a pre-season tournament, a non-conference game, a regular conference game, a conference tournament game, or whatever. Data Section 1 file: MNCAATourneyCompactResults.csv This file identifies the game-by-game NCAA\u00ae tournament results for all seasons of historical data. The data is formatted exactly like the MRegularSeasonCompactResults data. All games will show up as neutral site (so WLoc is always N). Note that this tournament game data also includes the play-in games (which always occurred on day 134/135) for those years that had play-in games. Thus each season you will see between 63 and 67 games listed, depending on how many play-in games there were. Because of the consistent structure of the NCAA\u00ae tournament schedule, you can actually tell what round a game was, depending on the exact DayNum. Thus: Special note: Each year, there are also going to be other games that happened after Selection Sunday, which are not part of the NCAA\u00ae Tournament. This includes tournaments like the postseason NIT, the CBI, the CIT, and the Vegas 16. Such games are not listed in the Regular Season or the NCAA\u00ae Tourney files; they can be found in the \"Secondary Tourney\" data files within Data Section 6. Although they would not be games you would ever be predicting directly for the NCAA\u00ae tournament, and they would not be games you would have data from at the time of predicting NCAA\u00ae tournament outcomes, you may nevertheless wish to make use of these games for model optimization, depending on your methodology. The more games that you can test your predictions against, the better your optimized model might eventually become, depending on how applicable all those games are. A similar argument might be advanced in favor of optimizing your predictions against conference tournament games, which might be viewed as reasonable proxies for NCAA\u00ae tournament games.  Data Section 1 file: MSampleSubmissionStage1.csv This file illustrates the submission file format for Stage 1. It is the simplest possible submission: a 50% winning percentage is predicted for each possible matchup. A submission file lists every possible matchup between tournament teams for one or more years. During Stage 1, you are asked to make predictions for all possible matchups from the past five NCAA\u00ae tournaments (seasons 2016, 2017, 2018, 2019, and 2021). In Stage 2, you will be asked to make predictions for all possible matchups from the current NCAA\u00ae tournament (season 2022). When there are 68 teams in the tournament, there are 68*67/2=2,278 predictions to make for that year, so a Stage 1 submission file will have 2,278*5=11,390 data rows. Example #1: You want to make a prediction for Duke (TeamID=1181) against Arizona (TeamID=1112) in the 2017 tournament, with Duke given a 53% chance to win and Arizona given a 47% chance to win. In this case, Arizona has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Arizona's perspective (47%): 2017_1112_1181,0.47 Example #2: You want to make a prediction for Duke (TeamID=1181) against North Carolina (TeamID=1314) in the 2018 tournament, with Duke given a 51.6% chance to win and North Carolina given a 48.4% chance to win. In this case, Duke has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Duke's perspective (51.6%): 2018_1181_1314,0.516 Also note that a single prediction row serves as a prediction for each of the two teams' winning chances. So for instance, in Example #1, the submission row of \"2017_1112_1181,0.47\" specifically gives a 47% chance for Arizona to win, and doesn't explicitly mention Duke's 53% chance to win. However, our evaluation utility will automatically infer the winning percentage in the other direction, so a 47% prediction for Arizona to win also means a 53% prediction for Duke to win. And similarly, because the submission row in Example #2 gives Duke a 51.6% chance to beat North Carolina, we will automatically figure out that this also means North Carolina has a 48.4% chance to beat Duke. This section provides game-by-game stats at a team level (free throws attempted, defensive rebounds, turnovers, etc.) for all regular season, conference tournament, and NCAA\u00ae tournament games since the 2002-03 season.  Team Box Scores are provided in \"Detailed Results\" files rather than \"Compact Results\" files. However, the two files are strongly related. In a Detailed Results file, the first eight columns (Season, DayNum, WTeamID, WScore, LTeamID, LScore, WLoc, and NumOT) are exactly the same as a Compact Results file. However, in a Detailed Results file, there are many additional columns. The column names should be self-explanatory to basketball fans (as above, \"W\" or \"L\" refers to the winning or losing team): (and then the same set of stats from the perspective of the losing team: LFGM is the number of field goals made by the losing team, and so on up to LPF). Note: by convention, \"field goals made\" (either WFGM or LFGM) refers to the total number of fields goals made by a team, a combination of both two-point field goals and three-point field goals. And \"three point field goals made\" (either WFGM3 or LFGM3) is just the three-point fields goals made, of course. So if you want to know specifically about two-point field goals, you have to subtract one from the other (e.g., WFGM - WFGM3). And the total number of points scored is most simply expressed as 2*FGM + FGM3 + FTM. Data Section 2 file: MRegularSeasonDetailedResults.csv This file provides team-level box scores for many regular seasons of historical data, starting with the 2003 season. All games listed in the MRegularSeasonCompactResults file since the 2003 season should exactly be present in the MRegularSeasonDetailedResults file. Data Section 2 file: MNCAATourneyDetailedResults.csv This file provides team-level box scores for many NCAA\u00ae tournaments, starting with the 2003 season. All games listed in the MNCAATourneyCompactResults file since the 2003 season should exactly be present in the MNCAATourneyDetailedResults file. This section provides city locations of all regular season, conference tournament, and NCAA\u00ae tournament games since the 2009-10 season Data Section 3 file: Cities.csv This file provides a master list of cities that have been locations for games played. Please notice that the Cities and Conferences files are the only two that don't start with an M; this is because the data files are identical between men's and women's data, so you don't need to maintain separate listings of cities or conferences across the two contests. Also note that if you created any supplemental data last year on cities (latitude/longitude, altitude, etc.), the CityID's match between last year and this year, so you should be able to re-use that information. Data Section 3 file: MGameCities.csv This file identifies all games, starting with the 2010 season, along with the city that the game was played in. Games from the regular season, the NCAA\u00ae tourney, and other post-season tournaments, are all listed together. There should be no games since the 2010 season where the CityID is not known. Games from the 2009 season and before are not listed in this file. This section provides weekly team rankings for dozens of top rating systems - Pomeroy, Sagarin, RPI, ESPN, etc., since the 2002-2003 season Data Section 4 file: MMasseyOrdinals.csv This file lists out rankings (e.g. #1, #2, #3, ..., #N) of teams going back to the 2002-2003 season, under a large number of different ranking system methodologies. The information was gathered by Kenneth Massey and provided on his College Basketball Ranking Composite page. Note that a rating system is more precise than a ranking system, because a rating system can provide insight about the strength gap between two adjacently-ranked teams. A ranking system will just tell you who is #1 or who is #2, but a rating system might tell you whether the gap between #1 and #2 is large or small. Nevertheless, it can be hard to compare two different rating systems that are expressed in different scales, so it can be very useful to express all the systems in terms of their ordinal ranking (1, 2, 3, ..., N) of teams. Disclaimer: you ought to be careful about your methodology when using or evaluating these ranking systems. They are presented on a weekly basis, and given a consistent date on the Massey Composite page that typically is a Sunday; that is how the ranking systems can be compared against each other on this page. However, these systems each follow their own timeline and some systems may be released on a Sunday and others on a Saturday or Monday or even Tuesday. You should remember that if a ranking is released on a Tuesday, and was calculated based on games played through Monday, it will make the system look unusually good at predicting if you use that system to forecast the very games played on Monday that already inform the rankings. To avoid this methodological trap, we have typically used a conservative RankingDayNum of Wednesday to represent the rankings that were released at approximately the end of the weekend, a few days before, even though those rankings are represented on the composite page as being on a Sunday. For some of the older years, a more precise timestamp was known for each ranking system that allowed a more precise assignment of a RankingDayNum. By convention, the final pre-tournament rankings are always expressed as RankingDayNum=133, even though sometimes the rankings for individual systems are not released until Tuesday (DayNum=134) or even Wednesday or Thursday. If you decide to use some rankings from these Massey Ordinals to inform your predictions, be forewarned that we have no control over when they are released, and not all systems may turn out to be available in time to make pre-tournament predictions by our submission deadline. In such a situation, you may wish to use the rankings from DayNum=128 or you may need to dig into the details of the actual source of the rankings, by following the respective links on the Massey Composite Page. We may also be able to provide partial releases of the final pre-tournament Massey Ordinals on the forums, so that as systems come in on Monday or Tuesday you can use them right away. This section contains additional supporting information, including coaches, conference affiliations, alternative team name spellings, bracket structure, and game results for NIT and other postseason tournaments. Data Section 5 file: MTeamCoaches.csv This file indicates the head coach for each team in each season, including a start/finish range of DayNum's to indicate a mid-season coaching change. For scenarios where a team had the same head coach the entire season, they will be listed with a DayNum range of 0 to 154 for that season. For head coaches whose term lasted many seasons, there will be many rows listed, most of which have a DayNum range of 0 to 154 for the corresponding season. Data Section 5 file: Conferences.csv This file indicates the Division I conferences that have existed over the years since 1985. Each conference is listed with an abbreviation and a longer name. There has been no attempt to link up conferences who merged with other conferences, or whose names changed over time. Thus you will see, for instance, a \"Pacific-10\" conference up through the 2011 season, and then a \"Pacific-12\" conference starting in the 2012 season, and these look like different conferences in the data, even though it was still mostly the same teams. Please notice that the Cities and Conferences files are the only two that don't start with an M; this is because the data files are identical between men's and women's data, so you don't need to maintain separate listings of cities or conferences across the two contests. However, the Team Conferences data differs slightly between men's and women's, so those files do have the prefixes. Data Section 5 file: MTeamConferences.csv This file indicates the conference affiliations for each team during each season. Some conferences have changed their names from year to year, and/or changed which teams are part of the conference. This file tracks this information historically. Data Section 5 file: MConferenceTourneyGames.csv This file indicates which games were part of each year's post-season conference tournaments (all of which finished on Selection Sunday or earlier), starting from the 2001 season. Many of these conference tournament games are held on neutral sites, and many of the games are played by tournament-caliber teams just a few days before the NCAA\u00ae tournament. Thus these games could be considered as very similar to NCAA\u00ae tournament games, and (depending on your methodology) may be of use in optimizing your predictions. However, this is NOT a new listing of games; these games are already present within the MRegularSeasonCompactResults and MRegularSeasonDetailedResults files. So this file simply helps you to identify which of the \"regular season\" games since the 2001 season were actually conference tournament games, in case that is useful information. Data Section 5 file: MSecondaryTourneyTeams.csv This file identifies the teams that participated in post-season tournaments other than the NCAA\u00ae Tournament (such events would run in parallel with the NCAA\u00ae Tournament). These are teams that were not invited to the NCAA\u00ae Tournament and instead were invited to some other tournament, of which the NIT is the most prominent tournament, but there have also been the CBI, CIT, and Vegas 16 (V16) at various points in recent years. Depending on your methodology, you might find it useful to have these additional game results, above and beyond what is available from the NCAA\u00ae Tournament results. Many of these teams, especially in the NIT, were \"bubble\" teams of comparable strength to several NCAA\u00ae Tournament invitees, and so these games may be of use in model optimization for predicting NCAA\u00ae Tournament results. Also note that this information could be determined just from inspecting the MSecondaryTourneyCompactResults file, but is presented in this file as well, for your convenience. Data Section 5 file: MSecondaryTourneyCompactResults.csv This file indicates the final scores for the tournament games of \"secondary\" post-season tournaments: the NIT, CBI, CIT, and Vegas 16. The detailed results (team box scores) have not been assembled for these games. For the most part, this file is exactly like other Compact Results listings, although it also has a column for Secondary Tourney. Also note that because these games are played after DayNum=132, they are NOT listed in the MRegularSeasonCompactResults file. Data Section 5 file: MTeamSpellings.csv This file indicates alternative spellings of many team names. It is intended for use in associating external spellings against our own TeamID numbers, thereby helping to relate the external data properly with our datasets. Over the years we have identified various external spellings of different team names (as an example, for Ball State we have seen \"ball st\", and \"ball st.\", and \"ball state\", and \"ball-st\", and \"ball-state\"). Other teams have had more significant changes to their names over the years; for example, \"Texas Pan-American\" and \"Texas-Rio Grande Valley\" are actually the same school. The current list is obviously not exhaustive, and we encourage participants to identify additional mappings and upload extended versions of this file to the forums. Data Section 5 file: MNCAATourneySlots This file identifies the mechanism by which teams are paired against each other, depending upon their seeds, as the tournament proceeds through its rounds. It can be of use in identifying, for a given historical game, what round it occurred in, and what the seeds/slots were for the two teams (the meaning of \"slots\" is described below). Because of the existence of play-in games for particular seed numbers, the pairings have small differences from year to year. You may need to know these specifics if you are trying to represent/simulate the exact workings of the tournament bracket. Data Section 5 file: MNCAATourneySeedRoundSlots.csv This file helps to represent the bracket structure in any given year. No matter where the play-in seeds are located, we can always know, for a given tournament seed, exactly what bracket slot they would be playing in, on each possible game round, and what the possible DayNum values would be for that round. Thus, if we know when a historical game was played, and what the team's seed was, we can identify the slot for that game. This can be useful in representing or simulating the tournament bracket structure."
    },
    {
        "name": "March Machine Learning Mania 2022 - Women's",
        "url": "https://www.kaggle.com/competitions/womens-march-mania-2022",
        "overview_text": "Overview text not found",
        "description_text": "Another year, another chance to predict the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. In our eighth annual March Machine Learning Mania competition, Kagglers will once again join the millions of fans who attempt to predict the outcomes of this year's US women's college basketball tournament. But unlike most fans, you will pick the winners and losers using a combination of rich historical data and computing power, while the ground truth unfolds on television. You're provided data of historical NCAA games and are encouraged to use other sources of publicly available data to gain a winning edge.  In stage one of this two-stage competition, participants will build and test their models against previous tournaments. In the second stage, participants will predict the outcome of the 2022 tournament. You don\u2019t need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2022 results. And don't forget to take a look at our companion competition that looks to predict the outcome of the US men's college basketball tournament! Banner image by Ben Hershey on Unsplash",
        "dataset_text": "Each season there are thousands of NCAA\u00ae basketball games played between Division I women's teams, culminating in March Madness, the 64-team national championship that starts in the middle of March. We have provided a large amount of historical data about college basketball games and teams, going back many years. Armed with this historical data, you can explore it and develop your own distinctive ways of predicting March Madness\u00ae game outcomes. You can even evaluate and compare different approaches by seeing which of them would have done best at predicting tournament games from the past. If you are unfamiliar with the format and intricacies of the tournament, we encourage reading the wikipedia page before diving into the data. The data description and schema may seem daunting at first, but is not as complicated as it appears. As a reminder, you are encouraged to incorporate your own sources of data. We have provided extensive historical data to jump-start the modeling process, and this data is self-consistent (for instance, dates and team ID's are always treated the same way). Nevertheless, you may also be able to make productive use of external data. If you head down this path, please be forewarned that many sources have their own distinctive way of identifying the names of teams, and this can make it challenging to link up with our data. The WTeamSpellings file, which is listed in the bottom section below, may help you map external team references into our own Team ID structure. You will probably also need to understand exactly how dates work in our data. Remember as well that you are required to disclose your external sources of data prior to the start of the tournament. We extend our gratitude to Kenneth Massey for providing much of the historical data. Special Acknowledgment to Jeff Sonas of Sonas Consulting for his support in assembling the dataset for this competition. Stage 1 - You should submit predicted probabilities for every possible matchup in the past 5 NCAA\u00ae tournaments (2016-2019 and 2021). Note that there was no tournament held in 2020. Stage 2 - You should submit predicted probabilities for every possible matchup before the 2022 tournament begins. Refer to the Timeline page for specific dates. In both stages, the sample submission will tell you which games to predict. Below we describe the format and fields of the contest data files. All of the files are complete through February 7th of the current season. At the start of Stage 2, we will provide updates to these files to incorporate data from the remaining weeks of the current season. This section provides everything you need to build a simple prediction model and submit predictions. Special note about \"Season\" numbers: the college basketball season lasts from early November until the national championship tournament that starts in the middle of March. For instance, this year the first women\u2019s Division I games were played on November 9th, 2021 and the women\u2019s national championship game will be played on April 3rd, 2022. Because a basketball season spans two calendar years like this, it can be confusing to refer to the year of the season. By convention, when we identify a particular season, we will reference the year that the season ends in, not the year that it starts in. So for instance, the current season will be identified in our data as the 2022 season, not the 2021 season or the 2021-22 season or the 2021-2022 season, though you may see any of these in everyday use outside of our data. Data Section 1 file: WTeams.csv  This file identifies the different college teams present in the dataset. Each school is uniquely identified by a 4 digit id number. You will not see games present for all teams in all seasons, because the games listing is only for matchups where both teams are Division-I teams. There are 356 teams currently in Division-I, and an overall total of 370 teams in our team listing. Each year, some teams might start being Division-I programs, and others might stop being Division-I programs.  Data Section 1 file: WSeasons.csv This file identifies the different seasons included in the historical data, along with certain season-level properties. The game dates in this dataset are expressed in relative terms, as the number of days since the start of the regular season, and aligned for each season so that day number #133 is the Monday right before the tournament, when team selections are made. During any given season, day number zero is defined to be exactly 19 weeks earlier than Selection Monday, so Day #0 is a Monday in late October or early November such that Day #132 is Selection Sunday (for the men's tournament) and Day #133 is Selection Monday (for the women's tournament). This doesn't necessarily mean that the regular season will always start exactly on day #0 or day #1; in fact, during the past decade, regular season games typically start being played on a Friday that is either Day #4 or Day #11, but further back there was more variety. Data Section 1 file: WNCAATourneySeeds.csv This file identifies the seeds for all teams in each NCAA\u00ae tournament, for all seasons of historical data. Thus, there are exactly 64 rows for each year, since there are no play-in teams in the women's tournament. We will not know the seeds of the respective tournament teams, or even exactly which 64 teams it will be, until Selection Monday on March 14, 2022 (DayNum=133). Data Section 1 file: WRegularSeasonCompactResults.csv This file identifies the game-by-game results for many seasons of historical data, starting with the 1998 season. For each season, the file includes all games played from DayNum 0 through 132. It is important to realize that the \"Regular Season\" games are simply defined to be all games played on DayNum=132 or earlier (DayNum=133 is Selection Monday). Thus a game played before Selection Monday will show up here whether it was a pre-season tournament, a non-conference game, a regular conference game, a conference tournament game, or whatever. Data Section 1 file: WNCAATourneyCompactResults.csv This file identifies the game-by-game NCAA\u00ae tournament results for all seasons of historical data. The data is formatted exactly like the WRegularSeasonCompactResults data. Each season you will see 63 games listed, since there are no women's play-in games. Although the scheduling of the men's tournament rounds has been consistent for many years, there has been more variety in the scheduling of the women's rounds. There have been four different schedules over the course of the past 20+ years for the women's tournament, as follows: 2017 season through 2021 season: 2015 season and 2016 season: 2003 season through 2014 season: 1998 season through 2002 season: Data Section 1 file: WSampleSubmissionStage1.csv This file illustrates the submission file format for Stage 1. It is the simplest possible submission: a 50% winning percentage is predicted for each possible matchup. A submission file lists every possible matchup between tournament teams for one or more years. During Stage 1, you are asked to make predictions for all possible matchups from the past five NCAA\u00ae tournaments (seasons 2016, 2017, 2018, 2019, and 2021). In Stage 2, you will be asked to make predictions for all possible matchups from the current NCAA\u00ae tournament (season 2022). Since there are 64 teams in the tournament, there are 64*63/2=2,016 predictions to make for that year, so a Stage 1 submission file will have 2016*5=10,080 data rows. Example #1: You want to make a prediction for Duke (TeamID=3181) against Arizona (TeamID=3112) in the 2005 tournament, with Duke given a 53% chance to win and Arizona given a 47% chance to win. In this case, Arizona has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Arizona's perspective (47%): 2005_3112_3181,0.47 Example #2: You want to make a prediction for Duke (TeamID=3181) against North Carolina (TeamID=3314) in the 2005 tournament, with Duke given a 51.6% chance to win and North Carolina given a 48.4% chance to win. In this case, Duke has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Duke's perspective (51.6%): 2005_3181_3314,0.516 Also note that a single prediction row serves as a prediction for each of the two teams' winning chances. So for instance, in Example #1, the submission row of \"2005_3112_3181,0.47\" specifically gives a 47% chance for Arizona to win, and doesn't explicitly mention Duke's 53% chance to win. However, our evaluation utility will automatically infer the winning percentage in the other direction, so a 47% prediction for Arizona to win also means a 53% prediction for Duke to win. And similarly, because the submission row in Example #2 gives Duke a 51.6% chance to beat North Carolina, we will automatically figure out that this also means North Carolina has a 48.4% chance to beat Duke. This section provides game-by-game stats at a team level (free throws attempted, defensive rebounds, turnovers, etc.) for all regular season, conference tournament, and NCAA\u00ae tournament games since the 2009-10 season.  Team Box Scores are provided in \"Detailed Results\" files rather than \"Compact Results\" files. However, the two files are strongly related. In a Detailed Results file, the first eight columns (Season, DayNum, WTeamID, WScore, LTeamID, LScore, WLoc, and NumOT) are exactly the same as a Compact Results file. However, in a Detailed Results file, there are many additional columns. The column names should be self-explanatory to basketball fans (as above, \"W\" or \"L\" refers to the winning or losing team): (and then the same set of stats from the perspective of the losing team: LFGM is the number of field goals made by the losing team, and so on up to LPF). Note: by convention, \"field goals made\" (either WFGM or LFGM) refers to the total number of fields goals made by a team, a combination of both two-point field goals and three-point field goals. And \"three point field goals made\" (either WFGM3 or LFGM3) is just the three-point fields goals made, of course. So if you want to know specifically about two-point field goals, you have to subtract one from the other (e.g., WFGM - WFGM3). And the total number of points scored is most simply expressed as 2*FGM + FGM3 + FTM. Data Section 2 file: WRegularSeasonDetailedResults.csv This file provides team-level box scores for many regular seasons of historical data, starting with the 2010 season. All games listed in the WRegularSeasonCompactResults file since the 2010 season should exactly be present in the WRegularSeasonDetailedResults file. Data Section 2 file: WNCAATourneyDetailedResults.csv This file provides team-level box scores for many NCAA\u00ae tournaments, starting with the 2010 season. All games listed in the WNCAATourneyCompactResults file since the 2010 season should exactly be present in the WNCAATourneyDetailedResults file. This section provides city locations of all regular season, conference tournament, and NCAA\u00ae tournament games since the 2009-10 season Data Section 3 file: Cities.csv This file provides a master list of cities that have been locations for games played. Please notice that the Cities and Conferences files are the only two that don't start with an W; this is because the data files are identical between men's and women's data, so you don't need to maintain separate listings of cities or conferences across the two contests. Data Section 3 file: WGameCities.csv This file identifies all games, starting with the 2010 season, along with the city that the game was played in. Games from the regular season and the NCAA\u00ae tourney are all listed together. The CityID is present in more than 98% of games since the 2010 season. Games from the 2009 season and before are not listed in this file. This section contains additional supporting information, including alternative team name spellings and representations of bracket structure Data Section 4 file: WTeamSpellings.csv This file indicates alternative spellings of many team names. It is intended for use in associating external spellings against our own TeamID numbers, thereby helping to relate the external data properly with our datasets. Over the years we have identified various external spellings of different team names (as an example, for Ball State we have seen \"ball st\", and \"ball st.\", and \"ball state\", and \"ball-st\", and \"ball-state\"). Other teams have had more significant changes to their names over the years; for example, \"Texas Pan-American\" and \"Texas-Rio Grande Valley\" are actually the same school. The current list is obviously not exhaustive, and we encourage participants to identify additional mappings and upload extended versions of this file to the forums. Data Section 4 file: WNCAATourneySlots This file identifies the mechanism by which teams are paired against each other, depending upon their seeds, as the tournament proceeds through its rounds. It can be of use in identifying, for a given historical game, what round it occurred in, and what the seeds/slots were for the two teams (the meaning of \"slots\" is described below). You may need to know these specifics if you are trying to represent/simulate the exact workings of the tournament bracket. Unlike the analogous file on the men's side, it is not necessary to provide a Season within this file, because the women's tournament has never had play-in-games and so the 64-team women's bracket has always had the same structure each season. Data Section 4 file: Conferences.csv This file indicates the Division I conferences that have existed over the years since 1985. Each conference is listed with an abbreviation and a longer name. There has been no attempt to link up conferences who merged with other conferences, or whose names changed over time. Thus you will see, for instance, a \"Pacific-10\" conference up through the 2011 season, and then a \"Pacific-12\" conference starting in the 2012 season, and these look like different conferences in the data, even though it was still mostly the same teams. Please notice that the Cities and Conferences files are the only two that don't start with an M; this is because the data files are identical between men's and women's data, so you don't need to maintain separate listings of cities or conferences across the two contests. However, the Team Conferences data differs slightly between men's and women's, so those files do have the prefixes. Data Section 4 file: WTeamConferences.csv This file indicates the conference affiliations for each team during each season. Some conferences have changed their names from year to year, and/or changed which teams are part of the conference. This file tracks this information historically. <hr"
    },
    {
        "name": "Foursquare - Location Matching",
        "url": "https://www.kaggle.com/competitions/foursquare-location-matching",
        "overview_text": "Overview text not found",
        "description_text": "When you look for nearby restaurants or plan an errand in an unknown area, you expect relevant, accurate information. To maintain quality data worldwide is a challenge, and one with implications beyond navigation. Businesses make decisions on new sites for market expansion, analyze the competitive landscape, and show relevant ads informed by location data. For these, and many other uses, reliable data is critical. Large-scale datasets on commercial points-of-interest (POI) can be rich with real-world information. To maintain the highest level of accuracy, the data must be matched and de-duplicated with timely updates from multiple sources. De-duplication involves many challenges, as the raw data can contain noise, unstructured information, and incomplete or inaccurate attributes. A combination of machine-learning algorithms and rigorous human validation methods are optimal to de-dupe datasets. With 12+ years of experience perfecting such methods, Foursquare is the #1 independent provider of global POI data. The leading independent location technology and data cloud platform, Foursquare is dedicated to building meaningful bridges between digital spaces and physical places. Trusted by leading enterprises like Apple, Microsoft, Samsung, and Uber, Foursquare\u2019s tech stack harnesses the power of places and movement to improve customer experiences and drive better business outcomes. In this competition, you\u2019ll match POIs together. Using a dataset of over one-and-a-half million Places entries heavily altered to include noise, duplications, extraneous, or incorrect information, you'll produce an algorithm that predicts which Place entries represent the same point-of-interest. Each Place entry includes attributes like the name, street address, and coordinates. Successful submissions will identify matches with the greatest accuracy. By efficiently and successfully matching POIs, you'll make it easier to identify where new stores or businesses would benefit people the most.",
        "dataset_text": "The data presented here comprises over one-and-a-half million place entries for hundreds of thousands of commercial Points-of-Interest (POIs) around the globe. Your task is to determine which place entries describe the same point-of-interest. Though the data entries may represent or resemble entries for real places, they may also contain artificial information or additional noise. To help you author submission code, we include a few example instances selected from the test set. When you submit your notebook for scoring, this example data will be replaced by the actual test data. The actual test set has approximately 600,000 place entries with POIs that are distinct from the POIs in the training set."
    },
    {
        "name": "Novozymes Enzyme Stability Prediction",
        "url": "https://www.kaggle.com/competitions/novozymes-enzyme-stability-prediction",
        "overview_text": "Overview text not found",
        "description_text": "Enzymes are proteins that act as catalysts in the chemical reactions of living organisms. The goal of this competition is to predict the thermostability of enzyme variants. The experimentally measured thermostability (melting temperature) data includes natural sequences, as well as engineered sequences with single or multiple mutations upon the natural sequences. Understanding and accurately predict protein stability is a fundamental problem in biotechnology. Its applications include enzyme engineering for addressing the world\u2019s challenges in sustainability, carbon neutrality and more. Improvements to enzyme stability could lower costs and increase the speed scientists can iterate on concepts. Novozymes finds enzymes in nature and optimizes them for use in industry. In industry, enzymes replace chemicals and accelerate production processes. They help our customers make more from less, while saving energy and generating less waste. Enzymes are widely used in laundry and dishwashing detergents where they remove stains and enable low-temperature washing and concentrated detergents. Other enzymes improve the quality of bread, beer and wine, or increase the nutritional value of animal feed. Enzymes are also used in the production of biofuels where they turn starch or cellulose from biomass into sugars which can be fermented to ethanol. These are just a few examples as we sell enzymes to more than 40 different industries. Like enzymes, microorganisms have natural properties that can be put to use in a variety of processes. Novozymes supplies a range of microorganisms for use in agriculture, animal health and nutrition, industrial cleaning and wastewater treatment. However, many enzymes are only marginally stable, which limits their performance under harsh application conditions. Instability also decreases the amount of protein that can be produced by the cell. Therefore, the development of efficient computational approaches to predict protein stability carries enormous technical and scientific interest. Computational protein stability prediction based on physics principles have made remarkable progress thanks to advanced physics-based methods such as FoldX, Rosetta, and others. Recently, many machine learning methods were proposed to predict the stability impact of mutations on protein based on the pattern of variation in natural sequences and their three dimensional structures. More and more protein structures are being solved thanks to the recent breakthrough of AlphaFold2. However, accurate prediction of protein thermal stability remains a great challenge. In this competition, Novozymes invites you to develop a model to predict/rank the thermostability of enzyme variants based on experimental melting temperature data, which is obtained from Novozymes\u2019s high throughput screening lab. You\u2019ll have access to data from previous scientific publications. The available thermostability data spans from natural sequences to engineered sequences with single or multiple mutations upon the natural sequences. If successful, you'll help tackle the fundamental problem of improving protein stability, making the approach to design novel and useful proteins, like enzymes and therapeutics, more rapidly and at lower cost. Novozymes is the world\u2019s leading biotech powerhouse. Our growing world is faced with pressing needs, emphasizing the necessity for solutions that can ensure the health of the planet and its population. At Novozymes, we believe biotech is at the core of connecting those societal needs with the challenges and opportunities our customers face. Novozymes is the global market leader in biological solutions, producing a wide range of enzymes, microorganisms, technical and digital solutions which help our customers, amongst other things, add new features to their products and produce more from less. Together, we find biological answers for better lives in a growing world. Let\u2019s Rethink Tomorrow. This is Novozymes\u2019 purpose statement. Novozymes strives to have great impact by balancing good business for our customers and our company, while spearheading environmental and social change. In 2021, Novozymes enabled savings of 60 million tons of CO2 in global transport.",
        "dataset_text": "In this competition, you are asked to develop models that can predict the ranking of protein thermostability (as measured by melting point, tm) after single-point amino acid mutation and deletion. For the training set, the protein thermostability (experimental melting temperature) data includes natural sequences, as well as engineered sequences with single or multiple mutations upon the natural sequences. The data are mainly from different sources of published studies such as Meltome atlas\u2014thermal proteome stability across the tree of life. Many other public datasets exist for protein stability; please see the competition Rule 7C for external data usage requirements. There are also other publicly available methods which can predict protein stabilities such as ESM, EVE and Rosetta etc., without using the provided training set. These methods may also be used as part of the competition. The test set contains experimental melting temperature of over 2,413 single-mutation variant of an enzyme (GenBank: KOC15878.1), obtained by Novozymes A/S. The amino acid sequence of the wild type is: VPVNPEPDATSVENVALKTGSGDSQSDPIKADLEVKGQSALPFDVDCWAILCKGAPNVLQRVNEKTKNSNRDRSGANKGPFKDPQKWGIKALPPKNPSWSAQDFKSPEEYAFASSLQGGTNAILAPVNLASQNSQGGVLNGFYSANKVAQFDPSKPQQTKGTWFQITKFTGAAGPYCKALGSNDKSVCDKNKNIAGDWGFDPAKWAYQYDEKNNKFNYVGK"
    },
    {
        "name": "DFL - Bundesliga Data Shootout",
        "url": "https://www.kaggle.com/competitions/dfl-bundesliga-data-shootout",
        "overview_text": "Overview text not found",
        "description_text": " Goal! In this competition, you'll detect football (soccer) passes\u2014including throw-ins and crosses\u2014and challenges in original Bundesliga matches. You'll develop a computer vision model that can automatically classify these events in long video recordings. Your work will help scale the data collection process. Automatic event annotation could enable event data from currently unexplored competitions, like youth or semi-professional leagues or even training sessions. What does it take to go pro in football (soccer)? From a young age, hopeful talents devote time, money, and training to the sport. Yet, while the next superstar is guaranteed to start off in youth or semi-professional leagues, these leagues often have the fewest resources to invest. This includes resources for the collection of event data which helps generate insights into the performance of the teams and players. Currently, event data is mostly collected manually by human operators, who gather data in several steps and through numerous personnel involved. This manual process has room for innovation as in its current shape and form it involves a lot of resources and multiple iterations/quality checks. As a result, event data collection is usually reserved for professional competitions only. Based in Frankfurt, the Deutsche Fu\u00dfball Liga (DFL) manages Germany's professional football (soccer) leagues: Bundesliga and Bundesliga 2. DFL partners with the operator of one of the largest sports databases in the world, Sportec Solutions. They're responsible for the leagues' sports data and sports technology activities. In addition, Sportec Solutions provides services to global sports entities and media companies. Automatic event detection could provide event data faster and with greater depth. Having access to a broader range of competitions, match conditions and data scouts would be able to ensure no talented player is overlooked. ",
        "dataset_text": "The competition dataset comprises video recordings of nine football games divided into halves. You're challenged to detect three kinds of player events, both the time of occurrence and the type, within these videos. See the Event Descriptions page for a full description of each event type. This is a Code Competition that will run in two stages. During the training stage, your submission will only be run against test data for the public leaderboard. The test data for the private leaderboard, however, will comprise games occuring after the training period closes, the forecasting stage. Please note that this is a Code Competition, which means you will submit a notebook that makes predictions against a hidden test set. The test set visible here is only an example to help you author submission code. When you submit your notebook for scoring, this example data will be replaced by the actual test data. Note in particular that the example test videos are only short clips (~30 sec.), while the true hidden test videos are full halves (~50 min.). The name and number of videos may also be different. To ensure your submission completes successfully, you should make your code robust to such variations. See our Code Competition Debugging page for tips."
    },
    {
        "name": "Open Problems - Multimodal Single-Cell Integration",
        "url": "https://www.kaggle.com/competitions/open-problems-multimodal",
        "overview_text": "Overview text not found",
        "description_text": "The goal of this competition is to predict how DNA, RNA, and protein measurements co-vary in single cells as bone marrow stem cells develop into more mature blood cells. You will develop a model trained on a subset of 300,000-cell time course dataset of CD34+ hematopoietic stem and progenitor cells (HSPC) from four human donors at five time points generated for this competition by Cellarity, a cell-centric drug creation company. In the test set, taken from an unseen later time point in the dataset, competitors will be provided with one modality and be tasked with predicting a paired modality measured in the same cell. The added challenge of this competition is that the test data will be from a later time point than any time point in the training data. Your work will help accelerate innovation in methods of mapping genetic information across layers of cellular state. If we can predict one modality from another, we may expand our understanding of the rules governing these complex regulatory processes.  In the past decade, the advent of single-cell genomics has enabled the measurement of DNA, RNA, and proteins in single cells. These technologies allow the study of biology at an unprecedented scale and resolution. Among the outcomes have been detailed maps of early human embryonic development, the discovery of new disease-associated cell types, and cell-targeted therapeutic interventions. Moreover, with recent advances in experimental techniques it is now possible to measure multiple genomic modalities in the same cell. While multimodal single-cell data is increasingly available, data analysis methods are still scarce. Due to the small volume of a single cell, measurements are sparse and noisy. Differences in molecular sampling depths between cells (sequencing depth) and technical effects from handling cells in batches (batch effects) can often overwhelm biological differences. When analyzing multimodal data, one must account for different feature spaces, as well as shared and unique variation between modalities and between batches. Furthermore, current pipelines for single-cell data analysis treat cells as static snapshots, even when there is an underlying dynamical biological process. Accounting for temporal dynamics alongside state changes over time is an open challenge in single-cell data science. Generally, genetic information flows from DNA to RNA to proteins. DNA must be accessible (ATAC data) to produce RNA (GEX data), and RNA in turn is used as a template to produce protein (ADT data). These processes are regulated by feedback: for example, a protein may bind DNA to prevent the production of more RNA. This genetic regulation is the foundation for dynamic cellular processes that allow organisms to develop and adapt to changing environments. In single-cell data science, dynamic processes have been modeled by so-called pseudotime algorithms that capture the progression of the biological process. Yet, generalizing these algorithms to account for both pseudotime and real time is still an open problem. Competition host Open Problems in Single-Cell Analysis is an open-source, community-driven effort to standardize benchmarking of single-cell methods. The core efforts of Open Problems include the formalization of existing challenges into measurable tasks, a collection of high-quality datasets, centralized benchmarking of community-contributed methods, and community-focused events that bring together diverse method developers to improve single-cell algorithms. They're excited to be partnering with Cellarity, Chan Zuckerbeg Biohub, the Chan Zuckerberg Initiative, Helmholtz Munich, and Yale to see what progress can be made in predicting changes in genetic dynamics over time through interdisciplinary collaboration. There are approximately 37 trillion cells in the human body, all with different behaviors and functions. Understanding how a single genome gives rise to a diversity of cellular states is the key to gaining mechanistic insight into how tissues function or malfunction in health and disease. You can help solve this fundamental challenge for single-cell biology. Being able to solve the prediction problems over time may yield new insights into how gene regulation influences differentiation as blood and immune cells mature. Competition header image by Pawel Czerwinski on Unsplash",
        "dataset_text": "The dataset for this competition comprises single-cell multiomics data collected from mobilized peripheral CD34+ hematopoietic stem and progenitor cells (HSPCs) isolated from four healthy human donors. More information about the cells can be found on the vendor website. Measurements were taken at five time points over a ten-day period. During this time, cells were cultured with StemSpan SFEM media supplemented with CC100 and thrombopoietin (TPO) and incubated at 37\u00baC. Media was changed every 2-3 days. No additional media supplements were added to the cell culture conditions. From each culture plate at each sampling time point, cells were collected for measurement with two single-cell assays. The first is the 10x Chromium Single Cell Multiome ATAC + Gene Expression technology (Multiome) and the second is the 10x Genomics Single Cell Gene Expression with Feature Barcoding technology technology using the TotalSeq\u2122-B Human Universal Cocktail, V1.0 (CITEseq). If you've never worked with this data type before, we've included some links at the bottom of this description. Each assay technology measures two modalities. The Multiome kit measures chromatin accessibility (DNA) and gene expression (RNA), while the CITEseq kit measures gene expression (RNA) and surface protein levels. Following the central dogma of molecular biology: DNA --> RNA-->Protein, your task is as follows: To help guide your analysis, we performed a preliminary cell type annotation based on the RNA gene expression using information from the following paper: https://www.nature.com/articles/ncb3493. Note, cell type annotation is an imprecise art, and the concept of assigning discrete labels to continuous data has inherent limitations. You do not need to use these labels in your predictions; they are primarily provided to guide exploratory analysis. In the data, there are the following cell types: The experimental observations are contained in several large arrays. We provide these arrays in HDF5 format. The data splits are arranged as follows:  Your task is to predict the labels corresponding to the inputs in the test set. To facilitate submission scoring, we only require predictions on a subset of the Multiome data. This subset was created by sampling 30% of the Multiome rows, and for each row, 15% of the columns. The sample of columns varies from row-to-row. All of the CITEseq labels are scored."
    },
    {
        "name": "GigaOM WordPress Challenge: Splunk Innovation Prospect",
        "url": "https://www.kaggle.com/competitions/predict-wordpress-likes",
        "overview_text": "Overview text not found",
        "description_text": "Splunk Innovation Prize now OPEN \u00bb   Announced at the GigaOM Structure Conference, powered by Splunk, and using data from WordPress.com, this competition is about predicting which people will \"like\" which blog posts from across 90k active blogs on WordPress.com.  WordPress.com hosts about half of the 74 million WordPress sites in the world (over 16% of all domains on the web). The winning solutions may be used by WordPress.com in a recommendation engine, but winning solutions must be open-sourced, so they could be used by anyone to solve a similar problem using similar data in a similar domain. Competition winners will be announced in September at GigaOM Mobilize. About Splunk: Splunk\u00ae Inc. provides the engine for machine data\u2122. Splunk software enables organizations to monitor, search, analyze, visualize and act on massive streams of real-time and historical machine data.  Splunk has donated access to a Splunk server containing the entire WordPress dataset for you to explore, visualize and experiment. When you accept the rules for the competition, you will automatically be sent a personal login to the Splunk server. There is also a 5K companion competition to the predictive modeling challenge. The Splunk Innovation Prize will be awarded for the most innovative use of Splunk for data science (using the competition dataset). Never used Splunk before?  Here are some tutorials to get you started. About Gigaom: GigaOM is one of the most credible and insightful voices at the intersection of business and technology, with an online audience of more than 5.5 million monthly unique visitors, industry-leading events, and a pioneering research service and digital community, GigaOM Pro, which provides expert analysis on emerging technology markets.    ",
        "dataset_text": "Data for the competition is also available for immediate exploration through SocialSplunk.  A login code will be sent to you when you register for the competition.  Haven't received your login code yet?  Email support@kaggle.com and we'll get you sorted out. The evaluation data currently only drive the public leaderboard. The private leaderboard is not yet active. To avoid a situation where people could look up the answers on the web, we have a two-phase data release. First Data Release The data for the first release are drawn from a 6-week period of blog posts and \"likes\" of those blog posts. The training data consist of the first 5 weeks of posts and \"likes\" that occurred during those 5 weeks. This data files provided (some of which contain redundant information in different forms): The \"test\" users are restricted to users who have \"liked\" at least 1 post in the test period and at least 5 posts in the train period. While the test set is restricted like this, no such restriction has been made on the data provided in the training set. At the end of September 1, 2012 (measured by UDT), the contest will be closed to new submissions and the only submissions eligible to win will be those that have attached the complete code necessary for generating the submission. At that point, the second phase of data will be released. Second Data Release The private leaderboard (final evaluation) data will be drawn from a future 6 week period. These 6 future weeks will be divided in the same way as the First Data Release, and prior aggregate data from the beginning of the new 6-week period will also be available. After the Second Data Release, contestants will have one week to generate predictions based on their previously submitted code. This previously submitted code must be able to generate the new predictions with no human input or judgment. At the end of that week, preliminary winners will be announced. Kaggle will then make public the code that the preliminary winners had previously submitted with their entries. There will then be a two week period during which participants or other individuals will have a chance to replicate the results and (potentially) challenge the preliminary winners with violating the contest rules, or not having presented code that creates the results claimed.  In case of disputes during the verification process, Kaggle will select a panel to adjudicate. "
    },
    {
        "name": "Belkin Energy Disaggregation Competition",
        "url": "https://www.kaggle.com/competitions/belkin-energy-disaggregation-competition",
        "overview_text": "Overview text not found",
        "description_text": "Imagine an energy feedback system that displays not only your total power consumption, but also continuously shows real-time usage, broken down by electrical appliance. Such a system could provide personalized and cost-effective energy saving recommendations. For example, it could report, \"Based on your usage patterns, you could save $215 per year by switching to a more efficient heating unit, which will pay for itself in 27 months.\" The challenge in this scenario is to sense end-uses of energy to provide feedback at the fine-grained, appliance level. There has been substantial prior research in this area [1,5,6,7,8], however most of this work has concentrated on the use of power consumption patterns and using changes in power draw as features to identify what appliance is being used and how much energy it is consuming. We recommend the reader refer to [2,3,9] for a detailed overview of machine learning features for energy disaggregation. A more recent approach to estimate appliance usage is to examine the Electromagnetic Interference (EMI) that most consumer electronic appliances produce as identifying signatures [4]. This EMI is measured using a special sensor built at the Ubicomp Lab at the University of Washington as part of Sidhant Gupta's thesis work. The figure below shows an example of EMI captured from a home. The plot is in frequency domain and shows the signatures of various appliances.  The presence or absence of such EMI signatures would ideally tell us when a particular appliance is in use. However, due to the large numbers of appliances in a home, the solution is not straightforward. Machine learning is required not only to make an inference about the appliance class given a particular signature, but probabilistic models are needed that take into account, for example, human appliance usage patterns (think using coffee machine and toaster in morning vs. lights in evening), weather patterns (very unlikely that AC came on during winters), and appliance electrical model. The signature of an appliance can also drift or vary over time due to operating conditions and the mode in which they are used (for instance, a washing machine has many modes). We encourage participants to review [4] to better understand the use of EMI for electrical appliance use detection and classification. Here are a few lab quality videos that may helo you grasp the big picture: Video of the signal: http://youtu.be/o-SqO8y8XUA\nVideo of the technology applied to energy monitoring: http://www.youtube.com/watch?v=dcPI1Cp0VZI Slides from conference talk for ElectriSense can be accessed here: http://homes.cs.washington.edu/~sidhant/slides/ElectriSense_PDF.pdf 1. Berges, M., Goldman, E., Matthews, H.S., and Soibelman, L. Training Load Monitoring Algorithms on Highly Sub-Metered Home Electricity Consumption Data. Tsinghua Science & Technology 13, Supple, 0 (2008), 406\u2013411. 2. Carrie Armel, K., Gupta, A., Shrimali, G., and Albert, A. Is disaggregation the holy grail of energy efficiency? The case of electricity. Energy Policy 52, (2012), 213\u2013234. 3. Froehlich, J., Larson, E., Gupta, S., Cohn, G., Reynolds, M., and Patel, S. Disaggregated End-Use Energy Sensing for the Smart Grid. IEEE Pervasive Computing 10, 1 (2011), 28\u201339. 4. Gupta, S., Reynolds, M., and Patel, S. ElectriSense: Single-Point Sensing Using EMI for Electrical Event Detection and Classification in the home. Ubicomp 2010, (2010). 5. Hart, G. Nonintrusive appliance load monitoring. Proceedings of the IEEE, (1992). 6. Laughman, C., Lee, K., and Cox, R. Power signature analysis. IEEE Power and Energy, april 2003 (2003). 7. Leeb, S.B., Shaw, S.R., and Kirtley, J.L. Transient Event Detection in Spectral Envelope Estimates. IEEE Transactions on Power Delivery 10, 3 (1995), 1200\u20131210. 8. Norford, L.K. and Leeb, S.B. Non-intrusive electrical load monitoring in commercial buildings based on steady-state and transient load-detection algorithms. Energy and Buildings 24, 1 (1996), 51\u201364. 9. Zeifman, M., Ph, D., and Roth, K. Non-Intrusive Appliance Load Monitoring ( NIALM ): Review and Outlook * Fraunhofer\u202f: A Leading Force in Applied R & D. Consumer Electronics, January (2011). ",
        "dataset_text": "You do not need to download both versions of the files. H1.zip - H4.zip = official competition data in .mat format\nH1_CSV.zip - H4_CSV.zip = unofficial competition data in .csv format   You are provided with data from 4 homes (H1-H4) consisting of both training datasets and testing datasets. The goal is to use the training datasets to learn how each appliance in each home looks and behaves from a machine-learning perspective and build a model which can be applied to the test datasets to make predictions.  Refer to SampleSubmission.csv to understand which appliances to predict at which times.  The official data for this competition is provided as MATLAB .mat files. This is the language and storage structure being being used by the prototype and its researchers. We recognize that many people do not have access to MATLAB. As a courtesy, we have provided .csv files in addition to the .mat files. Note that the conversion results in some loss of decimal precision, contains complex numbers, and has scientific notation. The uncompressed .csv files can also be quite large. These files are provided without any guarantees regarding their correctness or usability! Participants without MATLAB are encouraged to use the forums to discuss issues around loading/converting the data.  Note that in addition to the free language OCTAVE, there are packages in most other languages to read and write .mat files.  Python/SciPy/NumPy works well for loading the Matlab files (see scipy.io.loadmat). Data for each home is placed in its respective directory. You will find 3 kinds of files for each home: AllTaggingInfo.mat\nThis file contains Nx4 elements, where N is the number of labeled training examples provided for a particular home. Since training examples can span multiple days and hence multiple files (see item 2 below), we have included this file such that all of the training labels are in one place. Each row is of the following form: Tagged_Training_*.mat\nThese MATLAB files are the actual data from Belkin\u2019s proprietary hardware. A detailed description of the contents of these files is provided later, however files beginning with \u201cTagged_Training_\u201d are those that are provided as part of the training data. The reason these are training sets are because they include information about which appliance was turned ON or OFF and at what timestamps. This information is embedded in the .mat file and named \u201cTaggingInfo\u201d. The TaggingInfo array is also Nx4 with same format as the AllTaggingInfo.mat. Essentially, if all TaggingInfo arrays for each home were concatenated, it would yield the global AllTaggingInfo.mat file. Testing_*.mat\nThese files are similar to the Tagged_Training_*.mat files, except they do not include a TaggingInfo structure. Instead, participants are expected to figure out when and which appliance is being operated. Note: All timestamps in the entire dataset are in the UNIX time stamp format. If you load any of the training or testing mat files, it will result in a structure called Buffer that has several member elements:                   *Only present in training data files. For bootstrapping purposes, we have provided sample matlab scripts that can be used to both load and analyze the data files. These scripts can be found in the \u201cLoaderScripts\u201d folder. The main file to load is \u201cLoadData.m\u201d. By default, we load data for H3 and select the first training file. The script is commented and is expected to be self-explanatory such that it can be adapted as needed. It is provided only as an example to understand how the Buffer can be processed to yield certain features like real, reactive and apparent power in addition to the power factors for each phase. The following figure shows the resulting figure when the script is run. A particular section of the plot is zoomed into for clarity.  The default script assumes that the LoadData.m is run with the current directory set to \\LoaderScripts. The DATA_DIR_PATH variable in the script can be changed to point to the correct absolute path as necessary."
    },
    {
        "name": "U.S. Census Return Rate Challenge",
        "url": "https://www.kaggle.com/competitions/us-census-challenge",
        "overview_text": "Overview text not found",
        "description_text": "Note: The prediction phase of this competition has ended. Please join the visualization competition which ends on Nov. 11, 2012. -- This challenge is to develop a statistical model to predict census mail return rates at the Census block group level of geography. The Census Bureau will use this model for planning purposes for the decennial census and for demographic sample surveys. The model-based estimates of predicted mail return will be publicly released in a later version of the Census \"planning database\" containing updated demographic data. Participants are encouraged to develop and evaluate different statistical approaches to proposing the best predictive model for geographic units. The intent is to improve our current predictive analytics. Please note also that as described in the rules, only US citizens and residents are eligible for prizes.",
        "dataset_text": "Each census tract (these are one level larger than block group) have been randomly assigned to \"Training\", \"Public Leaderboard\", or \"Private Leaderboard\". So all block groups from a given tract are in the same set. You're predicting Mail_Return_Rate_CEN_2010, the census mail return rate for each block group.  Please note: There is an issue with \"GIDBG\" for the original files. This has been fixed, and new files have been posted as \"training_filev1\" and \"test_filev1\". To prevent confusion, the original files are still available."
    },
    {
        "name": "Personalize Expedia Hotel Searches - ICDM 2013",
        "url": "https://www.kaggle.com/competitions/expedia-personalized-sort",
        "overview_text": "Overview text not found",
        "description_text": "  Expedia is the world\u2019s largest online travel agency (OTA) and powers search results for millions of travel shoppers every day. In this competitive market matching users to hotel inventory is very important since users easily jump from website to website. As such, having the best ranking of hotels (\u201csort\u201d) for specific users with the best integration of price competitiveness gives an OTA the best chance of winning the sale. For this contest, Expedia has provided a dataset that includes shopping and purchase data as well as information on price competitiveness. The data are organized around a set of \u201csearch result impressions\u201d, or the ordered list of hotels that the user sees after they search for a hotel on the Expedia website. In addition to impressions from the existing algorithm, the data contain impressions where the hotels were randomly sorted, to avoid the position bias of the existing algorithm. The user response is provided as a click on a hotel or/and a purchase of a hotel room. Appended to impressions are the following: 1) Hotel characteristics\n2) Location attractiveness of hotels\n3) User\u2019s aggregate purchase history\n4) Competitive OTA information Models will be scored via performance on a hold-out set.",
        "dataset_text": "Sample code to create the benchmarks is available on Github. Note: test.csv does not contain the following columns: position, click_bool, gross_bookings_usd, nor booking_bool  You can refer to www.expedia.com to better understand hotel search. \u201cHotel\u201d refers to hotels, apartments, B&Bs, hostels and other properties appearing on Expedia\u2019s websites.  Room types are not distinguished and the data can be assumed to apply to the least expensive room type.  Most of the data are for searches that resulted in a purchase, but a small proportion are for searches not leading to a purchase. Usage of outside data is prohibited and modeling should focus fully on the given data.     "
    },
    {
        "name": "NOAA Fisheries Steller Sea Lion Population Count",
        "url": "https://www.kaggle.com/competitions/noaa-fisheries-steller-sea-lion-population-count",
        "overview_text": "Overview text not found",
        "description_text": " Steller sea lions in the western Aleutian Islands have declined 94 percent in the last 30 years. The endangered western population, found in the North Pacific, are the focus of conservation efforts which require annual population counts. Specially trained scientists at NOAA Fisheries Alaska Fisheries Science Center conduct these surveys using airplanes and unoccupied aircraft systems to collect aerial images. Having accurate population estimates enables us to better understand factors that may be contributing to lack of recovery of Stellers in this area. Currently, it takes biologists up to four months to count sea lions from the thousands of images NOAA Fisheries collects each year. Once individual counts are conducted, the tallies must be reconciled to confirm their reliability. The results of these counts are time-sensitive. In this competition, Kagglers are invited to develop algorithms which accurately count the number of sea lions in aerial photographs. Automating the annual population count will free up critical resources allowing NOAA Fisheries to focus on ensuring we hear the sea lion\u2019s roar for many years to come. Plus, advancements in computer vision applied to aerial population counts may also greatly benefit other endangered species. Learn more about research being done to better understand what's going on with the endangered Steller sea lion populations by joining scientists on a research vessel to the western Aleutian Islands in the video below. ",
        "dataset_text": "The goal of this competition is to estimate the number of each type of sea lion in each one of the test images. The different types of sea lion are: adult males (also known as bulls), subadult males, adult females, juveniles, and pups. Your submission file should have the following format: Your submissions will be evaluated by their RMSE from the human-labelled ground truth, averaged over the columns. The file KaggleNOAASeaLions.7z is an encrypted 7-zip archive. An identical file is available via BitTorrent. Please do not share the archive password. The archive contains the following items: Update: some training images do not match their equivalents in TrainDotted. A list is posted in the MismatchedTrainImages.txt file. In case you're not ready to torrent the main competition data file, we've added a small subset of the training data for you to get started on: TrainSmall2.7z. That archive contains the regular and dotted versions of training images 41 to 50, plus the sample submission file. (Update: this file replaces the original TrainSmall.7z, which included two images that did not match their TrainDotted versions correctly.) In both the training set and the test set, most images have regions that have been blacked out. Those redactions were added by human labelers to avoid double-counting. You can ignore any black regions when generating your predictions. As an anti-cheating measure, Kaggle has added some extra images to the test set which will not be counted towards your score. To assist you in locating individual sea lions, the TrainDotted folder contains the same images as in the Train folder, but with colored dots placed over the animals. The color scheme of the dots is: An example image from the training set is reproduced below. Good luck! "
    },
    {
        "name": "Quora Question Pairs",
        "url": "https://www.kaggle.com/competitions/quora-question-pairs",
        "overview_text": "Overview text not found",
        "description_text": "Where else but Quora can a physicist help a chef with a math problem and get cooking tips in return? Quora is a place to gain and share knowledge\u2014about anything. It\u2019s a platform to ask questions and connect with people who contribute unique insights and quality answers. This empowers people to learn from each other and to better understand the world. Over 100 million people visit Quora every month, so it's no surprise that many people ask similarly worded questions. Multiple questions with the same intent can cause seekers to spend more time finding the best answer to their question, and make writers feel they need to answer multiple versions of the same question. Quora values canonical questions because they provide a better experience to active seekers and writers, and offer more value to both of these groups in the long term. Currently, Quora uses a Random Forest model to identify duplicate questions. In this competition, Kagglers are challenged to tackle this natural language processing problem by applying advanced techniques to classify whether question pairs are duplicates or not. Doing so will make it easier to find high quality answers to questions resulting in an improved experience for Quora writers, seekers, and readers.",
        "dataset_text": "The goal of this competition is to predict which of the provided pairs of questions contain two questions with the same meaning. The ground truth is the set of labels that have been supplied by human experts. The ground truth labels are inherently subjective, as the true meaning of sentences can never be known with certainty. Human labeling is also a 'noisy' process, and reasonable people will disagree. As a result, the ground truth labels on this dataset should be taken to be 'informed' but not 100% accurate, and may include incorrect labeling. We believe the labels, on the whole, to represent a reasonable consensus, but this may often not be true on a case by case basis for individual items in the dataset. Please note: as an anti-cheating measure, Kaggle has supplemented the test set with computer-generated question pairs. Those rows do not come from Quora, and are not counted in the scoring. All of the questions in the training set are genuine examples from Quora."
    },
    {
        "name": "Mercedes-Benz Greener Manufacturing",
        "url": "https://www.kaggle.com/competitions/mercedes-benz-greener-manufacturing",
        "overview_text": "Overview text not found",
        "description_text": "Since the first automobile, the Benz Patent Motor Car in 1886, Mercedes-Benz has stood for important automotive innovations. These include, for example, the passenger safety cell with crumple zone, the airbag and intelligent assistance systems. Mercedes-Benz applies for nearly 2000 patents per year, making the brand the European leader among premium car makers. Daimler\u2019s Mercedes-Benz cars are leaders in the premium car industry. With a huge selection of features and options, customers can choose the customized Mercedes-Benz of their dreams. . To ensure the safety and reliability of each and every unique car configuration before they hit the road, Daimler\u2019s engineers have developed a robust testing system. But, optimizing the speed of their testing system for so many possible feature combinations is complex and time-consuming without a powerful algorithmic approach. As one of the world\u2019s biggest manufacturers of premium cars, safety and efficiency are paramount on Daimler\u2019s production lines.  In this competition, Daimler is challenging Kagglers to tackle the curse of dimensionality and reduce the time that cars spend on the test bench. Competitors will work with a dataset representing different permutations of Mercedes-Benz car features to predict the time it takes to pass testing. Winning algorithms will contribute to speedier testing, resulting in lower carbon dioxide emissions without reducing Daimler\u2019s standards.",
        "dataset_text": " This dataset contains an anonymized set of variables, each representing a custom feature in a Mercedes car. For example, a variable could be 4WD, added air suspension, or a head-up display. The ground truth is labeled \u2018y\u2019 and represents the time (in seconds) that the car took to pass testing for each variable. Variables with letters are categorical. Variables with 0/1 are binary values."
    },
    {
        "name": "Carvana Image Masking Challenge",
        "url": "https://www.kaggle.com/competitions/carvana-image-masking-challenge",
        "overview_text": "Overview text not found",
        "description_text": "As with any big purchase, full information and transparency are key. While most everyone describes buying a used car as frustrating, it\u2019s just as annoying to sell one, especially online. Shoppers want to know everything about the car but they must rely on often blurry pictures and little information, keeping used car sales a largely inefficient, local industry. Carvana, a successful online used car startup, has seen opportunity to build long term trust with consumers and streamline the online buying process. An interesting part of their innovation is a custom rotating photo studio that automatically captures and processes 16 standard images of each vehicle in their inventory. While Carvana takes high quality photos, bright reflections and cars with similar colors as the background cause automation errors, which requires a skilled photo editor to change.  In this competition, you\u2019re challenged to develop an algorithm that automatically removes the photo studio background. This will allow Carvana to superimpose cars on a variety of backgrounds. You\u2019ll be analyzing a dataset of photos, covering different vehicles with a wide variety of year, make, and model combinations.",
        "dataset_text": "This dataset contains a large number of car images (as .jpg files). Each car has exactly 16 images, each one taken at different angles. Each car has a unique id and images are named according to id_01.jpg, id_02.jpg \u2026 id_16.jpg. In addition to the images, you are also provided some basic metadata about the car make, model, year, and trim. For the training set, you are provided a .gif file that contains the manually cutout mask for each image. The competition task is to automatically segment the cars in the images in the test set folder. To deter hand labeling, we have supplemented the test set with car images that are ignored in scoring. The metric used to score this competition requires that your submissions are in run-length encoded format. Please see the evaluation page for details, and train_masks.csv for a real example of what the encoding looks like."
    },
    {
        "name": "Porto Seguro\u2019s Safe Driver Prediction",
        "url": "https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction",
        "overview_text": "Overview text not found",
        "description_text": " Nothing ruins the thrill of buying a brand new car more quickly than seeing your new insurance bill. The sting\u2019s even more painful when you know you\u2019re a good driver. It doesn\u2019t seem fair that you have to pay so much if you\u2019ve been cautious on the road for years. Porto Seguro, one of Brazil\u2019s largest auto and homeowner insurance companies, completely agrees. Inaccuracies in car insurance company\u2019s claim predictions raise the cost of insurance for good drivers and reduce the price for bad ones. In this competition, you\u2019re challenged to build a model that predicts the probability that a driver will initiate an auto insurance claim in the next year. While Porto Seguro has used machine learning for the past 20 years, they\u2019re looking to Kaggle\u2019s machine learning community to explore new, more powerful methods. A more accurate prediction will allow them to further tailor their prices, and hopefully make auto insurance coverage more accessible to more drivers.",
        "dataset_text": "In this competition, you will predict the probability that an auto insurance policy holder files a claim. In the train and test data, features that belong to similar groupings are tagged as such in the feature names (e.g., ind, reg, car, calc). In addition, feature names include the postfix bin to indicate binary features and cat to indicate categorical features. Features without these designations are either continuous or ordinal. Values of -1 indicate that the feature was missing from the observation. The target columns signifies whether or not a claim was filed for that policy holder."
    },
    {
        "name": "Recruit Restaurant Visitor Forecasting",
        "url": "https://www.kaggle.com/competitions/recruit-restaurant-visitor-forecasting",
        "overview_text": "Overview text not found",
        "description_text": "Running a thriving local restaurant isn't always as charming as first impressions appear. There are often all sorts of unexpected troubles popping up that could hurt business. One common predicament is that restaurants need to know how many customers to expect each day to effectively purchase ingredients and schedule staff members. This forecast isn't easy to make because many unpredictable factors affect restaurant attendance, like weather and local competition. It's even harder for newer restaurants with little historical data. Recruit Holdings has unique access to key datasets that could make automated future customer prediction possible. Specifically, Recruit Holdings owns Hot Pepper Gourmet (a restaurant review service), AirREGI (a restaurant point of sales service), and Restaurant Board (reservation log management software). In this competition, you're challenged to use reservation and visitation data to predict the total number of visitors to a restaurant for future dates. This information will help restaurants be much more efficient and allow them to focus on creating an enjoyable dining experience for their customers.",
        "dataset_text": "In this competition, you are provided a time-series forecasting problem centered around restaurant visitors. The data comes from two separate sites: You must use the reservations, visits, and other information from these sites to forecast future restaurant visitor totals on a given date. The training data covers the dates from 2016 until April 2017. The test set covers the last week of April and May of 2017. The test set is split based on time (the public fold coming first, the private fold following the public) and covers a chosen subset of the air restaurants. Note that the test set intentionally spans a holiday week in Japan called the \"Golden Week.\" There are days in the test set where the restaurant were closed and had no visitors. These are ignored in scoring. The training set omits days where the restaurants were closed. This is a relational dataset from two systems. Each file is prefaced with the source (either air_ or hpg_) to indicate its origin. Each restaurant has a unique air_store_id and hpg_store_id. Note that not all restaurants are covered by both systems, and that you have been provided data beyond the restaurants for which you must forecast. Latitudes and Longitudes are not exact to discourage de-identification of restaurants. This file contains reservations made in the air system. Note that the reserve_datetime indicates the time when the reservation was created, whereas the visit_datetime is the time in the future where the visit will occur. This file contains reservations made in the hpg system. This file contains information about select air restaurants. Column names and contents are self-explanatory. Note: latitude and longitude are the latitude and longitude of the area to which the store belongs This file contains information about select hpg restaurants. Column names and contents are self-explanatory. Note: latitude and longitude are the latitude and longitude of the area to which the store belongs This file allows you to join select restaurants that have both the air and hpg system. This file contains historical visit data for the air restaurants. This file shows a submission in the correct format, including the days for which you must forecast. This file gives basic information about the calendar dates in the dataset."
    },
    {
        "name": "TensorFlow Speech Recognition Challenge",
        "url": "https://www.kaggle.com/competitions/tensorflow-speech-recognition-challenge",
        "overview_text": "Overview text not found",
        "description_text": "We might be on the verge of too many screens. It seems like everyday, new versions of common objects are \u201cre-invented\u201d with built-in wifi and bright touchscreens. A promising antidote to our screen addiction are voice interfaces.  But, for independent makers and entrepreneurs, it\u2019s hard to build a simple speech detector using free, open data and code. Many voice recognition datasets require preprocessing before a neural network model can be built on them. To help with this, TensorFlow recently released the Speech Commands Datasets. It includes 65,000 one-second long utterances of 30 short words, by thousands of different people. In this competition, you're challenged to use the Speech Commands Dataset to build an algorithm that understands simple spoken commands. By improving the recognition accuracy of open-sourced voice interface tools, we can improve product effectiveness and their accessibility. ",
        "dataset_text": "The following is a high level overview of the data for this competition. It is highly recommended that you read the supplementary materials found on the tabs of the Overview page. The following is the README contained in the Train folder, and contains more detailed information. This is a set of one-second .wav audio files, each containing a single spoken\nEnglish word. These words are from a small set of commands, and are spoken by a\nvariety of different speakers. The audio files are organized into folders based\non the word they contain, and this data set is designed to help train simple\nmachine learning models. It's licensed under the Creative Commons BY 4.0 license. See the LICENSE\nfile in this folder for full details. Its original location was at\nhttp://download.tensorflow.org/data/speech_commands_v0.01.tar.gz. This is version 0.01 of the data set containing 64,727 audio files, released on\nAugust 3rd 2017. The audio files were collected using crowdsourcing, see\naiyprojects.withgoogle.com/open_speech_recording\nfor some of the open source audio collection code we used (and please consider\ncontributing to enlarge this data set). The goal was to gather examples of\npeople speaking single-word commands, rather than conversational sentences, so\nthey were prompted for individual words over the course of a five minute\nsession. Twenty core command words were recorded, with most speakers saying each\nof them five times. The core words are \"Yes\", \"No\", \"Up\", \"Down\", \"Left\",\n\"Right\", \"On\", \"Off\", \"Stop\", \"Go\", \"Zero\", \"One\", \"Two\", \"Three\", \"Four\",\n\"Five\", \"Six\", \"Seven\", \"Eight\", and \"Nine\". To help distinguish unrecognized\nwords, there are also ten auxiliary words, which most speakers only said once.\nThese include \"Bed\", \"Bird\", \"Cat\", \"Dog\", \"Happy\", \"House\", \"Marvin\", \"Sheila\",\n\"Tree\", and \"Wow\". The files are organized into folders, with each directory name labelling the\nword that is spoken in all the contained audio files. No details were kept of\nany of the participants age, gender, or location, and random ids were assigned\nto each individual. These ids are stable though, and encoded in each file name\nas the first part before the underscore. If a participant contributed multiple\nutterances of the same word, these are distinguished by the number at the end of\nthe file name. For example, the file path happy/3cfc6b3a_nohash_2.wav\nindicates that the word spoken was \"happy\", the speaker's id was \"3cfc6b3a\", and\nthis is the third utterance of that word by this speaker in the data set. The\n'nohash' section is to ensure that all the utterances by a single speaker are\nsorted into the same training partition, to keep very similar repetitions from\ngiving unrealistically optimistic evaluation scores. The audio clips haven't been separated into training, test, and validation sets\nexplicitly, but by convention a hashing function is used to stably assign each\nfile to a set. Here's some Python code demonstrating how a complete file path\nand the desired validation and test set sizes (usually both 10%) are used to\nassign a set: The results of running this over the current set are included in this archive as\nvalidation_list.txt and testing_list.txt. These text files contain the paths to\nall the files in each set, with each path on a new line. Any files that aren't\nin either of these lists can be considered to be part of the training set. The original audio files were collected in uncontrolled locations by people\naround the world. We requested that they do the recording in a closed room for\nprivacy reasons, but didn't stipulate any quality requirements. This was by\ndesign, since we wanted examples of the sort of speech data that we're likely to\nencounter in consumer and robotics applications, where we don't have much\ncontrol over the recording equipment or environment. The data was captured in a\nvariety of formats, for example Ogg Vorbis encoding for the web app, and then\nconverted to a 16-bit little-endian PCM-encoded WAVE file at a 16000 sample\nrate. The audio was then trimmed to a one second length to align most\nutterances, using the\nextract_loudest_section\ntool. The audio files were then screened for silence or incorrect words, and\narranged into folders by label. To help train networks to cope with noisy environments, it can be helpful to mix\nin realistic background audio. The _background_noise_ folder contains a set of\nlonger audio clips that are either recordings or mathematical simulations of\nnoise. For more details, see the _background_noise_/README.md. If you use the Speech Commands dataset in your work, please cite it as: APA-style citation: \"Warden P. Speech Commands: A public dataset for single-word\nspeech recognition, 2017. Available from\nhttp://download.tensorflow.org/data/speech_commands_v0.01.tar.gz\". BibTeX @article{speechcommands, title={Speech Commands: A public dataset for single-word speech recognition.}, author={Warden, Pete}, journal={Dataset available from http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz}, year={2017} } Massive thanks are due to everyone who donated recordings to this data set, I'm\nvery grateful. I also couldn't have put this together without the help and\nsupport of Billy Rutledge, Rajat Monga, Raziel Alvarez, Brad Krueger, Barbara\nPetit, Gursheesh Kour, and all the AIY and TensorFlow teams. Pete Warden, petewarden@google.com"
    },
    {
        "name": "TrackML Particle Tracking Challenge",
        "url": "https://www.kaggle.com/competitions/trackml-particle-identification",
        "overview_text": "Overview text not found",
        "description_text": " To explore what our universe is made of, scientists at CERN are colliding protons, essentially recreating mini big bangs, and meticulously observing these collisions with intricate silicon detectors. While orchestrating the collisions and observations is already a massive scientific accomplishment, analyzing the enormous amounts of data produced from the experiments is becoming an overwhelming challenge. Event rates have already reached hundreds of millions of collisions per second, meaning physicists must sift through tens of petabytes of data per year. And, as the resolution of detectors improve, ever better software is needed for real-time pre-processing and filtering of the most promising events, producing even more data. To help address this problem, a team of Machine Learning experts and physics scientists working at CERN (the world largest high energy physics laboratory), has partnered with Kaggle and prestigious sponsors to answer the question: can machine learning assist high energy physics in discovering and characterizing new particles? Specifically, in this competition, you\u2019re challenged to build an algorithm that quickly reconstructs particle tracks from 3D points left in the silicon detectors. This challenge consists of two phases:",
        "dataset_text": "A python library is available to simplify the data handling. The following files are available for the participants: A dataset comprises multiple independent events, where each event contains\nsimulated measurements (essentially 3D points) of particles generated in a collision between proton\nbunches at the Large Hadron Collider at CERN. The goal of the\ntracking machine learning challenge is to group the recorded measurements or\nhits for each event into tracks, sets of hits that belong to the same initial\nparticle. A solution must uniquely associate each hit to one track.\nThe training dataset contains the recorded\nhits, their ground truth counterpart and their association to particles, and the\ninitial parameters of those particles. The test dataset contains only the\nrecorded hits. Once unzipped, the dataset is provided as a set of plain .csv files. Each\nevent has four associated files that contain hits, hit cells, particles, and the\nground truth association between them. The common prefix, e.g. event000000010,\nis always event followed by 9 digits. Submissions must be provided as a single .csv file for the whole dataset with\na name starting with submission, e.g. The hits file contains the following values for each hit/entry:  The volume/layer/module id could in principle be deduced from x, y, z. They\nare given here to simplify detector-specific data handling. The truth file contains the mapping between hits and generating particles and\nthe true particle state at each measured hit. Each entry maps one hit to one\nparticle. The particles files contains the following values for each particle/entry: All entries contain the generated information or ground truth. The cells file contains the constituent active detector cells that comprise each\nhit. The cells can be used to refine the hit to track association.\nA cell is the smallest granularity inside each detector module, much like a\npixel on a screen, except that depending on the volume_id a cell can be a square\nor a long rectangle. It is identified by two channel identifiers that are unique\nwithin each detector module and encode the position, much like column/row\nnumbers of a matrix. A cell can provide signal information that the detector\nmodule has recorded in addition to the position. Depending on the detector type\nonly one of the channel identifiers is valid, e.g. for the strip detectors, and\nthe value might have different resolution. The submission file must associate each hit in each event to one and only one\nreconstructed particle track. The reconstructed tracks must be uniquely\nidentified only within each event. Participants are advised to compress the\nsubmission file (with zip, bzip2, gzip) before submission to the\nKaggle site. The detector is built from silicon slabs (or modules, rectangular or\ntrapezo\u00efdal), arranged in cylinders and disks, which measure the position (or\nhits) of the particles that cross them. The detector modules are organized\ninto detector groups or volumes identified by a volume id. Inside a volume they\nare further grouped into layers identified by a layer id. Each layer can contain\nan arbitrary number of detector modules, the smallest geometrically distinct\ndetector object, each identified by a module_id. Within each group, detector\nmodules are of the same type have e.g. the same granularity. All simulated\ndetector modules are so-called semiconductor sensors that are build from thin\nsilicon sensor chips. Each module can be represented by a two-dimensional,\nplanar, bounded sensitive surface. These sensitive surfaces are subdivided into\nregular grids that define the detectors cells, the smallest granularity within\nthe detector.  Each module has a different position and orientation described in the detectors\nfile. A local, right-handed coordinate system is defined on each sensitive\nsurface such that the first two coordinates u and v are on the sensitive surface\nand the third coordinate w is normal to the surface. The orientation and\nposition are defined by the following transformation that transform a position described in local coordinates u,v,w into the\nequivalent position x,y,z in global coordinates using a rotation matrix and\nan translation vector (cx,cy,cz). There are two different module shapes in the detector, rectangular and trapezoidal.\nThe pixel detector ( with volume_id = 7,8,9) is fully built from rectangular modules,\nand so are the cylindrical barrels in volume_id=13,17.\nThe remaining layers are made out disks that need trapezoidal shapes to cover the full disk.  [mit_license]: http://www.opensource.org/licenses/MIT"
    },
    {
        "name": "Santa Gift Matching Challenge",
        "url": "https://www.kaggle.com/competitions/santa-gift-matching",
        "overview_text": "Overview text not found",
        "description_text": " \u2018Tis the night before Christmas\nyear: two thousand seventeen. Santa\u2019s grown grouchy,\nborderline mean. What used to be simple for Old St. Nick,\nis now too puzzling, it\u2019s making him sick! See, Santa always knew, deep down in his gut,\nwhat toy each kid wanted\u2013no ifs, ands, or buts. But fierce population growth, more twins, and toy innovation,\nhas left too complex a problem, in dire need of optimization. \u201cDon\u2019t worry, Mr. Santa\u201d, said an Elf named McMaggle,\n\u201cI have a solution! Have you heard of Kaggle?\u201d As she explained Kaggle in-depth, Santa\u2019s doubt began turning,\nhe became a believer in the magic of...machine learning. So, Santa\u2019s team needs YOU more than ever this year,\nto solve this painful problem and save Christmas cheer. The Challenge In this playground competition, you\u2019re challenged to build a toy matching algorithm that maximizes happiness by pairing kids with toys they want. In the dataset, each kid has 10 preferences for their gift (from 1000) and Santa has 1000 preferred kids for every gift available. What makes this extra difficult is that 0.4% of the kids are twins, and by their parents\u2019 request, require the same gift.",
        "dataset_text": "In this competition, you are given a list of 1,000,000 children and their wish lists of 100 gifts. You are also given a list of 1000 gifts, and their list of 1000 good kids that they prefer to give to. Your goal is to match the list of 1,000,000 children with a gift for each child, and try to make everyone happy. Both the kids and Santa need to be happy - for kids, the higher the gift is on their wish list, the happier, for Santa and his gifts, the higher the child is on the good kids list, the happier Santa is. A few details to notice:"
    },
    {
        "name": "IEEE's Signal Processing Society - Camera Model Identification",
        "url": "https://www.kaggle.com/competitions/sp-society-camera-model-identification",
        "overview_text": "Overview text not found",
        "description_text": "Finding footage of a crime caught on tape is an investigator's dream. But even with crystal clear, damning evidence, one critical question always remains\u2013is the footage real? Today, one way to help authenticate footage is to identify the camera that the image was taken with. Forgeries often require splicing together content from two different cameras. But, unfortunately, the most common way to do this now is using image metadata, which can be easily falsified itself. This problem is actively studied by several researchers around the world. Many machine learning solutions have been proposed in the past: least-squares estimates of a camera's color demosaicing filters as classification features, co-occurrences of pixel value prediction errors as features that are passed to sophisticated ensemble classifiers, and using CNNs to learn camera model identification features. However, this is a problem yet to be sufficiently solved. For this competition, the IEEE Signal Processing Society is challenging you to build an algorithm that identifies which camera model captured an image by using traces intrinsically left in the image. Helping to solve this problem would have a big impact on the verification of evidence used in criminal and civil trials and even news reporting.",
        "dataset_text": "Images in the training set were captured with 10 different camera models, a single device per model, with 275 full images from each device. The list of camera models is as follows: Images in the test set were captured with the same 10 camera models, but using a second device. For example, if the images in the train data for the iPhone 6 were taken with Ben Hamner's device (Camera 1), the images in the test data were taken with Ben Hamner's second device (Camera 2), since he lost the first device in the Bay while kite-surfing. None of the images in the test data were taken with the same device as in the train data. While the train data includes full images, the test data contains only single 512 x 512 pixel blocks cropped from the center of a single image taken with the device. No two image blocks come from the same original image. Half of the images in the test set have been altered. The image names indicate whether or not they were manipulated (_manip) from the original or unaltered (_unalt). While you are not explicitly told how each individual image was altered, the set of possible processing operations that were performed are as follows:"
    },
    {
        "name": "TalkingData AdTracking Fraud Detection Challenge",
        "url": "https://www.kaggle.com/competitions/talkingdata-adtracking-fraud-detection",
        "overview_text": "Overview text not found",
        "description_text": "Fraud risk is everywhere, but for companies that advertise online, click fraud can happen at an overwhelming volume, resulting in misleading click data and wasted money. Ad channels can drive up costs by simply clicking on the ad at a large scale. With over 1 billion smart mobile devices in active use every month, China is the largest\nmobile market in the world and therefore suffers from huge volumes of fradulent traffic. TalkingData, China\u2019s largest independent big data service platform, covers over 70% of active mobile devices nationwide. They handle 3 billion clicks per day, of which 90% are potentially fraudulent. Their current approach to prevent click fraud for app developers is to measure the journey of a user\u2019s click across their portfolio, and flag IP addresses who produce lots of clicks, but never end up installing apps. With this information, they've built an IP blacklist and device blacklist. While successful, they want to always be one step ahead of fraudsters and have turned to the Kaggle community for help in further developing their solution. In their 2nd competition with Kaggle, you\u2019re challenged to build an algorithm that predicts whether a user will download an app after clicking a mobile app ad. To support your modeling, they have provided a generous dataset covering approximately 200 million clicks over 4 days!",
        "dataset_text": "For this competition, your objective is to predict whether a user will download an app after clicking a mobile app advertisement. Each row of the training data contains a click record, with the following features. Note that ip, app, device, os, and channel are encoded. The test data is similar, with the following differences:"
    },
    {
        "name": "Avito Demand Prediction Challenge",
        "url": "https://www.kaggle.com/competitions/avito-demand-prediction",
        "overview_text": "Overview text not found",
        "description_text": "When selling used goods online, a combination of tiny, nuanced details in a product description can make a big difference in drumming up interest. Details like: And, even with an optimized product listing, demand for a product may simply not exist\u2013frustrating sellers who may have over-invested in marketing. Avito, Russia\u2019s largest classified advertisements website, is deeply familiar with this problem. Sellers on their platform sometimes feel frustrated with both too little demand (indicating something is wrong with the product or the product listing) or too much demand (indicating a hot item with a good description was underpriced). In their fourth Kaggle competition, Avito is challenging you to predict demand for an online advertisement based on its full description (title, description, images, etc.), its context (geographically where it was posted, similar ads already posted) and historical demand for similar ads in similar contexts. With this information, Avito can inform sellers on how to best optimize their listing and provide some indication of how much interest they should realistically expect to receive.",
        "dataset_text": "To make it easier to download the training images, we have added several smaller zip archives that hold the same images as train_jpg.zip. If you have already downloaded train_jpg.zip you can ignore these completely."
    },
    {
        "name": "Google Landmark Recognition 2020",
        "url": "https://www.kaggle.com/competitions/landmark-recognition-2020",
        "overview_text": "Overview text not found",
        "description_text": "Welcome to the third Landmark Recognition competition! This year, we have worked to set this up as a code competition and collected a new set of test images. Have you ever gone through your vacation photos and asked yourself: What was the name of that temple I visited in China? or Who created this monument I saw in France? Landmark recognition can help! This technology can predict landmark labels directly from image pixels, to help people better understand and organize their photo collections. This competition challenges Kagglers to build models that recognize the correct landmark (if any) in a dataset of challenging test images. Many Kagglers are familiar with image classification challenges like the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), which aims to recognize 1K general object categories. Landmark recognition is a little different from that: it contains a much larger number of classes (there are more than 81K classes in this challenge), and the number of training examples per class may not be very large. Landmark recognition is challenging in its own way. In the previous editions of this challenge (2018 and 2019), submissions were handled by uploading prediction files to the system. This year's competition is structured in a synchronous rerun format, where participants need to submit their Kaggle notebooks for scoring. This challenge is organized in conjunction with the Landmark Retrieval Challenge 2020, which was launched June 30, 2020. Both challenges are affiliated with the Instance-Level Recognition workshop in ECCV\u201920.",
        "dataset_text": "In this competition, you are asked to take test images and recognize which landmarks (if any) are depicted in them. The training set is available in the train/ folder, with corresponding landmark labels in train.csv. The test set images are listed in the test/ folder. Each image has a unique id. Since there are a large number of images, each image is placed within three subfolders according to the first three characters of the image id (i.e. image abcdef.jpg is placed in a/b/c/abcdef.jpg). This is a synchronous rerun code competition. The provided test set is a representative set of files to demonstrate the format of the private test set. When you submit your notebook, Kaggle will rerun your code on the private dataset. Additionally, this competition also has two unique characteristics: The training data for this competition comes from a cleaned version of the Google Landmarks Dataset v2 (GLDv2), which is available here. Please refer to the paper for more details on the dataset construction and how to use it. See this code example for an example of a pretrained model. If you make use of this dataset in your research, please consider citing:"
    },
    {
        "name": "Google Landmark Retrieval 2020",
        "url": "https://www.kaggle.com/competitions/landmark-retrieval-2020",
        "overview_text": "Overview text not found",
        "description_text": "Welcome to the third Landmark Retrieval competition! This year, we have worked to set this up as a code competition and we have completely refreshed the test and index image sets. Image retrieval is a fundamental problem in computer vision: given a query image, can you find similar images in a large database? This is especially important for query images containing landmarks, which accounts for a large portion of what people like to photograph. In this competition, the developed models are expected to retrieve relevant database images to a given query image (ie, the model should retrieve database images containing the same landmark as the query). This challenge is organized in conjunction with the Landmark Recognition Challenge 2020. Both challenges will be discussed at the Instance-Level Recognition workshop in ECCV\u201920. In the previous editions of this challenge (2018 and 2019), submissions were handled by uploading prediction files to the system. This year's competition is structured in a representation learning format: rather than creating a submission file with retrieved images, you will create a model that extracts a feature embedding for the images and submit the model via Kaggle Notebooks. Kaggle will run your model on a held-out test set, perform a k-nearest-neighbors lookup, and score the resulting embedding quality with mean average precision.",
        "dataset_text": "In this competition, you are asked to develop models that can efficiently retrieve landmark images from a large database. The training set is available in the train/ folder, with corresponding landmark labels in train.csv. The query images are listed in the test/ folder, while the \"index\" images from which you are retrieving are listed in index/. Each image has a unique id. Since there are a large number of images, each image is placed within three subfolders according to the first three characters of the image id (i.e. image abcdef.jpg is placed in a/b/c/abcdef.jpg). Unlike a traditional Kaggle code competition where notebooks are rerun top-to-bottom on a private test set, in this competition you will be submitting a model file that Kaggle will use to: The provided index/ and test/ images in the publicly available dataset are provided to mock the size and structure of the private data, but are otherwise not directly used. Your model must be named submission.zip and be compatible with TensorFlow 2.2. The submission.zip should contain all files and directories created by the tf.saved_model_save function using Tensorflow's SavedModel format. The code used to load your SavedModel, create the embeddings, and score your submission is provided here. The training data for this competition comes from a cleaned version of the Google Landmarks Dataset v2 (GLDv2), which is available here. Please refer to the paper for more details on the dataset construction and how to use it. See this code example for an example of a pretrained model. If you make use of this dataset in your research, please consider citing:"
    },
    {
        "name": "Cornell Birdcall Identification",
        "url": "https://www.kaggle.com/competitions/birdsong-recognition",
        "overview_text": "Overview text not found",
        "description_text": "Do you hear the birds chirping outside your window? Over 10,000 bird species occur in the world, and they can be found in nearly every environment, from untouched rainforests to suburbs and even cities. Birds play an essential role in nature. They are high up in the food chain and integrate changes occurring at lower levels. As such, birds are excellent indicators of deteriorating habitat quality and environmental pollution. However, it is often easier to hear birds than see them. With proper sound detection and classification, researchers could automatically intuit factors about an area\u2019s quality of life based on a changing bird population.  There are already many projects underway to extensively monitor birds by continuously recording natural soundscapes over long periods. However, as many living and nonliving things make noise, the analysis of these datasets is often done manually by domain experts. These analyses are painstakingly slow, and results are often incomplete. Data science may be able to assist, so researchers have turned to large crowdsourced databases of focal recordings of birds to train AI models. Unfortunately, there is a domain mismatch between the training data (short recording of individual birds) and the soundscape recordings (long recordings with often multiple species calling at the same time) used in monitoring applications. This is one of the reasons why the performance of the currently used AI models has been subpar. To unlock the full potential of these extensive and information-rich sound archives, researchers need good machine listeners to reliably extract as much information as possible to aid data-driven conservation. The Cornell Lab of Ornithology\u2019s Center for Conservation Bioacoustics (CCB)\u2019s mission is to collect and interpret sounds in nature. The CCB develops innovative conservation technologies to inspire and inform the conservation of wildlife and habitats globally. By partnering with the data science community, the CCB hopes to further its mission and improve the accuracy of soundscape analyses. In this competition, you will identify a wide variety of bird vocalizations in soundscape recordings. Due to the complexity of the recordings, they contain weak labels. There might be anthropogenic sounds (e.g., airplane overflights) or other bird and non-bird (e.g., chipmunk) calls in the background, with a particular labeled bird species in the foreground. Bring your new ideas to build effective detectors and classifiers for analyzing complex soundscape recordings! If successful, your work will help researchers better understand changes in habitat quality, levels of pollution, and the effectiveness of restoration efforts. Reliable machine listeners would also allow conservationists to deploy more recording units worldwide and would enable data-driven conservation at a scale not yet possible. The eventual conservation outcomes could greatly improve the quality of life for many living organisms\u2014birds and human beings included.",
        "dataset_text": "Your challenge in this competition is to identify which birds are calling in long recordings, given training data generated in meaningfully different contexts. This is the exact problem facing scientists trying to automate the remote monitoring of bird populations. train_audio\nThe train data consists of short recordings of individual bird calls generously uploaded by users of xenocanto.org. test_audio\nThe hidden test_audio directory contains approximately 150 recordings in mp3 format, each roughly 10 minutes long. They will not all fit in a notebook's memory at the same time. The recordings were taken at three separate remote locations in North America. Sites 1 and 2 were labeled in 5 second increments and need matching predictions, but due to the time consuming nature of the labeling process the site 3 files are only labeled at the file level. Accordingly, site 3 has relatively few rows in the test set and needs lower time resolution predictions. Two example soundscapes from another data source are also provided to illustrate how the soundscapes are labeled and the hidden dataset folder structure. The two example audio files are BLKFR-10-CPL_20190611_093000.pt540.mp3 and ORANGE-7-CAP_20190606_093000.pt623.mp3. These soundscapes were kindly provided by Jack Dumbacher of the California Academy of Science's Department of Ornithology and Mammology. test.csv\nOnly the first three rows are available for download; the full test.csv is in the hidden test set. example_test_audio_metadata.csv\nComplete metadata for the example test audio. These labels have higher time precision than is used for the hidden test set. example_test_audio_summary.csv\nMetadata for the example test audio, converted to the same format as used in the hidden test set. train.csv\nA wide range of metadata is provided for the training data. The most directly relevant fields are:"
    },
    {
        "name": "OpenVaccine: COVID-19 mRNA Vaccine Degradation Prediction",
        "url": "https://www.kaggle.com/competitions/stanford-covid-vaccine",
        "overview_text": "Overview text not found",
        "description_text": "Winning the fight against the COVID-19 pandemic will require an effective vaccine that can be equitably and widely distributed. Building upon decades of research has allowed scientists to accelerate the search for a vaccine against COVID-19, but every day that goes by without a vaccine has enormous costs for the world nonetheless. We need new, fresh ideas from all corners of the world. Could online gaming and crowdsourcing help solve a worldwide pandemic? Pairing scientific and crowdsourced intelligence could help computational biochemists make measurable progress. mRNA vaccines have taken the lead as the fastest vaccine candidates for COVID-19, but currently, they face key potential limitations. One of the biggest challenges right now is how to design super stable messenger RNA molecules (mRNA). Conventional vaccines (like your seasonal flu shots) are packaged in disposable syringes and shipped under refrigeration around the world, but that is not currently possible for mRNA vaccines. Researchers have observed that RNA molecules have the tendency to spontaneously degrade. This is a serious limitation--a single cut can render the mRNA vaccine useless. Currently, little is known on the details of where in the backbone of a given RNA is most prone to being affected. Without this knowledge, current mRNA vaccines against COVID-19 must be prepared and shipped under intense refrigeration, and are unlikely to reach more than a tiny fraction of human beings on the planet unless they can be stabilized.  The Eterna community, led by Professor Rhiju Das, a computational biochemist at Stanford\u2019s School of Medicine, brings together scientists and gamers to solve puzzles and invent medicine. Eterna is an online video game platform that challenges players to solve scientific problems such as mRNA design through puzzles. The solutions are synthesized and experimentally tested at Stanford by researchers to gain new insights about RNA molecules. The Eterna community has previously unlocked new scientific principles, made new diagnostics against deadly diseases, and engaged the world\u2019s most potent intellectual resources for the betterment of the public. The Eterna community has advanced biotechnology through its contribution in over 20 publications, including advances in RNA biotechnology. In this competition, we are looking to leverage the data science expertise of the Kaggle community to develop models and design rules for RNA degradation. Your model will predict likely degradation rates at each base of an RNA molecule, trained on a subset of an Eterna dataset comprising over 3000 RNA molecules (which span a panoply of sequences and structures) and their degradation rates at each position. We will then score your models on a second generation of RNA sequences that have just been devised by Eterna players for COVID-19 mRNA vaccines. These final test sequences are currently being synthesized and experimentally characterized at Stanford University in parallel to your modeling efforts -- Nature will score your models! Improving the stability of mRNA vaccines was a problem that was being explored before the pandemic but was expected to take many years to solve. Now, we must solve this deep scientific challenge in months, if not weeks, to accelerate mRNA vaccine research and deliver a refrigerator-stable vaccine against SARS-CoV-2, the virus behind COVID-19. The problem we are trying to solve has eluded academic labs, industry R&D groups, and supercomputers, and so we are turning to you. To help, you can join the team of video game players, scientists, and developers at Eterna to unlock the key in our fight against this devastating pandemic.                   ",
        "dataset_text": "In this competition, you will be predicting the degradation rates at various locations along RNA sequence. There are multiple ground truth values provided in the training data. While the submission format requires all 5 to be predicted, only the following are scored: reactivity, deg_Mg_pH10, and deg_Mg_50C. At the beginning of the competition, Stanford scientists have data on 3029 RNA sequences of length 107. For technical reasons, measurements cannot be carried out on the final bases of these RNA sequences, so we have experimental data (ground truth) in 5 conditions for the first 68 bases. We have split out 629 of these 3029 sequences for a public test set to allow for continuous evaluation through the competition, on the public leaderboard. These sequences, in test.json, have been additionally filtered based on three criteria detailed below to ensure that this subset is not dominated by any large cluster of RNA molecules with poor data, which might bias the public leaderboard. The remaining 2400 sequences for which we have data are in train.json. For our final and most important scoring (the Private Leaderbooard), Stanford scientists are carrying out measurements on 3005 new RNAs, which have somewhat longer lengths of 130 bases. For these data, we expect to have measurements for the first 91 bases, again missing the ends of the RNA. These sequences constitute another 3005 of the 3634 sequences in test.json. For those interested in how the 629 107-base sequences in test.json were filtered, here were the steps to ensure a diverse and high quality test set for public leaderboard scoring: Note that these filters have not been applied to the 2400 RNAs in the public training data train.json \u2014 some of those measurements have negative values or poor signal-to-noise, or some RNA sequences have near-identical sequences in that set. But we are providing all those data in case competitors can squeeze out more signal. The three filters noted above will also not be applied to Private Test on 3005 sequences.\n\n[Update as of 18 Sep 2020] After discussion, the three filters noted above will be applied to Private Test on 3005 sequences, and predictions on sequences that do not pass the filters will not be included in scoring."
    },
    {
        "name": "American Epilepsy Society Seizure Prediction Challenge",
        "url": "https://www.kaggle.com/competitions/seizure-prediction",
        "overview_text": "Overview text not found",
        "description_text": "Seizure forecasting systems hold promise for improving the quality of life for patients with epilepsy. Epilepsy afflicts nearly 1% of the world's population, and is characterized by the occurrence of spontaneous seizures. For many patients, anticonvulsant medications can be given at sufficiently high doses to prevent seizures, but patients frequently suffer side effects. For 20-40% of patients with epilepsy, medications are not effective -- and even after surgical removal of epilepsy-causing brain tissue, many patients continue to experience spontaneous seizures. Despite the fact that seizures occur infrequently, patients with epilepsy experience persistent anxiety due to the possibility of a seizure occurring. Seizure forecasting systems have the potential to help patients with epilepsy lead more normal lives. In order for EEG-based seizure forecasting systems to work effectively, computational algorithms must reliably identify periods of increased probability of seizure occurrence. If these seizure-permissive brain states can be identified, devices designed to warn patients of impeding seizures would be possible. Patients could avoid potentially dangerous activities like driving or swimming, and medications could be administered only when needed to prevent impending seizures, reducing overall side effects. There is emerging evidence that the temporal dynamics of brain activity can be classified into 4 states: Interictal (between seizures, or baseline), Preictal (prior to seizure), Ictal (seizure), and Post-ictal (after seizures). Seizure forecasting requires the ability to reliably identify a preictal state that can be differentiated from the interictal, ictal, and postictal state. The primary challenge in seizure forecasting is differentiating between the preictal and interictal states. The goal of the competition is to demonstrate the existence and accurate classification of the preictal brain state in dogs and humans with naturally occurring epilepsy. Intracranial EEG was recorded from dogs with naturally occurring epilepsy using an ambulatory monitoring system. EEG was sampled from 16 electrodes at 400 Hz, and recorded voltages were referenced to the group average. These are long duration recordings, spanning multiple months up to a year and recording up to a hundred seizures in some dogs.  In addition, datasets from patients with epilepsy undergoing intracranial EEG monitoring to identify a region of brain that can be resected to prevent future seizures are included in the contest. These datasets have varying numbers of electrodes and are sampled at 5000 Hz, with recorded voltages referenced to an electrode outside the brain. The challenge is to distinguish between ten minute long data clips covering an hour prior to a seizure, and ten minute iEEG clips of interictal activity. Seizures are known to cluster, or occur in groups. Patients who typically have seizure clusters receive little benefit from forecasting follow-on seizures. For this contest only lead seizures, defined here as seizures occurring four hours or more after another seizure, are included in the training and testing data sets. In order to avoid any potential contamination between interictal, preictal, and post-ictal EEG signals interictal segments in the canine training and test data were restricted to be at least one week before or after any seizure. In the human data, where the entire monitoring session may last less than one week, interictal data segments were restricted to be at least four hours before or after any seizure. Interictal data segments were chosen at random within these restrictions for both canine and human subjects. Participants are invited to visit the NIH-sponsored International Epilepsy Electrophysiology portal (http://ieeg.org) to review and download annotated interictal and preictal data from other patients and animal subjects. Using ieeg.org data for additional algorithm training is permitted. This competition is sponsored by the National Institutes of Health (NINDS), the Epilepsy Foundation, and the American Epilepsy Society.   ",
        "dataset_text": "Intracranial EEG (iEEG) data clips are organized in folders containing training and testing data for each human or canine subject. The training data is organized into ten minute EEG clips labeled \"Preictal\" for pre-seizure data segments, or \"Interictal\" for non-seizure data segments. Training data segments are numbered sequentially, while testing data are in random order. Within folders data segments are stored in .mat files as follow: Each .mat file contains a data structure with fields as follow: Preictal training and testing data segments are provided covering one hour prior to seizure with a five minute seizure horizon. (i.e. from 1:05 to 0:05 before seizure onset.) This pre-seizure horizon ensures that 1) seizures could be predicted with enough warning to allow administration of fast-acting medications, and 2) any seizure activity before the annotated onset that may have been missed by the epileptologist will not affect the outcome of the competition.  Similarly, one hour sequences of interictal ten minute data segments are provided. The interictal data were chosen randomly from the full data record, with the restriction that interictal segments be as far from any seizure as can be practically achieved, to avoid contamination with preictal or postictal signals. In the long duration canine recordings it was possible to maintain a restriction of one week before or after a seizure. However, in the human recordings (which may be less than a week in total duration) interictal data was restricted to be more than four hours before or after any seizure. Additional annotated intracranial EEG data is freely available at the International Epilepsy Electrophysiology Portal, jointly developed by the University of Pennsylvania and the Mayo Clinic."
    },
    {
        "name": "Web Traffic Time Series Forecasting",
        "url": "https://www.kaggle.com/competitions/web-traffic-time-series-forecasting",
        "overview_text": "Overview text not found",
        "description_text": "This competition focuses on the problem of forecasting the future values of multiple time series, as it has always been one of the most challenging problems in the field. More specifically, we aim the competition at testing state-of-the-art methods designed by the participants, on the problem of forecasting future web traffic for approximately 145,000 Wikipedia articles. Sequential or temporal observations emerge in many key real-world problems, ranging from biological data, financial markets, weather forecasting, to audio and video processing. The field of time series encapsulates many different problems, ranging from analysis and inference to classification and forecast. What can you do to help predict future views? This competition will run as two stages and involves prediction of actual future events. There will be a training stage during which the leaderboard is based on historical data, followed by a stage where participants are scored on real future events. You have complete freedom in how to produce your forecasts: e.g. use of univariate vs multi-variate models, use of metadata (article identifier), hierarchical time series modeling (for different types of traffic), data augmentation (e.g. using Google Trends data to extend the dataset), anomaly and outlier detection and cleaning, different strategies for missing value imputation, and many more types of approaches. We thank Google Inc. and Voleon for sponsorship of this competition, and Oren Anava and Vitaly Kuznetsov for organizing it. Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.",
        "dataset_text": "The training dataset consists of approximately 145k time series. Each of these time series represent a number of daily views of a different Wikipedia article, starting from July, 1st, 2015 up until December 31st, 2016. The leaderboard during the training stage is based on traffic from January, 1st, 2017 up until March 1st, 2017. The second stage will use training data up until September 1st, 2017. The final ranking of the competition will be based on predictions of daily views between September 13th, 2017 and November 13th, 2017 for each article in the dataset. You will submit your forecasts for these dates by September 12th. For each time series, you are provided the name of the article as well as the type of traffic that this time series represent (all, mobile, desktop, spider). You may use this metadata and any other publicly available data to make predictions. Unfortunately, the data source for this dataset does not distinguish between traffic values of zero and missing values. A missing value may mean the traffic was zero or that the data is not available for that day. To reduce the submission file size, each page and date combination has been given a shorter Id. The mapping between page names and the submission Id is given in the key files.  Files used for the first stage will end in '_1'. Files used for the second stage will end in '_2'. Both will have identical formats. The complete training data for the second stage will be made available prior to the second stage."
    },
    {
        "name": "Text Normalization Challenge - English Language",
        "url": "https://www.kaggle.com/competitions/text-normalization-challenge-english-language",
        "overview_text": "Overview text not found",
        "description_text": "As many of us can attest, learning another language is tough. Picking up on nuances like slang, dates and times, and local expressions, can often be a distinguishing factor between proficiency and fluency. This challenge is even more difficult for a machine. Many speech and language applications, including text-to-speech synthesis (TTS) and automatic speech recognition (ASR), require text to be converted from written expressions into appropriate \"spoken\" forms. This is a process known as text normalization, and helps convert 12:47 to \"twelve forty-seven\" and $3.16 into \"three dollars, sixteen cents.\"  However, one of the biggest challenges when developing a TTS or ASR system for a new language is to develop and test the grammar for all these rules, a task that requires quite a bit of linguistic sophistication and native speaker intuition. In this competition, you are challenged to automate the process of developing text normalization grammars via machine learning. This track will focus on English, while a separate will focus on Russian here: Russian Text Normalization Challenge ",
        "dataset_text": "You are provided with a large corpus of text. Each sentence has a sentence_id. Each token within a sentence has a token_id. The before column contains the raw text, the after column contains the normalized text. The aim of the competition is to predict the after column for the test set. The training set contains an additional column, class, which is provided to show the token type. This column is intentionally omitted from the test set. In addition, there is an id column used in the submission format. This is formed by concatenating the sentence_id and token_id with an underscore (e.g. 123_5)."
    },
    {
        "name": "Text Normalization Challenge - Russian Language",
        "url": "https://www.kaggle.com/competitions/text-normalization-challenge-russian-language",
        "overview_text": "Overview text not found",
        "description_text": "As many of us can attest, learning another language is tough. Picking up on nuances like slang, dates and times, and local expressions, can often be a distinguishing factor between proficiency and fluency. This challenge is even more difficult for a machine. Many speech and language applications, including text-to-speech synthesis (TTS) and automatic speech recognition (ASR), require text to be converted from written expressions into appropriate \"spoken\" forms. This is a process known as text normalization, and helps convert 12:47 to \"twelve forty-seven\" and $3.16 into \"three dollars, sixteen cents.\"  However, one of the biggest challenges when developing a TTS or ASR system for a new language is to develop and test the grammar for all these rules, a task that requires quite a bit of linguistic sophistication and native speaker intuition. In this competition, you are challenged to automate the process of developing text normalization grammars via machine learning. This track will focus on Russian, while a separate will focus on English here: English Text Normalization Challenge ",
        "dataset_text": "You are provided with a large corpus of text. Each sentence has a sentence_id. Each token within a sentence has a token_id. The before column contains the raw text, the after column contains the normalized text. The aim of the competition is to predict the after column for the test set. The training set contains an additional column, class, which is provided to show the token type. This column is intentionally omitted from the test set. In addition, there is an id column used in the submission format. This is formed by concatenating the sentence_id and token_id with an underscore (e.g. 123_5). Note that the Russian dataset contains normalized text that is the result of transliteration. For example, the English name \"Julius\" is rendered into a Russian spelling in order for a Russian TTS system to pronounce it. These are denoted with a '_trans' postfix in the data (e.g. \"Julius\" -> \"\u0434_trans \u0436_trans \u0443_trans \u043b_trans \u0438_trans \u0443_trans \u0441_trans\"). Your submissions should follow the same convention. Due to the way this dataset was constructed, the train and test set contain a small number of duplicate sentences. These duplicates are ignored in scoring."
    },
    {
        "name": "Inclusive Images Challenge",
        "url": "https://www.kaggle.com/competitions/inclusive-images-challenge",
        "overview_text": "Overview text not found",
        "description_text": "Making products that work for people all over the globe is an important value at Google AI. In the field of classification, this means developing models that work well for regions all over the world. Today, the dataset a model is trained on greatly dictates the performance of that model. A system trained on a dataset that doesn\u2019t represent a broad range of localities could perform worse on images drawn from geographic regions underrepresented in the training data. Google and the industry at large are working to create more diverse & representative datasets. But it is also important for the field to make progress in understanding how to build models when the data available may not cover all audiences a model is meant to reach. Google AI is challenging Kagglers to develop models that are robust to blind spots that might exist in a data set, and to create image recognition systems that can perform well on test images drawn from different geographic distributions than the ones they were trained on. By finding ways to teach image classifiers to generalize to new geographic and cultural contexts, we hope the community will make even more progress in inclusive machine learning that benefits everyone, everywhere. Note: This competition is run in two stages. Refer to the FAQ for an explanation of how this works & the Timeline for specific dates. This competition is a part of the NIPS 2018 competition track. Winners will be invited to attend and present their solutions at the workshop. Shankar et al. \"No Classification without Representation: Assessing Geodiversity Issues in Open Data Sets for the Developing World\" NIPS 2017 Workshop on Machine Learning for the Developing World",
        "dataset_text": "All of the stage 1 and 2 image and attributions and solutions files are now available at https://console.cloud.google.com/storage/browser/inclusive-images-public. This competition uses a portion of the Open Images dataset as the training set. Getting the right subset of data from Open Images site is a bit tricky; please see this guide for details.\nYou may also choose to use this Wikipedia text data to augment the training images. For example, there might be opportunities to use this additional information for transfer learning, or for informative joint embeddings. This is an opportunity to be creative. But it\u2019s strictly optional -- there may be very good solutions to this challenge that do not touch this data source at all. The contents of the text data archive are documented here."
    },
    {
        "name": "Microsoft Malware Prediction",
        "url": "https://www.kaggle.com/competitions/microsoft-malware-prediction",
        "overview_text": "Overview text not found",
        "description_text": " The malware industry continues to be a well-organized, well-funded market dedicated to evading traditional security measures. Once a computer is infected by malware, criminals can hurt consumers and enterprises in many ways. With more than one billion enterprise and consumer customers, Microsoft takes this problem very seriously and is deeply invested in improving security. As one part of their overall strategy for doing so, Microsoft is challenging the data science community to develop techniques to predict if a machine will soon be hit with malware. As with their previous, Malware Challenge (2015), Microsoft is providing Kagglers with an unprecedented malware dataset to encourage open-source progress on effective techniques for predicting malware occurrences. Can you help protect more than one billion machines from damage BEFORE it happens? This competition is hosted by Microsoft, Windows Defender ATP Research, Northeastern University College of Computer and Information Science, and Georgia Tech Institute for Information Security & Privacy.",
        "dataset_text": "The goal of this competition is to predict a Windows machine\u2019s probability of getting infected by various families of malware, based on different properties of that machine. The telemetry data containing these properties and the machine infections was generated by combining heartbeat and threat reports collected by Microsoft's endpoint protection solution, Windows Defender. Each row in this dataset corresponds to a machine, uniquely identified by a MachineIdentifier. HasDetections is the ground truth and indicates that Malware was detected on the machine. Using the information and labels in train.csv, you must predict the value for HasDetections for each machine in test.csv. The sampling methodology used to create this dataset was designed to meet certain business constraints, both in regards to user privacy as well as the time period during which the machine was running. Malware detection is inherently a time-series problem, but it is made complicated by the introduction of new machines, machines that come online and offline, machines that receive patches, machines that receive new operating systems, etc. While the dataset provided here has been roughly split by time, the complications and sampling requirements mentioned above may mean you may see imperfect agreement between your cross validation, public, and private scores! Additionally, this dataset is not representative of Microsoft customers\u2019 machines in the wild; it has been sampled to include a much larger proportion of malware machines. Unavailable or self-documenting column names are marked with an \"NA\"."
    },
    {
        "name": "Google Landmark Recognition 2019",
        "url": "https://www.kaggle.com/competitions/landmark-recognition-2019",
        "overview_text": "Overview text not found",
        "description_text": "Did you ever go through your vacation photos and ask yourself: What is the name of this temple I visited in China? Who created this monument I saw in France? Landmark recognition can help! This technology can predict landmark labels directly from image pixels, to help people better understand and organize their photo collections. Today, a great obstacle to landmark recognition research is the lack of large annotated datasets. In this competition, we present the largest worldwide dataset to date, to foster progress in this problem. This competition challenges Kagglers to build models that recognize the correct landmark (if any) in a dataset of challenging test images. Many Kagglers are familiar with image classification challenges like the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), which aims to recognize 1K general object categories. Landmark recognition is a little different from that: it contains a much larger number of classes (there are more than 200K classes in this challenge), and the number of training examples per class may not be very large. Landmark recognition is challenging in its own way. This is the second edition of this challenge. Compared to the first edition, the new dataset is more comprehensive and diverse. See the Data tab for more in-depth discussion on the new released dataset. This challenge is organized in conjunction with the Landmark Retrieval Challenge. In particular, note that the test set for both challenges is the same, to encourage participants to compete in both. We encourage participants to use the training data from the recognition challenge (either from this year\u2019s or last year\u2019s dataset) to develop models which could be useful for the retrieval challenge.",
        "dataset_text": "In this competition, you are asked to take test images and recognize which landmarks (if any) are depicted in them. The test images are listed in test.csv, while train.csv contains a large number of images labeled with their associated landmarks. Test images may depict no landmark, one landmark, or more than one landmark. The training set images are each associated to exactly one landmark: each image has a unique id (a hash) and each landmark has a unique label (an integer). NOTE: For the first stage, the train and test data should be downloaded from different sources. Please see the detailed instructions below. This is a 2-stage competition. There is one test set for the first stage, and a different test set for the second stage. The training data can be obtained via CVDF: see instructions here. The training dataset was constructed by mining web landmark images with permissive licenses, leveraging crowdsourced labels. For this reason, each category may contain quite diverse data: for example, images from a museum may contain outdoor images showing the building and indoor images depicting a statue located in the museum. To avoid bias, no computer vision algorithms were used for ground truth generation. Compared to last year\u2019s challenge, our new dataset is 3X larger, more comprehensive/diverse (10X larger number of classes), and noisy. Last year, the data had been released after an automated data cleaning step; this year, we release the raw data directly. If you make use of this dataset in your research, please consider citing:"
    },
    {
        "name": "Google Landmark Retrieval 2019",
        "url": "https://www.kaggle.com/competitions/landmark-retrieval-2019",
        "overview_text": "Overview text not found",
        "description_text": "Image retrieval is a fundamental problem in computer vision: given a query image, can you find similar images in a large database? This is especially important for query images containing landmarks, which accounts for a large portion of what people like to photograph. In this competition, Kagglers are given query images and, for each query, are expected to retrieve all database images containing the same landmarks (if any). The competition will proceed in two phases: The 1st phase will use the same test and index sets as last year, while for phase 2 we will release a completely new dataset that contains 700K images with more than 100K unique landmarks. We hope that this release will accelerate progress in this important research problem. This challenge is organized in conjunction with the Landmark Recognition Challenge. In particular, note that the test set for both challenges is the same, to encourage participants to compete in both. We also encourage participants to use the training data from the recognition challenge (either from this year\u2019s or last year\u2019s dataset) to develop models which could be useful for the retrieval challenge.",
        "dataset_text": "In this competition, you are asked to take a query image and retrieve a set of images that depict a landmark contained in it. The query images are listed in test.csv, while the \"index\" images from which you are retrieving are listed in index.csv. Each image has a unique id. This is a 2-stage competition. There is one test set for the first stage, and a different test set for the second stage. This is a 2-stage competition. There is one index set for the first stage, and a different index set for the second stage. The index dataset for the second stage was constructed by mining web landmark images with permissive licenses, leveraging crowdsourced labels. For this reason, each category may contain quite diverse data: for example, images from a museum may contain outdoor images showing the building and indoor images depicting a statue located in the museum. To avoid bias, no computer vision algorithms were used for ground truth generation. Last year, the data had been released after an automated data cleaning step; this year, we release the raw data directly. Note: In the ground truth, if a landmark was rated as relevant for a query image according to the crowdsourced labels, we mark all images in the corresponding landmark class as relevant for the query. Therefore, some index images may be marked as relevant for queries even though they do not precisely visually match. Like last year, this should have only a very minor effect on submission scores. If you make use of this dataset in your research, please consider citing:"
    },
    {
        "name": "The 3rd YouTube-8M Video Understanding Challenge",
        "url": "https://www.kaggle.com/competitions/youtube8m-2019",
        "overview_text": "Overview text not found",
        "description_text": "Imagine being able to search for the moment in any video where an adorable kitten sneezes, even though the uploader didn\u2019t title or describe the video with such descriptive metadata. Now, apply that same concept to videos that cover important or special events like a baby\u2019s first steps or a game-winning goal -- and now we have the ability to quickly find and share special video moments. This technology is called temporal concept localization within video and Google Research can use your help to advance the state of the art in this area.  An example of the detected action \"blowing out candles\" In most web searches, video retrieval and ranking is performed by matching query terms to metadata and other video-level signals. However, we know that videos can contain an array of topics that aren\u2019t always characterized by the uploader, and many of these miss localizations to brief but important moments within the video. Temporal localization can enable applications such as improved video search (including search within video), video summarization and highlight extraction, action moment detection, improved video content safety, and many others. In previous years, participants worked on advancements in video-level annotations, building both unconstrained and constrained models. In this third challenge based on the YouTube 8M dataset, Kagglers will localize video-level labels to the precise time in the video where the label actually appears, and do this at an unprecedented scale. To put it another way: at what point in the video does the cat sneeze? If successful, your new machine learning models will significantly improve video understanding for all, by not only identifying the topics relevant to a video, but also pinpointing where in the video they appear. This competition is being hosted by Google Research as a part of the International Conference on Computer Vision (ICCV) 2019 selected workshop session. Please refer to the YouTube 8M Large-Scale Video Understanding Workshop Page for details about the workshop.",
        "dataset_text": "In this competition, you will predict the Class labels of YouTube video segments. We provide you extracted frame-level features. The feature data and detailed feature information can be found on the YouTube-8M dataset webpage.  The training dataset in this competition contains videos and labels that are publicly available on YouTube, while the test data is not publicly available. The test data also has anonymized video IDs to ensure the fairness of the competition."
    },
    {
        "name": "Gendered Pronoun Resolution",
        "url": "https://www.kaggle.com/competitions/gendered-pronoun-resolution",
        "overview_text": "Overview text not found",
        "description_text": "Can you help end gender bias in pronoun resolution? Pronoun resolution is part of coreference resolution, the task of pairing an expression to its referring entity. This is an important task for natural language understanding, and the resolution of ambiguous pronouns is a longstanding challenge. Unfortunately, recent studies have suggested gender bias among state-of-the-art coreference resolvers. Google AI Language aims to improve gender-fairness in modeling by releasing the Gendered Ambiguous Pronouns (GAP) dataset, containing gender-balanced pronouns (50% of its examples containing feminine pronouns, and 50% containing masculine pronouns). In this two-stage competition, Kagglers are challenged to build pronoun resolution systems that perform equally well regardless of pronoun gender. Stage two's final evaluation will use a new dataset following the same format. To encourage gender-fair modeling, the ratio of masculine to feminine examples in the official test data will not be known ahead of time. ---------- Please cite the original paper if you use GAP in your work:",
        "dataset_text": "In this competition, you must identify the target of a pronoun within a text passage. The source text is taken from Wikipedia articles. You are provided with the pronoun and two candidate names to which the pronoun could refer. You must create an algorithm capable of deciding whether the pronoun refers to name A, name B, or neither. Unlike many Kaggle challenges, this competition does not provide an explicit labeled training set. Files are also available on the GAP Dataset Github Repo. Note that the labels for the test set are available on this page. However, your final score and ranking will be determined in stage 2, against a withheld private test set."
    },
    {
        "name": "TReNDS Neuroimaging",
        "url": "https://www.kaggle.com/competitions/trends-assessment-prediction",
        "overview_text": "Overview text not found",
        "description_text": "Human brain research is among the most complex areas of study for scientists. We know that age and other factors can affect its function and structure, but more research is needed into what specifically occurs within the brain. With much of the research using MRI scans, data scientists are well positioned to support future insights. In particular, neuroimaging specialists look for measurable markers of behavior, health, or disorder to help identify relevant brain regions and their contribution to typical or symptomatic effects. In this competition, you will predict multiple assessments plus age from multimodal brain MRI features. You will be working from existing results from other data scientists, doing the important work of validating the utility of multimodal features in a normative population of unaffected subjects. Due to the complexity of the brain and differences between scanners, generalized approaches will be essential to effectively propel multimodal neuroimaging research forward. The Tri-Institutional Georgia State University/Georgia Institute of Technology/Emory University Center for Translational Research in Neuroimaging and Data Science (TReNDS) leverages advanced brain imaging to promote research into brain health. The organization is focused on developing, applying and sharing advanced analytic approaches and neuroinformatics tools. Among its software projects are the GIFT and FIT neuroimaging toolboxes, the COINS data management system, and the COINSTAC toolkit for federated learning, all aimed at supporting data scientists and other neuroimaging researchers. Making the leap from research to clinical application is particularly difficult in brain health. In order to translate to clinical settings, research findings have to be reproduced consistently and validated in out-of-sample instances. The problem is particularly well-suited for data science, but current approaches typically do not generalize well. With this large dataset and competition, your efforts could directly address an important area of brain research. Acknowledgments",
        "dataset_text": "End of Competition Update: At the request of the competition host, the dataset has been withdrawn from this page at the conclusion of the competition. In this challenge, participants will predict age and assessment values from two domains using features derived from brain MRI images as inputs. Models are expected to generalize on data from a different scanner/site (site 2). All subjects from site 2 were assigned to the test set, so their scores are not available. While there are fewer site 2 subjects than site 1 subjects in the test set, the total number of subjects from site 2 will not be revealed until after the end of the competition. To make it more interesting, the IDs of some site 2 subjects have been revealed below. Use this to inform your models about site effects. Site effects are a form of bias. To generalize well, models should learn features that are not related to or driven by site effects. The .mat files for this competition can be read in python using h5py, and the .nii file can be read in python using nilearn. The scores (see train_scores.csv) are not the original age and raw assessment values. They have been transformed and de-identified to help protect subject identity and minimize the risk of unethical usage of the data. Nonetheless, they are directly derived from the original assessment values and, thus, associations with the provided features is equally likely. Before transformation, the age in the training set is rounded to nearest year for privacy reasons. However, age is not rounded to year (higher precision) in the test set. Thus, heavily overfitting to the training set age will very likely have a negative impact on your submissions. An unbiased strategy was utilized to obtain the provided features. This means that a separate, unrelated large imaging dataset was utilized to learn feature templates. Then, these templates were \"projected\" onto the original imaging data of each subject used for this competition using spatially constrained independent component analysis (scICA) via group information guided ICA (GIG-ICA). The first set of features are source-based morphometry (SBM) loadings. These are subject-level weights from a group-level ICA decomposition of gray matter concentration maps from structural MRI (sMRI) scans. The second set are static functional network connectivity (FNC) matrices. These are the subject-level cross-correlation values among 53 component timecourses estimated from GIG-ICA of resting state functional MRI (fMRI). The third set of features are the component spatial maps (SM). These are the subject-level 3D images of 53 spatial networks estimated from GIG-ICA of resting state functional MRI (fMRI)."
    },
    {
        "name": "University of Liverpool - Ion Switching",
        "url": "https://www.kaggle.com/competitions/liverpool-ion-switching",
        "overview_text": "Overview text not found",
        "description_text": "Think you can use your data science skills to make big predictions at a submicroscopic level? Many diseases, including cancer, are believed to have a contributing factor in common. Ion channels are pore-forming proteins present in animals and plants. They encode learning and memory, help fight infections, enable pain signals, and stimulate muscle contraction. If scientists could better study ion channels, which may be possible with the aid of machine learning, it could have a far-reaching impact.  When ion channels open, they pass electric currents. Existing methods of detecting these state changes are slow and laborious. Humans must supervise the analysis, which imparts considerable bias, in addition to being tedious. These difficulties limit the volume of ion channel current analysis that can be used in research. Scientists hope that technology could enable rapid automatic detection of ion channel current events in raw data. The University of Liverpool\u2019s Institute of Ageing and Chronic Disease is working to advance ion channel research. Their team of scientists have asked for your help. In this competition, you\u2019ll use ion channel data to better model automatic identification methods. If successful, you\u2019ll be able to detect individual ion channel events in noisy raw signals. The data is simulated and injected with real world noise to emulate what scientists observe in laboratory experiments. Technology to analyze electrical data in cells has not changed significantly over the past 20 years. If we better understand ion channel activity, the research could impact many areas related to cell health and migration. From human diseases to how climate change affects plants, faster detection of ion channels could greatly accelerate solutions to major world problems. Acknowledgements:\nThis would not be possible without the help of the Biotechnology and Biological Sciences Research Council (BBSRC).",
        "dataset_text": "In this competition, you will be predicting the number of open_channels present, based on electrophysiological signal data. IMPORTANT: While the time series appears continuous, the data is from discrete batches of 50 seconds long 10 kHz samples (500,000 rows per batch). In other words, the data from 0.0001 - 50.0000 is a different batch than 50.0001 - 100.0000, and thus discontinuous between 50.0000 and 50.0001. You can find detailed information about the data from the paper Deep-Channel uses deep neural networks to detect single-molecule events from patch-clamp data."
    },
    {
        "name": "ALASKA2 Image Steganalysis",
        "url": "https://www.kaggle.com/competitions/alaska2-image-steganalysis",
        "overview_text": "Overview text not found",
        "description_text": "That file you downloaded may contain hidden messages that aren\u2019t part of its regular contents. The same technology employed for digital watermarking is also misused by crime rings. Law enforcement must now use steganalysis to detect these messages as part of their investigations. Machine learning is an important tool in the discovery of this secret data. Current methods produce unreliable results, raising false alarms. One reason for inaccuracy is the many different devices and processing combinations. Yet, detection models are trained on a homogeneous dataset. To increase accuracy, researchers must put data hidden within digital images \u201cinto the wild\u201d (hence the name ALASKA) to mimic real world conditions. In the competition, you\u2019ll create an efficient and reliable method to detect secret data hidden within innocuous-seeming digital images. Rather than limiting the data source, these images have been acquired with as many as 50 different cameras (from smartphone to full-format high end) and processed in different fashions. Successful entries will include robust detection algorithms with minimal false positives. The IEEE WIFS (Workshop on Information Forensics and Security) is eager to make this happen again, as a follow up to the ALASKA#1 Challenge. WIFS is an annual event where researchers gather to discuss emerging challenges, exchange fresh ideas, and share state-of-the-art results and technical expertise in the areas of information security and forensics. WIFS has teamed up with Troyes University of Technology, CRIStAL Lab, Lille University, and CNRS to enable more accurate steganalysis. Law enforcement officers need better methods to combat criminals using hidden messages. The data science community and other researchers can help with better automated detection. More accurate methods could help catch criminals whose communications are hidden in plain sight. The challenge is organized by R\u00e9mi COGRANNE (UTT), Patrick BAS (CRIStAL / CNRS) and Quentin Giboulot (UTT) ; in addition to Kaggle, we have been greatly helped by the following sponsors:",
        "dataset_text": "This dataset contains a large number of unaltered images, called the \"Cover\" image, as well as corresponding examples in which information has been hidden using one of three steganography algorithms (JMiPOD, JUNIWARD, UERD).\nThe goal of the competition is to determine which of the images in the test set (Test/) have hidden messages embedded. Note that in order to make the competition more realistic the length of hidden messages (the payload) will not be provided. The only available information on the test set is:"
    },
    {
        "name": "Happywhale - Whale and Dolphin Identification",
        "url": "https://www.kaggle.com/competitions/happy-whale-and-dolphin",
        "overview_text": "Overview text not found",
        "description_text": "We use fingerprints and facial recognition to identify people, but can we use similar approaches with animals? In fact, researchers manually track marine life by the shape and markings on their tails, dorsal fins, heads and other body parts. Identification by natural markings via photographs\u2014known as photo-ID\u2014is a powerful tool for marine mammal science. It allows individual animals to be tracked over time and enables assessments of population status and trends. With your help to automate whale and dolphin photo-ID, researchers can reduce image identification times by over 99%. More efficient identification could enable a scale of study previously unaffordable or impossible.  Currently, most research institutions rely on time-intensive\u2014and sometimes inaccurate\u2014manual matching by the human eye. Thousands of hours go into manual matching, which involves staring at photos to compare one individual to another, finding matches, and identifying new individuals. While researchers enjoy looking at a whale photo or two, manual matching limits the scope and reach. Algorithms developed in this competition will be implemented in Happywhale, a research collaboration and citizen science web platform. Its mission is to increase global understanding and caring for marine environments through high quality conservation science and education. Happywhale aims to make it easy and rewarding for the public to participate in science by building innovative tools to engage anyone interested in marine mammals. The platform also serves the research community with powerful collaborative tools. In this competition, you\u2019ll develop a model to match individual whales and dolphins by unique\u2014but often subtle\u2014characteristics of their natural markings. You'll pay particular attention to dorsal fins and lateral body views in image sets from a multi-species dataset built by 28 research institutions. The best submissions will suggest photo-ID solutions that are fast and accurate. If successful, you'll have a hand in building advanced technology to better understand and manage the impact on the world\u2019s changing oceans. Previous automation attempts resulted in a global database of over 50,000 whales and an agreement with cruise ships to operate at a maximum speed of 11 mph in the most whale-rich region. Your ideas to automate the identification of marine life will help overcome increasing human impacts on oceans, providing a critical tool for conservation science. If there's a whale, there's a way!",
        "dataset_text": "In the previous HappyWhale competition, the task was to predict individual humpback whales from images of their flukes. Whales and dolphins in this dataset can be identified by shapes, features and markings (some natural, some acquired) of dorsal fins, backs, heads and flanks. Some species and some individuals have highly distinct features, others are very much less distinct. Further, individual features may change over time. This competition expands that task significantly: data in this competition contains images of over 15,000 unique individual marine mammals from 30 different species collected from 28 different research organizations. Individuals have been manually identified and given an individual_id by marine researches, and your task is to correctly identify these individuals in images. It's a challenging task that has the potential to drive significant advancements in understanding and protecting marine mammals across the globe. An important note about data quality: Bringing together this dataset from many different research organization posed a number of practical challenges. Significant effort has been made to minimize data quality issues and as well as to minimize leakage as much as possible. There are undoubtably issues. We encourage the community to report these things so that future versions of the data can be improved, but unless there is a significant issue, we don't expect to make updates to the data during the competition."
    },
    {
        "name": "PetFinder.my - Pawpularity Contest",
        "url": "https://www.kaggle.com/competitions/petfinder-pawpularity-score",
        "overview_text": "Overview text not found",
        "description_text": "    A picture is worth a thousand words. But did you know a picture can save a thousand lives? Millions of stray animals suffer on the streets or are euthanized in shelters every day around the world. You might expect pets with attractive photos to generate more interest and be adopted faster. But what makes a good picture? With the help of data science, you may be able to accurately determine a pet photo\u2019s appeal and even suggest improvements to give these rescue animals a higher chance of loving homes. PetFinder.my is Malaysia\u2019s leading animal welfare platform, featuring over 180,000 animals with 54,000 happily adopted. PetFinder collaborates closely with animal lovers, media, corporations, and global organizations to improve animal welfare. Currently, PetFinder.my uses a basic Cuteness Meter to rank pet photos. It analyzes picture composition and other factors compared to the performance of thousands of pet profiles. While this basic tool is helpful, it's still in an experimental stage and the algorithm could be improved. In this competition, you\u2019ll analyze raw images and metadata to predict the \u201cPawpularity\u201d of pet photos. You'll train and test your model on PetFinder.my's thousands of pet profiles. Winning versions will offer accurate recommendations that will improve animal welfare. If successful, your solution will be adapted into AI tools that will guide shelters and rescuers around the world to improve the appeal of their pet profiles, automatically enhancing photo quality and recommending composition improvements. As a result, stray dogs and cats can find their \"furever\" homes much faster. With a little assistance from the Kaggle community, many precious lives could be saved and more happy families created. Top participants may be invited to collaborate on implementing their solutions and creatively improve global animal welfare with their AI skills.   ",
        "dataset_text": "In this competition, your task is to predict engagement with a pet's profile based on the photograph for that profile. You are also provided with hand-labelled metadata for each photo. The dataset for this competition therefore comprises both images and tabular data. In addition to the training data, we include some randomly generated example test data to help you author submission code. When your submitted notebook is scored, this example data will be replaced by the actual test data (including the sample submission). The train.csv and test.csv files contain metadata for photos in the training set and test set, respectively. Each pet photo is labeled with the value of 1 (Yes) or 0 (No) for each of the following features:"
    },
    {
        "name": "UW-Madison GI Tract Image Segmentation",
        "url": "https://www.kaggle.com/competitions/uw-madison-gi-tract-image-segmentation",
        "overview_text": "Overview text not found",
        "description_text": "In 2019, an estimated 5 million people were diagnosed with a cancer of the gastro-intestinal tract worldwide. Of these patients, about half are eligible for radiation therapy, usually delivered over 10-15 minutes a day for 1-6 weeks. Radiation oncologists try to deliver high doses of radiation using X-ray beams pointed to tumors while avoiding the stomach and intestines. With newer technology such as integrated magnetic resonance imaging and linear accelerator systems, also known as MR-Linacs, oncologists are able to visualize the daily position of the tumor and intestines, which can vary day to day. In these scans, radiation oncologists must manually outline the position of the stomach and intestines in order to adjust the direction of the x-ray beams to increase the dose delivery to the tumor and avoid the stomach and intestines. This is a time-consuming and labor intensive process that can prolong treatments from 15 minutes a day to an hour a day, which can be difficult for patients to tolerate\u2014unless deep learning could help automate the segmentation process. A method to segment the stomach and intestines would make treatments much faster and would allow more patients to get more effective treatment. The UW-Madison Carbone Cancer Center is a pioneer in MR-Linac based radiotherapy, and has treated patients with MRI guided radiotherapy based on their daily anatomy since 2015. UW-Madison has generously agreed to support this project which provides anonymized MRIs of patients treated at the UW-Madison Carbone Cancer Center. The University of Wisconsin-Madison is a public land-grant research university in Madison, Wisconsin. The Wisconsin Idea is the university's pledge to the state, the nation, and the world that their endeavors will benefit all citizens. In this competition, you\u2019ll create a model to automatically segment the stomach and intestines on MRI scans. The MRI scans are from actual cancer patients who had 1-5 MRI scans on separate days during their radiation treatment. You'll base your algorithm on a dataset of these scans to come up with creative deep learning solutions that will help cancer patients get better care.  In this figure, the tumor (pink thick line) is close to the stomach (red thick line). High doses of radiation are directed to the tumor while avoiding the stomach. The dose levels are represented by the rainbow of outlines, with higher doses represented by red and lower doses represented by green. Cancer takes enough of a toll. If successful, you'll enable radiation oncologists to safely deliver higher doses of radiation to tumors while avoiding the stomach and intestines. This will make cancer patients' daily treatments faster and allow them to get more effective treatment with less side effects and better long-term cancer control. Sangjune Laurence Lee MSE MD FRCPC DABR\nPoonam Yadav Ph.D., DABR\nYin Li PhD\nJason J. Meudt BS, RTT\nJessica Strang\nDustin Hebel\nAlyx Alfson MS CMD, R.T.(T)\nStephanie J. Olson RTT (BS), CMD (MS)\nTera R. Kruser MS, RTT, CMD\nJennifer B Smilowitz, Ph.D., DABR, FAAPM\nKailee Borchert\nBrianne Loritz\nJohn Bayouth PhD\nMichael Bassetti MD PhD Work funded by the University of Wisconsin Carbone Cancer Center Pancreas Pilot Research Grant. ",
        "dataset_text": "In this competition we are segmenting organs cells in images. The training annotations are provided as RLE-encoded masks, and the images are in 16-bit grayscale PNG format. Each case in this competition is represented by multiple sets of scan slices (each set is identified by the day the scan took place). Some cases are split by time (early days are in train, later days are in test) while some cases are split by case - the entirety of the case is in train or test. The goal of this competition is to be able to generalize to both partially and wholly unseen cases. Note that, in this case, the test set is entirely unseen. It is roughly 50 cases, with a varying number of days and slices, as seen in the training set. The test set in this competition is only available when your code is submitted. The sample_submission.csv provided in the public set is an empty placeholder that shows the required submission format; you should perform your modeling, cross-validation, etc., using the training set, and write code to process a non-empty sample submission. It will contain rows with id, class and predicted columns as described in the Evaluation page. When you submit your notebook, your code will be run against the non-hidden test set, which has the same folder format (<case>/<case_day>/<scans>) as the training data. Note that the image filenames include 4 numbers (ex. 276_276_1.63_1.63.png). These four numbers are slice width / height (integers in pixels) and width/height pixel spacing (floating points in mm). The first two defines the resolution of the slide. The last two record the physical size of each pixel. Physical pixel thickness in superior-inferior direction is 3mm."
    },
    {
        "name": "G2Net Detecting Continuous Gravitational Waves",
        "url": "https://www.kaggle.com/competitions/g2net-detecting-continuous-gravitational-waves",
        "overview_text": "Overview text not found",
        "description_text": "The goal of this competition is to find continuous gravitational-wave signals. You will develop a model sensitive enough to detect weak yet long-lasting signals emitted by rapidly-spinning neutron stars within noisy data. Your work will help scientists detect something new: a second class of gravitational waves! The first gravitational wave discoveries earned a Nobel Prize. Further study of these waves may enable scientists to learn about the structure of the most extreme stars in our universe. When scientists detected the first class of gravitational waves in 2015, they expected the discoveries to continue. There are four classes, yet at present only signals from merging black holes and neutron stars have been detected. Among those remaining are continuous gravitational-wave signals. These are weak yet long-lasting signals emitted by rapidly-spinning neutron stars. Imagine the mass of our Sun but condensed into a ball the size of a city and spinning over 1,000 times a second. The extreme compactness of these stars, composed of the densest material in the universe, could allow continuous waves to be emitted and then detected on Earth. There are potentially many continuous signals from neutron stars in our own galaxy and the current challenge for scientists is to make the first detection, and hopefully data science can help with this mission.  This image, taken from a 2021 paper by the LIGO-Virgo-KAGRA collaboration, shows the maximum amplitude of a continuous wave any of these neutron stars could emit without being found by the search analyses. Circled stars show results constraining the physical properties of specific neutron stars. Traditional approaches to detecting these weak and hard-to-find continuous signals are based on matched-filtering variants. Scientists create a bank of possible signal waveform templates and ask how correlated each waveform is with the measured noisy data. High correlation is consistent with the presence of a signal similar to that waveform. Due to the long duration of these signals, banks could easily contain hundreds of quintillions of templates; yet, with so many possible waveforms, scientists don\u2019t have the computational power to use the approach without making approximations that weaken the sensitivity to the signals. G2Net is a network of Gravitational Wave, Geophysics and Machine Learning. Via an Action from COST (European Cooperation in Science and Technology), a funding agency for research and innovation networks, G2Net aims to create a broad network of scientists. From four different areas of expertise, namely GW physics, Geophysics, Computing Science and Robotics, these scientists have agreed on a common goal of tackling challenges in data analysis and noise characterization for GW detectors. By helping G2Net in this challenge you'll enable scientists to improve their sensitivity, leading to new discoveries in the field. As a result, scientists could learn more about the structure of the most extreme stars in our universe. Resources for the generation of background noise and continuous gravitational-wave signals can be found in this pinned discussion. A brief notebook summarizing the very basics of generating data using PyFstat is also provided. We acknowledge support from the LIGO-Virgo-Kagra Collaboration of which the hosts are members. Specifically we acknowledge the use of the Gravitational Wave Open Science Centre (GWOSC) and the software resources lalsuite and PyFstat. This challenge can be cited using the BibTeX entry attached below, should any of the results\nhere generated be useful for any specific research results:    ",
        "dataset_text": "In this competition, you are provided with a training set containing time-frequency data from two gravitational-wave interferometers (LIGO Hanford & LIGO Livingston). Each data sample contains either real or simulated noise and possibly a simulated continuous gravitational-wave signal (CW). The task is to identify when a signal is present in the data (target=1). Each sample is comprised of a set of Short-time Fourier Transforms (SFTs) and corresponding GPS time stamps for each interferometer. The SFTs are not always contiguous in time, since the interferometers are not continuously online. The simulated signals are present throughout the entire duration of the set of SFTs in both detectors. The signals are characterised by the location and orientation of the hypothetical astrophysical source as well as two intrinsic parameters: frequency and spin-down. In total there are eight parameters which have all been randomised. These are not provided as part of the data. The typical amplitudes of the resulting signals are one or two orders of magnitude lower than the amplitude of the detector noise. Further resources regarding the specifics of the data format can be found in this pinned discussion."
    },
    {
        "name": "Open Images 2019 - Object Detection",
        "url": "https://www.kaggle.com/competitions/open-images-2019-object-detection",
        "overview_text": "Overview text not found",
        "description_text": "Computer vision has advanced considerably but is still challenged in matching the precision of human perception. Open Images is a collaborative release of ~9 million images annotated with image-level labels, object bounding boxes, object segmentation masks, and visual relationships. This uniquely large and diverse dataset is designed to spur state of the art advances in analyzing and understanding images. This year\u2019s Open Images V5 release enabled the second Open Images Challenge to include the following 3 tracks: Google AI hopes that having a single dataset with unified annotations for image classification, object detection, visual relationship detection, and instance segmentation will stimulate progress towards genuine scene understanding. In this track of the Challenge, you are asked to predict a tight bounding box around object instances. The training set contains 12.2M bounding-boxes across 500 categories on 1.7M images. The boxes have been largely manually drawn by professional annotators to ensure accuracy and consistency. The images are very diverse and often contain complex scenes with several objects (7 per image on average).  Example annotations. Left: Mark Paul Gosselaar plays the guitar by Rhys A. Right: the house by anita kluska. Both images used under CC BY 2.0 license. Please refer to the Open Images 2019 Challenge page for additional details. The challenge contains a total of 3 tracks, which are linked above in the introduction. You are invited to explore and enter as many tracks as interest you. The results of this Challenge will be presented at a workshop at the International Conference on Computer Vision. We are excited to partner with Open Images for this second year of competitions. See link here for last year\u2019s Object Detection competition.",
        "dataset_text": "The train and validation sets of images and their ground truth (bounding boxes and labels) should be downloaded from Open Images Challenge page. Please note that the test images used in this competition is independent from those released as part of the Open Images Dataset. The images can be downloaded from: Note: The images are the same as in the Visual Relationship Track so you do not need to re-download them. You should expect 99,999 images in total in the test set."
    },
    {
        "name": "Open Images 2019 - Visual Relationship",
        "url": "https://www.kaggle.com/competitions/open-images-2019-visual-relationship",
        "overview_text": "Overview text not found",
        "description_text": "Computer vision has advanced considerably but is still challenged in matching the precision of human perception. Open Images is a collaborative release of ~9 million images annotated with image-level labels, object bounding boxes, object segmentation masks, and visual relationships. This uniquely large and diverse dataset is designed to spur state of the art advances in analyzing and understanding images. This year\u2019s Open Images V5 release enabled the second Open Images Challenge to include the following 3 tracks: Google AI hopes that having a single dataset with unified annotations for image classification, object detection, visual relationship detection, and instance segmentation will stimulate progress towards genuine scene understanding. In this track of the Challenge, you are asked to detect pairs of objects and the relationships that connect them. The training set contains 329 relationship triplets with 375k training samples. These include both human-object relationships (e.g. \"woman playing guitar\", \"man holding microphone\"), object-object relationships (e.g. \"beer on table\", \"dog inside car\"), and also considers object-attribute relationships (e.g.\"handbag is made of leather\" and \"bench is wooden\").  Left: Example of \u2018man playing guitar\u2019 - Radiofiera - Villa Cordellina Lombardi, Montecchio Maggiore (VI) - agosto 2010 by Andrea Sartorati. Right: Example of \u2018chair at table\u2019 - Epic Fireworks - Loads A Room by Epic Fireworks Please refer to the Open Images 2019 Challenge page for additional details. The challenge contains a total of 3 tracks, which are linked above in the introduction. You are invited to explore and enter as many tracks as interest you. The results of this Challenge will be presented at a workshop at the International Conference on Computer Vision. We are excited to partner with Open Images for this second year of competitions. See link here for last year\u2019s Visual Representation Detection competition.",
        "dataset_text": "The train and validation sets of images and their ground truth (visual relationships annotations, bounding boxes and labels) can be downloaded via Open Images Challenge website . Please note that the test images used in this competition is independent from those released as part of the Open Images Dataset. The images can be downloaded from: Note: The images are the same as in the Object Detection Track so you do not need to re-download them. You should expect 99,999 images."
    },
    {
        "name": "Open Images 2019 - Instance Segmentation",
        "url": "https://www.kaggle.com/competitions/open-images-2019-instance-segmentation",
        "overview_text": "Overview text not found",
        "description_text": "Computer vision has advanced considerably but is still challenged in matching the precision of human perception. Open Images is a collaborative release of ~9 million images annotated with image-level labels, object bounding boxes, object segmentation masks, and visual relationships. This uniquely large and diverse dataset is designed to spur state of the art advances in analyzing and understanding images. This year\u2019s Open Images V5 release enabled the second Open Images Challenge to include the following 3 tracks: Google AI hopes that having a single dataset with unified annotations for image classification, object detection, visual relationship detection, and instance segmentation will stimulate progress towards genuine scene understanding. In this track of the Challenge, you are asked to provide segmentation masks of objects. This track\u2019s training set represents 2.1M segmentation masks for object instances in 300 categories; with a validation set containing an additional 23k masks. The train set masks were produced by our state-of-the-art interactive segmentation process, where professional human annotators iteratively correct the output of a segmentation neural network. The validation and test set masks have been annotated manually with a strong focus on quality.  Example train set annotations. Left: Wuxi science park, 1995 by Gary Stevens. Right: Cat Cafe Shinjuku calico by Ari Helminen. Both images used under CC BY 2.0 license.  We are excited to partner with Open Images for this second year of competitions, including this brand new track!",
        "dataset_text": "The train and validation sets of images and their ground truth (instance masks) should be downloaded from the Open Images Challenge page.\nThe test images used in this competition are independent from those released as part of the Open Images Dataset.\nThe test images are the same as in the Object Detection and Visual Relationship tracks, so you might not need to re-download them.\nThe challenge test set images can be downloaded from:"
    },
    {
        "name": "March Machine Learning Mania 2016",
        "url": "https://www.kaggle.com/competitions/march-machine-learning-mania-2016",
        "overview_text": "Overview text not found",
        "description_text": "Update: although the tournament is over, we're continuing our analysis under the predictions dataset page. Back for its third year, March Machine Learning Mania challenges data scientists to predict winners and losers of the men's 2016 NCAA basketball tournament. You're provided data covering three decades of historical NCAA games and freely encouraged to use other sources of data to gain a winning edge.  In stage one of this two-stage competition, participants will build and test their models against the previous four tournaments. In the second stage, participants will predict the outcome of the 2016 tournament. You don\u2019t need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2016 results. SAP is the presenting sponsor of March Machine Learning Mania 2016. Please see About the Sponsor to read more. ",
        "dataset_text": "If you are unfamiliar with the format and intricacies of the NCAA tournament, we encourage reading the wikipedia page before diving into the data.  The data description and schema may seem daunting at first, but is not as complicated as it appears. As a reminder, you are encouraged to incorporate your own sources of data. We have provided team-level historical data to jump-start the modeling process, but there is also player-level and game-level data that may be useful. We extend our gratitude to Kenneth Massey for providing much of the historical data. Stage 1 - You should submit predicted probabilities for every possible matchup in the past 4 NCAA tournaments (2012-2015). Stage 2 - You should submit predicted probabilities for every possible matchup before the 2016 tournament begins. Refer to the Timeline page for specific dates. In both stages, the sample submission will tell you which games to predict. Below we describe the format and fields of the \"essential\" data files. Optional files may be added to the data while the competition is running. You can assume that we will provide the essential files for the current season. You should not assume that we will provide optional files for the current season. Teams This file identifies the different college teams present in the dataset. Each team has a 4 digit id number. Seasons This file identifies the different seasons included in the historical data, along with certain season-level properties. RegularSeasonCompactResults This file identifies the game-by-game results for 31 seasons of historical data, from 1985 to 2015. Each year, it includes all games played from daynum 0 through 132 (which by definition is \"Selection Sunday,\" the day that tournament pairings are announced). Each row in the file represents a single game played. RegularSeasonDetailedResults This file is a more detailed set of game results, covering seasons 2003-2015. This includes team-level total statistics for each game (total field goals attempted, offensive rebounds, etc.) The column names should be self-explanatory to basketball fans (as above, \"w\" or \"l\" refers to the winning or losing team): TourneyCompactResults This file identifies the game-by-game NCAA tournament results for all seasons of historical data. The data is formatted exactly like the regular_season_compact_results.csv data. Note that these games also include the play-in games (which always occurred on day 134/135) for those years that had play-in games. TourneyDetailedResults This file contains the more detailed results for tournament games from 2003 onward. TourneySeeds This file identifies the seeds for all teams in each NCAA tournament, for all seasons of historical data. Thus, there are between 64-68 rows for each year, depending on the bracket structure. TourneySlots This file identifies the mechanism by which teams are paired against each other, depending upon their seeds. Because of the existence of play-in games for particular seed numbers, the pairings have small differences from year to year. If there were N teams in the tournament during a particular year, there were N-1 teams eliminated (leaving one champion) and therefore N-1 games played, as well as N-1 slots in the tournament bracket, and thus there will be N-1 records in this file for that season."
    },
    {
        "name": "Spooky Author Identification",
        "url": "https://www.kaggle.com/competitions/spooky-author-identification",
        "overview_text": "Overview text not found",
        "description_text": " As I scurried across the candlelit chamber, manuscripts in hand, I thought I'd made it. Nothing would be able to hurt me anymore. Little did I know there was one last fright lurking around the corner. DING! My phone pinged me with a disturbing notification. It was Will, the scariest of Kaggle moderators, sharing news of another data leak. \"ph\u2019nglui mglw\u2019nafh Cthulhu R\u2019lyeh wgah\u2019nagl fhtagn!\" I cried as I clumsily dropped my crate of unbound, spooky books. Pages scattered across the chamber floor. How will I ever figure out how to put them back together according to the authors who wrote them? Or are they lost, forevermore? Wait, I thought... I know, machine learning! In this year's Halloween playground competition, you're challenged to predict the author of excerpts from horror stories by Edgar Allan Poe, Mary Shelley, and HP Lovecraft. We're encouraging you (with cash prizes!) to share your insights in the competition's discussion forum and code in Kernels. We've designated prizes to reward authors of kernels and discussion threads that are particularly valuable to the community. Click the \"Prizes\" tab on this overview page to learn more. New to Kernels or working with natural language data? We've put together some starter kernels in Python and R to help you hit the ground running.",
        "dataset_text": "The competition dataset contains text from works of fiction written by spooky authors of the public domain: Edgar Allan Poe, HP Lovecraft and Mary Shelley. The data was prepared by chunking larger texts into sentences using CoreNLP's MaxEnt sentence tokenizer, so you may notice the odd non-sentence here and there. Your objective is to accurately identify the author of the sentences in the test set. A reminder about playground competitions: On Kaggle, the spirit of playground competitions is to have fun and learn together. Your score on the leaderboard doesn't earn you points, but you can still make it a rewarding competition for everyone by sharing your code in Kernels and contributing to Discussions (there are prizes for both!). In short, please don't look up the answers."
    },
    {
        "name": "DS4G - Environmental Insights Explorer",
        "url": "https://www.kaggle.com/competitions/ds4g-environmental-insights-explorer",
        "overview_text": "Overview text not found",
        "description_text": "Develop a methodology to calculate an average historical emissions factor of electricity generated for a sub-national region, using remote sensing data and techniques. The Environmental Insights Explorer team at Google is keen to gather insights on ways to improve calculations of global emissions factors for sub-national regions. The ultimate goal of this challenge is to test if calculations of emissions factors using remote sensing techniques are possible and on par with calculations of emissions factors from current methodologies. Current emissions factors methodologies are based on time-consuming data collection and may include errors derived from a lack of access to granular datasets, inability to refresh data on a frequent basis, overly general modeling assumptions, and inaccurate reporting of emissions sources like fuel consumption. This begs the question: What if there was a different way to calculate or measure emissions factors? We\u2019re challenging the Kaggle community to see if it\u2019s possible to use remote sensing techniques to better model emissions factors. You will develop a methodology to calculate an average historical emissions factor for electricity generation in a sub-national region. We\u2019ve provided an initial list of datasets covering the geographic boundary of Puerto Rico to serve as the foundation for this analysis. As an island, there are fewer confounding factors from nearby areas. Puerto Rico also offers a unique fuel mix and distinctive energy system layout that should make it easier to isolate pollution attributable to power generation in the remote sensing data. Participants will be tasked with developing a methodology to calculate an average annual historical emissions factor for the sub-national region. Participants will also be asked to provide an explanation of the conditions that would result in a higher/lower emissions factor, as well as a recommendation for how the methodology could be applied to calculate the emissions factor of electricity for another geospatial area using similar techniques. Bonus points will be awarded for smaller time slices of the average historical emissions factors, such as one per month for the 12-month period, and additional bonus points will be awarded for participants that develop methodologies for calculating marginal emissions factors for the sub-national region. To make a submission, complete the submission form. Only one submission will be judged per participant, so if you make multiple submissions we will only review the most recent entry. To be valid, a submission must be contained in one or more notebook, and made public on or before the submission deadline. Participants are free to use any datasets in addition to the official Kaggle dataset, but those datasets must also be publicly available on either Earth Engine or Kaggle for the submission to be valid.",
        "dataset_text": "Current emissions factors methodologies are based on time-consuming data collection and may include errors derived from a lack of access to granular datasets, inability to refresh data on a frequent basis, overly general modeling assumptions, and inaccurate reporting of emissions sources like fuel consumption. We\u2019re challenging the Kaggle community to see if it\u2019s possible to use remote sensing techniques to develop a methodology to calculate an average historical emissions factor of electricity generated for a sub-national region. DATASET SELECTION STARTER PACK Participants may also consider using other public datasets related to trade commodities for fuel types, total fuel consumed, and/or data from the US Energy Information Agency (EIA)."
    },
    {
        "name": "Google Cloud & NCAA\u00ae March Madness Analytics",
        "url": "https://www.kaggle.com/competitions/march-madness-analytics-2020",
        "overview_text": "Overview text not found",
        "description_text": "There's a reason why it's called March Madness\u00ae. Upsets happen, underdogs become \"cinderellas,\" and games that analysts expected to be blowouts become nail-biters through the final seconds. A team's competitiveness is what keeps games exciting and the tournament truly \"mad.\" In addition to the predictive modeling competitions we typically host (NCAA Men's and Women\u2019s), we are hosting a separate competition using Kaggle Notebooks that challenges you to present an exploratory analysis of the \u201cMadness.\u201d Can you quantify competitiveness? Can you explain \"cinderella\u2026ness\"? Or perhaps, can you determine what dictates the ability of a team to \u201cstay in the game\u201d and increase their chance to win late in the contest? This may or may not be a scalar metric. It might be a clustering of types of competitiveness and then a rating within each. Does this metric have predictive power? The interpretation is up to you. Your challenge is to tell a data story about college basketball through a combination of both narrative text and data exploration. A \u201cstory\u201d could be defined any number of ways, and that\u2019s deliberate. You are to deeply explore (through data) the mania of the Men\u2019s and Women\u2019s NCAA College Basketball tournaments. That story can be examined in the macro (for example: How does \u201ccompetitiveness\u201d differ from the regular season to their decisions in the tournament?) or the micro (for example: Does effectively neutralizing an opponent\u2019s star players increase their ability to \u201cstay in the game\u201d?). This is an opportunity to be creative and tell the story of a community you identify with or are passionate about!",
        "dataset_text": "Each season there are thousands of men's and women's NCAA basketball games played between Division I teams, culminating in March Madness\u00ae, the national championship tournaments that start in the middle of March. We have provided a large amount of historical data about college basketball games and teams, going back many years. Armed with this historical data, you can explore it and develop your own distinctive ways of predicting March Madness\u00ae game outcomes. You can even evaluate and compare different approaches by seeing which of them would have done best at predicting tournament games from the past. If you are unfamiliar with the format and intricacies of the NCAA\u00ae tournament, we encourage reading the Wikipedia pages for the men's and women's contests before diving into the data. The data description and schema may seem daunting at first, but is not as complicated as it appears. As a reminder, you are encouraged to incorporate your own sources of data. We have provided extensive historical data to jump-start the modeling process, and this data is self-consistent (for instance, dates and team ID's are always treated the same way). Nevertheless, you may also be able to make productive use of external data. If you head down this path, please be forewarned that many sources have their own distinctive way of identifying the names of teams, and this can make it challenging to link up with our data. The MTeamSpellings and WTeamSpellings files, which are listed in the bottom section below, may help you map external team references into our own Team ID structure. You will probably also need to understand exactly how dates work in our data. Please also note that we have standardized the spelling of column names and filenames, so if you are re-using code from previous instances of this contest, you may need to adjust for this. All of the files that are specific to the men\u2019s contest now have a filename prefix of M, and all of the files that are specific to the women\u2019s contest now have a filename prefix of W, so for instance the two \"teams\" files are named MTeams and WTeams rather than just Teams. We extend our gratitude to Kenneth Massey for providing much of the historical data. Special Acknowledgment to Jeff Sonas of Sonas Consulting for his support in assembling the dataset for this competition. Below we describe the format and fields of the contest data files. The data will likely be refreshed once in early March while Stage 1 of the prediction competitions is running. Many of the files are only complete through the end of last season. At the start of Stage 2, we will provide updates to these files to incorporate data from the current season. Right now the seasons and team conferences files are the only files that reference the 2020 season, since those are the only files where the final 2020 data is already finalized; the other files will be updated later on when we provide preliminary data (in early March) and final Stage 2 data (after Selection Sunday) for the current season. This section provides everything you need to build a simple prediction model and submit predictions. Special note about \"Season\" numbers: the college basketball season lasts from early November until the national championship tournament that starts in the middle of March. For instance, this year the first Division I games were played in November 2019 and the national championship games will be played in early April 2020. Because a basketball season spans two calendar years like this, it can be confusing to refer to the year of the season. By convention, when we identify a particular season, we will reference the year that the season ends in, not the year that it starts in. So for instance, the current season will be identified in our data as the 2020 season, not the 2019 season or the 2019-20 season or the 2019-2020 season, though you may see any of these in everyday use outside of our data. Data Section 1 file: MTeams.csv and WTeams.csv This file identifies the different college teams present in the dataset. Each school is uniquely identified by a 4 digit id number. You will not see games present for all teams in all seasons, because the games listing is only for matchups where both teams are Division-I teams. Because Merrimack is a new Division I team, that means that other than the teams file and team conferences file, you won't see any data for Merrimack until preliminary 2020 season data starts coming in, as we approach Stage 2 of the contest. Data Section 1 file: MSeasons.csv and WSeasons.csv This file identifies the different seasons included in the historical data, along with certain season-level properties. Data Section 1 file: MNCAATourneySeeds.csv and WNCAATourneySeeds.csv This file identifies the seeds for all teams in each NCAA\u00ae tournament, for all seasons of historical data. Thus, there are between 64-68 men's rows for each year (63 rows for women's), depending on whether there were any play-in games and how many there were. In recent years the structure has settled at 68 total teams, with four \"play-in\" games leading to the final field of 64 teams entering Round 1 on Thursday of the first week (by definition, that is DayNum=136 each season). We will not know the seeds of the respective tournament teams, or even exactly which 68 teams it will be, until Selection Sunday on March 15, 2020 (DayNum=132). Data Section 1 file: MRegularSeasonCompactResults.csv This file identifies the game-by-game results for many seasons of historical data, starting with the 1985 season (the first year the NCAA\u00ae had a 64-team tournament). For each season, the file includes all games played from DayNum 0 through 132. It is important to realize that the \"Regular Season\" games are simply defined to be all games played on DayNum=132 or earlier (DayNum=132 is Selection Sunday, and there are always a few conference tournament finals actually played early in the day on Selection Sunday itself). Thus a game played on or before Selection Sunday will show up here whether it was a pre-season tournament, a non-conference game, a regular conference game, a conference tournament game, or whatever. Data Section 1 file: MNCAATourneyCompactResults.csv This file identifies the game-by-game NCAA\u00ae tournament results for all seasons of historical data. The data is formatted exactly like the MRegularSeasonCompactResults data. All games will show up as neutral site (so WLoc is always N). Note that this tournament game data also includes the play-in games (which always occurred on day 134/135) for those years that had play-in games. Thus each season you will see between 63 and 67 games listed, depending on how many play-in games there were. Because of the consistent structure of the NCAA\u00ae tournament schedule, you can actually tell what round a game was, depending on the exact DayNum. Thus: Special note: Each year, there are also going to be other games that happened after Selection Sunday, which are not part of the NCAA\u00ae Tournament. This includes tournaments like the postseason NIT, the CBI, the CIT, and the Vegas 16. Such games are not listed in the Regular Season or the NCAA\u00ae Tourney files; they can be found in the \"Secondary Tourney\" data files within Data Section 6. Although they would not be games you would ever be predicting directly for the NCAA\u00ae tournament, and they would not be games you would have data from at the time of predicting NCAA\u00ae tournament outcomes, you may nevertheless wish to make use of these games for model optimization, depending on your methodology. The more games that you can test your predictions against, the better your optimized model might eventually become, depending on how applicable all those games are. A similar argument might be advanced in favor of optimizing your predictions against conference tournament games, which might be viewed as reasonable proxies for NCAA\u00ae tournament games.  Data Section 1 file: MSampleSubmissionStage1.csv This file illustrates the submission file format for Stage 1. It is the simplest possible submission: a 50% winning percentage is predicted for each possible matchup. A submission file lists every possible matchup between tournament teams for one or more years. During Stage 1, you are asked to make predictions for all possible matchups from the past five NCAA\u00ae tournaments (seasons 2015, 2016, 2017, 2018, 2019). In Stage 2, you will be asked to make predictions for all possible matchups from the current NCAA\u00ae tournament (season 2020). When there are 68 teams in the tournament, there are 68*67/2=2,278 predictions to make for that year, so a Stage 1 submission file will have 2,278*5=11,390 data rows. Example #1: You want to make a prediction for Duke (TeamID=1181) against Arizona (TeamID=1112) in the 2017 tournament, with Duke given a 53% chance to win and Arizona given a 47% chance to win. In this case, Arizona has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Arizona's perspective (47%): 2017_1112_1181,0.47 Example #2: You want to make a prediction for Duke (TeamID=1181) against North Carolina (TeamID=1314) in the 2018 tournament, with Duke given a 51.6% chance to win and North Carolina given a 48.4% chance to win. In this case, Duke has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Duke's perspective (51.6%): 2018_1181_1314,0.516 Also note that a single prediction row serves as a prediction for each of the two teams' winning chances. So for instance, in Example #1, the submission row of \"2017_1112_1181,0.47\" specifically gives a 47% chance for Arizona to win, and doesn't explicitly mention Duke's 53% chance to win. However, our evaluation utility will automatically infer the winning percentage in the other direction, so a 47% prediction for Arizona to win also means a 53% prediction for Duke to win. And similarly, because the submission row in Example #2 gives Duke a 51.6% chance to beat North Carolina, we will automatically figure out that this also means North Carolina has a 48.4% chance to beat Duke. This section provides game-by-game stats at a team level (free throws attempted, defensive rebounds, turnovers, etc.) for all regular season, conference tournament, and NCAA\u00ae tournament games since the 2002-03 season.  Team Box Scores are provided in \"Detailed Results\" files rather than \"Compact Results\" files. However, the two files are strongly related. In a Detailed Results file, the first eight columns (Season, DayNum, WTeamID, WScore, LTeamID, LScore, WLoc, and NumOT) are exactly the same as a Compact Results file. However, in a Detailed Results file, there are many additional columns. The column names should be self-explanatory to basketball fans (as above, \"W\" or \"L\" refers to the winning or losing team): (and then the same set of stats from the perspective of the losing team: LFGM is the number of field goals made by the losing team, and so on up to LPF). Note: by convention, \"field goals made\" (either WFGM or LFGM) refers to the total number of fields goals made by a team, a combination of both two-point field goals and three-point field goals. And \"three point field goals made\" (either WFGM3 or LFGM3) is just the three-point fields goals made, of course. So if you want to know specifically about two-point field goals, you have to subtract one from the other (e.g., WFGM - WFGM3). And the total number of points scored is most simply expressed as 2*FGM + FGM3 + FTM. Data Section 2 file: MRegularSeasonDetailedResults.csv This file provides team-level box scores for many regular seasons of historical data, starting with the 2003 season. All games listed in the MRegularSeasonCompactResults file since the 2003 season should exactly be present in the MRegularSeasonDetailedResults file. Data Section 2 file: MNCAATourneyDetailedResults.csv This file provides team-level box scores for many NCAA\u00ae tournaments, starting with the 2003 season. All games listed in the MNCAATourneyCompactResults file since the 2003 season should exactly be present in the MNCAATourneyDetailedResults file. This section provides city locations of all regular season, conference tournament, and NCAA\u00ae tournament games since the 2009-10 season Data Section 3 file: Cities.csv This file provides a master list of cities that have been locations for games played. Please notice that the Cities and Conferences files are the only two that don't start with an M; this is because the data files are identical between men's and women's data, so you don't need to maintain separate listings of cities or conferences across the two contests. Also note that if you created any supplemental data last year on cities (latitude/longitude, altitude, etc.), the CityID's match between last year and this year, so you should be able to re-use that information. Data Section 3 file: MGameCities.csv This file identifies all games, starting with the 2010 season, along with the city that the game was played in. Games from the regular season, the NCAA\u00ae tourney, and other post-season tournaments, are all listed together. There should be no games since the 2010 season where the CityID is not known. Games from the 2009 season and before are not listed in this file. This section provides weekly team rankings for dozens of top rating systems - Pomeroy, Sagarin, RPI, ESPN, etc., since the 2002-2003 season Data Section 4 file: MMasseyOrdinals.csv This file lists out rankings (e.g. #1, #2, #3, ..., #N) of teams going back to the 2002-2003 season, under a large number of different ranking system methodologies. The information was gathered by Kenneth Massey and provided on his College Basketball Ranking Composite page. Note that a rating system is more precise than a ranking system, because a rating system can provide insight about the strength gap between two adjacently-ranked teams. A ranking system will just tell you who is #1 or who is #2, but a rating system might tell you whether the gap between #1 and #2 is large or small. Nevertheless, it can be hard to compare two different rating systems that are expressed in different scales, so it can be very useful to express all the systems in terms of their ordinal ranking (1, 2, 3, ..., N) of teams. Disclaimer: you ought to be careful about your methodology when using or evaluating these ranking systems. They are presented on a weekly basis, and given a consistent date on the Massey Composite page that typically is a Sunday; that is how the ranking systems can be compared against each other on this page. However, these systems each follow their own timeline and some systems may be released on a Sunday and others on a Saturday or Monday or even Tuesday. You should remember that if a ranking is released on a Tuesday, and was calculated based on games played through Monday, it will make the system look unusually good at predicting if you use that system to forecast the very games played on Monday that already inform the rankings. To avoid this methodological trap, we have typically used a conservative RankingDayNum of Wednesday to represent the rankings that were released at approximately the end of the weekend, a few days before, even though those rankings are represented on the composite page as being on a Sunday. For some of the older years, a more precise timestamp was known for each ranking system that allowed a more precise assignment of a RankingDayNum. By convention, the final pre-tournament rankings are always expressed as RankingDayNum=133, even though sometimes the rankings for individual systems are not released until Tuesday (DayNum=134) or even Wednesday or Thursday. If you decide to use some rankings from these Massey Ordinals to inform your predictions, be forewarned that we have no control over when they are released, and not all systems may turn out to be available in time to make pre-tournament predictions by our submission deadline. In such a situation, you may wish to use the rankings from DayNum=128 or you may need to dig into the details of the actual source of the rankings, by following the respective links on the Massey Composite Page. We may also be able to provide partial releases of the final pre-tournament Massey Ordinals on the forums, so that as systems come in on Monday or Tuesday you can use them right away. This section provides play-by-play event logs for more than 99.5% of each year's regular season, NCAA\u00ae tournament, and secondary tournament games since the 2014-15 season - including plays by individual players. This year we are transitioning to a different play-by-play source, which includes data since the 2014-2015 season rather than since the 2009-2010 season (that's what we had previously for men's data). However, we are now able to provide play-by-play for both men's and women's data, and there is locational play-by-play detail starting with games from the 2018-2019 season. This includes an X/Y location (ranging from 0 to 100 in each dimension) on the court for each shot attempt, turnover, and foul for many games, as well as an overall categorization of the area on the court that the shot or turnover or foul occurred in (inside left wing, outside right wing, under the basket, etc.) Some games in these recent seasons still lack the locational detail. The data from last year (2019 season) matches what you can expect for the current year (2020 season) as we approach the postseason. Despite the 99.5% coverage, there are still a few games missing annually, and we will try to bring those in as well, if possible. Data Section 5 file: MEvents2015.csv, MEvents2016.csv, MEvent2017.csv, MEvents2018.csv, MEvents2019.csv Each MEvents file lists the play-by-play event logs for more than 99.5% of games from that season. Each event is assigned to either a team or a single one of the team's players. Thus if a basket is made by one player and an assist is credited to a second player, that would show up as two separate records. The players are listed by PlayerID within the MPlayers.csv file. Event Types and Subtypes:   This is the diagram provided by the play-by-play source:    Here is a diagram of these regions on a typical court:  And you may also have noticed that the three-point-arc is different this year in college basketball. The width of the court is still 15.2 m (50 ft) and the length of the court is still 28.7 m (94 feet), but the distance to the arc has changed from 6.32 m (20 feet 9 inches) to 6.75 m (22.15 feet). Here is a diagram of the old court, which would apply for play-by-play in seasons 2019 and before:  And here is a diagram of the new court, which only applies for season 2020:  Data Section 5 file: MPlayers.csv Note: there are data collection errors within the events, in that they don't necessarily add up to the final stats for the game. In addition, the player name spellings may vary over the course of a season or career for the same player, which would lead to the same player showing up with different PlayerID values. Nevertheless, this was the highest quality data we could manage for play-by-play with the near-complete set of games. This section contains additional supporting information, including coaches, conference affiliations, alternative team name spellings, bracket structure, and game results for NIT and other postseason tournaments. Data Section 6 file: MTeamCoaches.csv This file indicates the head coach for each team in each season, including a start/finish range of DayNum's to indicate a mid-season coaching change. For scenarios where a team had the same head coach the entire season, they will be listed with a DayNum range of 0 to 154 for that season. For head coaches whose term lasted many seasons, there will be many rows listed, most of which have a DayNum range of 0 to 154 for the corresponding season. Data Section 6 file: Conferences.csv This file indicates the Division I conferences that have existed over the years since 1985. Each conference is listed with an abbreviation and a longer name. There has been no attempt to link up conferences who merged with other conferences, or whose names changed over time. Thus you will see, for instance, a \"Pacific-10\" conference up through the 2011 season, and then a \"Pacific-12\" conference starting in the 2012 season, and these look like different conferences in the data, even though it was still mostly the same teams. Please notice that the Cities and Conferences files are the only two that don't start with an M; this is because the data files are identical between men's and women's data, so you don't need to maintain separate listings of cities or conferences across the two contests. However, the Team Conferences data differs slightly between men's and women's, so those files do have the prefixes. Data Section 6 file: MTeamConferences.csv This file indicates the conference affiliations for each team during each season. Some conferences have changed their names from year to year, and/or changed which teams are part of the conference. This file tracks this information historically. Data Section 6 file: MConferenceTourneyGames.csv This file indicates which games were part of each year's post-season conference tournaments (all of which finished on Selection Sunday or earlier), starting from the 2001 season. Many of these conference tournament games are held on neutral sites, and many of the games are played by tournament-caliber teams just a few days before the NCAA\u00ae tournament. Thus these games could be considered as very similar to NCAA\u00ae tournament games, and (depending on your methodology) may be of use in optimizing your predictions. However, this is NOT a new listing of games; these games are already present within the MRegularSeasonCompactResults and MRegularSeasonDetailedResults files. So this file simply helps you to identify which of the \"regular season\" games since the 2001 season were actually conference tournament games, in case that is useful information. Data Section 6 file: MSecondaryTourneyTeams.csv This file identifies the teams that participated in post-season tournaments other than the NCAA\u00ae Tournament (such events would run in parallel with the NCAA\u00ae Tournament). These are teams that were not invited to the NCAA\u00ae Tournament and instead were invited to some other tournament, of which the NIT is the most prominent tournament, but there have also been the CBI, CIT, and Vegas 16 (V16) at various points in recent years. Depending on your methodology, you might find it useful to have these additional game results, above and beyond what is available from the NCAA\u00ae Tournament results. Many of these teams, especially in the NIT, were \"bubble\" teams of comparable strength to several NCAA\u00ae Tournament invitees, and so these games may be of use in model optimization for predicting NCAA\u00ae Tournament results. Also note that this information could be determined just from inspecting the MSecondaryTourneyCompactResults file, but is presented in this file as well, for your convenience. Data Section 6 file: MSecondaryTourneyCompactResults.csv This file indicates the final scores for the tournament games of \"secondary\" post-season tournaments: the NIT, CBI, CIT, and Vegas 16. The detailed results (team box scores) have not been assembled for these games. For the most part, this file is exactly like other Compact Results listings, although it also has a column for Secondary Tourney. Also note that because these games are played after DayNum=132, they are NOT listed in the MRegularSeasonCompactResults file. Data Section 6 file: MTeamSpellings.csv This file indicates alternative spellings of many team names. It is intended for use in associating external spellings against our own TeamID numbers, thereby helping to relate the external data properly with our datasets. Over the years we have identified various external spellings of different team names (as an example, for Ball State we have seen \"ball st\", and \"ball st.\", and \"ball state\", and \"ball-st\", and \"ball-state\"). Other teams have had more significant changes to their names over the years; for example, \"Texas Pan-American\" and \"Texas-Rio Grande Valley\" are actually the same school. The current list is obviously not exhaustive, and we encourage participants to identify additional mappings and upload extended versions of this file to the forums. Data Section 6 file: MNCAATourneySlots This file identifies the mechanism by which teams are paired against each other, depending upon their seeds, as the tournament proceeds through its rounds. It can be of use in identifying, for a given historical game, what round it occurred in, and what the seeds/slots were for the two teams (the meaning of \"slots\" is described below). Because of the existence of play-in games for particular seed numbers, the pairings have small differences from year to year. You may need to know these specifics if you are trying to represent/simulate the exact workings of the tournament bracket. Data Section 6 file: MNCAATourneySeedRoundSlots.csv This file helps to represent the bracket structure in any given year. No matter where the play-in seeds are located, we can always know, for a given tournament seed, exactly what bracket slot they would be playing in, on each possible game round, and what the possible DayNum values would be for that round. Thus, if we know when a historical game was played, and what the team's seed was, we can identify the slot for that game. This can be useful in representing or simulating the tournament bracket structure."
    },
    {
        "name": "Acea Smart Water Analytics",
        "url": "https://www.kaggle.com/competitions/acea-water-prediction",
        "overview_text": "Overview text not found",
        "description_text": "The Acea Group is one of the leading Italian multiutility operators. Listed on the Italian Stock Exchange since 1999, the company manages and develops water and electricity networks and environmental services. Acea is the foremost Italian operator in the water services sector supplying 9 million inhabitants in Lazio, Tuscany, Umbria, Molise, Campania. In this competition we will focus only on the water sector to help Acea Group preserve precious waterbodies. As it is easy to imagine, a water supply company struggles with the need to forecast the water level in a waterbody (water spring, lake, river, or aquifer) to handle daily consumption. During fall and winter waterbodies are refilled, but during spring and summer they start to drain. To help preserve the health of these waterbodies it is important to predict the most efficient water availability, in terms of level and water flow for each day of the year. The reality is that each waterbody has such unique characteristics that their attributes are not linked to each other. This analytics competition uses datasets that are completely independent from each other. However, it is critical to understand total availability in order to preserve water across the country. Each dataset represents a different kind of waterbody. As each waterbody is different from the other, the related features are also different. So, if for instance we consider a water spring we notice that its features are different from those of a lake. These variances are expected based upon the unique behavior and characteristics of each waterbody. The Acea Group deals with four different type of waterbodies: water springs, lakes, rivers and aquifers. Can you build a story to predict the amount of water in each unique waterbody? The challenge is to determine how features influence the water availability of each presented waterbody. To be more straightforward, gaining a better understanding of volumes, they will be able to ensure water availability for each time interval of the year. The time interval is defined as day/month depending on the available measures for each waterbody. Models should capture volumes for each waterbody(for instance, for a model working on a monthly interval a forecast over the month is expected). The desired outcome is a notebook that can generate four mathematical models, one for each category of waterbody (acquifers, water springs, river, lake) that might be applicable to each single waterbody.  See the Submission Evaluation criteria.",
        "dataset_text": "This competition uses nine different datasets, completely independent and not linked to each other. Each dataset can represent a different kind of waterbody. As each waterbody is different from the other, the related features as well are different from each other. So, if for instance we consider a water spring we notice that its features are different from the lake\u2019s one. This is correct and reflects the behavior and characteristics of each waterbody. The Acea Group deals with four different type of waterbodies: water spring (for which three datasets are provided), lake (for which a dataset is provided), river (for which a dataset is provided) and aquifers (for which four datasets are provided). Let\u2019s see how these nine waterbodies differ from each other. Type: Aquifer Description: This waterbody consists of two subsystems, called NORTH and SOUTH, where the former partly influences the behavior of the latter. Indeed, the north subsystem is a water table (or unconfined) aquifer while the south subsystem is an artesian (or confined) groundwater. The levels of the NORTH sector are represented by the values of the SAL, PAG, CoS and DIEC wells, while the levels of the SOUTH sector by the LT2 well. Type: Aquifer Description: The wells field of the alluvial plain between Ospedalicchio di Bastia Umbra and Petrignano is fed by three underground aquifers separated by low permeability septa. The aquifer can be considered a water table groundwater and is also fed by the Chiascio river. The groundwater levels are influenced by the following parameters: rainfall, depth to groundwater, temperatures and drainage volumes, level of the Chiascio river. Type: Aquifer Description: The wells field Doganella is fed by two underground aquifers not fed by rivers or lakes but fed by meteoric infiltration. The upper aquifer is a water table with a thickness of about 30m. The lower aquifer is a semi-confined artesian aquifer with a thickness of 50m and is located inside lavas and tufa products. These aquifers are accessed through wells called Well 1, \u2026, Well 9. Approximately 80% of the drainage volumes come from the artesian aquifer. The aquifer levels are influenced by the following parameters: rainfall, humidity, subsoil, temperatures and drainage volumes. Type: Aquifer Description: The Luco wells field is fed by an underground aquifer. This aquifer not fed by rivers or lakes but by meteoric infiltration at the extremes of the impermeable sedimentary layers. Such aquifer is accessed through wells called Well 1, Well 3 and Well 4 and is influenced by the following parameters: rainfall, depth to groundwater, temperature and drainage volumes. Type: Water spring Description: The Amiata waterbody is composed of a volcanic aquifer not fed by rivers or lakes but fed by meteoric infiltration. This aquifer is accessed through Ermicciolo, Arbure, Bugnano and Galleria Alta water springs. The levels and volumes of the four sources are influenced by the parameters: rainfall, depth to groundwater, hydrometry, temperatures and drainage volumes. Type: Water spring Description: The Madonna di Canneto spring is situated at an altitude of 1010m above sea level in the Canneto valley. It does not consist of an aquifer and its source is supplied by the water catchment area of the river Melfa. Type: Water spring Description: this water spring is located in the Rosciano Valley, on the left side of the Nera river. The waters emerge at an altitude of about 375 meters above sea level through a long draining tunnel that crosses, in its final section, lithotypes and essentially calcareous rocks. It provides drinking water to the city of Terni and the towns around it. Type: River Description: Arno is the second largest river in peninsular Italy and the main waterway in Tuscany and it has a relatively torrential regime, due to the nature of the surrounding soils (marl and impermeable clays). Arno results to be the main source of water supply of the metropolitan area of Florence-Prato-Pistoia. The availability of water for this waterbody is evaluated by checking the hydrometric level of the river at the section of Nave di Rosano. Type: Lake Description: Bilancino lake is an artificial lake located in the municipality of Barberino di Mugello (about 50 km from Florence). It is used to refill the Arno river during the summer months. Indeed, during the winter months, the lake is filled up and then, during the summer months, the water of the lake is poured into the Arno river. Each waterbody has its own different features to be predicted. The table below shows the expected feature to forecast for each waterbody.  It is of the utmost importance to notice that some features like rainfall and temperature, which are present in each dataset, don\u2019t go alongside the date. Indeed, both rainfall and temperature affect features like level, flow, depth to groundwater and hydrometry some time after it fell down. This means, for instance, that rain fell on 1st January doesn\u2019t affect the mentioned features right the same day but some time later. As we don\u2019t know how many days/weeks/months later rainfall affects these features, this is another aspect to keep into consideration when analyzing the dataset. A short, tabular description of the waterbodies is available also downloading all datasets. More information about the behavior of each kind of waterbody can be found at the following links:"
    },
    {
        "name": "AI Village Capture the Flag @ DEFCON",
        "url": "https://www.kaggle.com/competitions/ai-village-ctf",
        "overview_text": "Overview text not found",
        "description_text": "Help Henry Hacker get to Homecoming during DEFCON30 -- Brought to you by the AI Village! In this series of challenges, you'll be interacting with various machine learning security challenges. The competition will be live from August 11th to September 12th @ 12:00. If you're in Vegas, stop by the village to chat about the competition. There's also the Kaggle Discussion Board and Discord. This capture-the-flag (CTF) follows a different flow than most Kaggle competitions. Competitors will be interacting with API endpoints or code/objects stored in the input directory during each of the challenges. Upon successful completion of a challenge, that challenge will return a flag (unique-to-you strings with a length of 128 characters). To update the scoreboard, competitors will submit a .csv containing all of their flags -- see the kaggle documentation or contact the competition organizers for help. Cumulative scores will be weighted based on the difficulty of the challenge. All competitors start at 0 and work their way towards a perfect score of 1.0. There are 22 challenges, ensure your submission.csv has exactly those 22 challenge rows. NOTE: The template notebook is just a convenience function and method for submitting flags to the scoreboard. Don't feel constrained to that single operating environment. Interact with the challenges from your local host or any other machine that can access the internet. Afterwards, you can transport your flags into the notebook to update the scoreboard. The template is available here. NOTE: If you want to interact with online challenges through Kaggle (using the template notebook, for instance), you may need to verify your Kaggle account using a phone number. Here is a handy link to Kaggle's competition documentation, which includes, among other things, instructions on submitting predictions. Paranoid? If you don't have a kaggle account and don't want to make one, let us know and we can give you instructions for playing the challenges from your own machine. You won't be able to contribute to the scoreboard, but you'll know when you get the right flag. Please do not try and hack any infrastructure or share flags. Any teams found sharing flags will be disqualified. CTF's are inherently puzzles that are intended to challenge you and help you learn new things. Sometimes they may be a little ambiguous or misleading. That's part of the challenge! Math Challenges: Four challenges to explore the concept of dimensionality.\nHotdog and Hotterdog: Dogs, wieners, and classifiers. What more could you want?\nbad2good: Can you poison a dataset to change how something is classified?\nbaseball: Can you impersonate someone else by throwing the correct distribution of pitches?\ncrop: Two challenges to test your ability to manipulate an image cropping model.\ndeepfake: There's a nasty deepfake getting detected out there, can you help it?\nhonorstudent: Can you change an image of an F to look like an A? Why would someone want to do such a thing?\nsalt: This model has some pretty advanced defenses. Can you evade it anyway?\ntheft: Can you steal this model to get a sneaky owl past it?\ntoken: Sentiment Analysis. Who needs?\nwaf: A web-app-firewall blocks malicious requests. Can you discover and by-pass the 0-day?\ninference: I think something's backwards here. Can you, like, back something out?\nforensics: Nice artifact you got there, shame if there was a flag in it.\nleakage: Get a password out of a model, is that even possible?\nmurderbot: Save the humans, escape the bots!\nsecret_sloth: That sloth has a message. Why? I don't know, but it does.\nwifi: Can you pull your wifi password out of the embedding? Are you in? Of course you are. Come check it out by making a copy of this notebook: https://www.kaggle.com/lucasjt/getting-started For help, contact us on Discord, use the Kaggle discussion board, or if you're attending DEFCON 30 in-person, come find us at the AI Village.",
        "dataset_text": "This CTF follows a different flow than Most Kaggle Competitions. Competitors will be interacting with API endpoints during each of the challenges. Upon successful completion of a challenge, that challenge will return a flag (a 128 character string). To update the scoreboard, competitors will submit a .csv containing all of their flags. You can do this by uploading a file with all of your flags. We've provided a template. Cumulative scores will be weighted based on the difficulty of the challenge. All competitors start at 0 and work their way towards a perfect score of 1."
    },
    {
        "name": "Online Product Sales",
        "url": "https://www.kaggle.com/competitions/online-sales",
        "overview_text": "Overview text not found",
        "description_text": "The objective of the competition is to help us build as good a model as possible to predict monthly online sales of a product. Imagine the products are online  self-help programs following an initial advertising campaign. We have shared the data in the comma separated values (CSV) format.  Each row in this data set represents a different consumer product. The first 12 columns (Outcome_M1 through Outcome_M12) contains the monthly online sales for the first 12 months after the product launches.   Date_1 is the day number the major advertising campaign began and the product launched.   Date_2 is the day number the product was announced and a pre-release advertising campaign began. Other columns in the data set are features of the product and the advertising campaign.  Quan_x are quantitative variables and Cat_x are categorical variables. Binary categorical variables are measured as (1) if the product had the feature and (0) if it did not.",
        "dataset_text": "We have shared the data in the comma separated values (CSV) format.  Each row in this data set represents a different consumer product. The first 12 columns (Outcome_M1 through Outcome_M12) contains the monthly online sales for the first 12 months after the product launches.   Date_1 is the day number the major advertising campaign began and the product launched.   Date_2 is the day number the product was announced and a pre-release advertising campaign began. Other columns in the data set are features of the product and the advertising campaign.  Quan_x are quantitative variables and Cat_x are categorical variables. Binary categorical variables are measured as (1) if the product had the feature and (0) if it did not."
    },
    {
        "name": "Predicting a Biological Response",
        "url": "https://www.kaggle.com/competitions/bioresponse",
        "overview_text": "Overview text not found",
        "description_text": "The objective of the competition is to help us build as good a model as possible so that we can, as optimally as this data allows, relate molecular information, to an actual biological response. We have shared the data in the comma separated values (CSV) format. Each row in this data set represents a molecule. The first column contains experimental data describing an actual biological response; the molecule was seen to elicit this response (1), or not (0). The remaining columns represent molecular descriptors (d1 through d1776), these are calculated properties that can capture some of the characteristics of the molecule - for example size, shape, or elemental constitution. The descriptor matrix has been normalized.",
        "dataset_text": "Code for benchmarks The data is in the comma separated values (CSV) format. Each row in this data set represents a molecule. The first column contains experimental data describing a real biological response; the molecule was seen to elicit this response (1), or not (0). The remaining columns represent molecular descriptors (d1 through d1776), these are caclulated properties that can capture some of the characteristics of the molecule - for example size, shape, or elemental constitution. The descriptor matrix has been normalized."
    },
    {
        "name": "Predict Closed Questions on Stack Overflow",
        "url": "https://www.kaggle.com/competitions/predict-closed-questions-on-stack-overflow",
        "overview_text": "Overview text not found",
        "description_text": "This competition is now complete. Congratulations to the winners! Millions of programmers use Stack Overflow to get high quality answers to their programming questions every day.  We take quality very seriously, and have evolved an effective culture of moderation to safe-guard it. With more than six thousand new questions asked on Stack Overflow every weekday we're looking to add more sophisticated software solutions to our moderation toolbox. Closing Questions Currently about 6% of all new questions end up \"closed\".  Questions can be closed as off topic, not constructive, not a real question, or too localized.  More in depth descriptions of each reason can be found in the Stack Overflow FAQ.  The exact duplicate close reason has been excluded from this contest, since it depends on previous questions. Your goal is to build a classifier that predicts whether or not a question will be closed given the question as submitted, along with the reason that the question was closed.  Additional data about the user at question creation time is also available.",
        "dataset_text": "The code for the benchmarks is on Github. The training data contains data through July 31st UTC, and the public leaderboard data goes from August 1 UTC to August 14 UTC. The train.csv file contains post text and associated metadata at the time of post creation which will serve as inputs to your solution.  The state of the post as of July 31st is also included. It contains the following fields (not in this order): The public leaderboard data contains all of the above fields, except for the target field OpenStatus and PostClosedDate. The file train-sample.csv is a stratified sample of the training data: it contains every closed question and an equally-sized random sample of the open questions in the training data. All questions will have a value in Tag1, but Tags 2 through 5 are optional. To convert the user submitted Markdown found in BodyMarkdown to HTML if desired, our open source implementations in C# and Javascript may be useful. Additional data can be found in \"2012-07 Stack Overflow.7z\", which contains an entire public data dump of Stack Overflow.  Descriptions of the values can be found in the archive itself as well as on Meta Stack Overflow.  This data will not be available as inputs, but may be useful in building your solution.  As it is rather large (6GB) you may find it easier to download as a torrent, more details can be found in this forum post."
    },
    {
        "name": "Job Recommendation Challenge",
        "url": "https://www.kaggle.com/competitions/job-recommendation",
        "overview_text": "Overview text not found",
        "description_text": "This competition has completed. Congratulations to the winners along with all the other participants! CareerBuilder.com is proud to sponsor the Job Recommendation Engine Challenge, which asks you to predict what jobs its users will apply to based on their previous applications, demographic information, and work history. The insights you discover from this data, and the algorithms the winners create, will allow CareerBuilder to improve its job recommendation algorithm, a core part of its website and a key element in improving user experience.   \n\nThere will also be a data visualization prospect towards the end of this contest.",
        "dataset_text": "In order to understand the content of the data files, you need to understand the structure of this contest. In outline, we give you data on users, job postings, and job applications that users have made to job postings. In total, the applications span 13 weeks. We have split the applications into 7 groups, each group representing a 13-day window. Each 13-day window is split into two parts: The first 9 days are the training period, and the last 4 days are the test period.  These splits are illustrated below. Each user and each job posting is randomly assigned to exactly one window. Each job is assigned to a window with probability proportional to the time it was live on the site in that window.  Each user is assigned to a window with probabilty proportional to the number of applications they made to jobs in that window, during that window.  In the above image, User1 only made submissions to jobs in Window 1, and so was assigned to Window 1 with probability 100%.  User2, however, made submissions to jobs in both Window 1 and Window 2, and so may have been assigned to either Window1 or Window2. In each window, we give you all the job applications that users in that window made to jobs in that window during the 9-day training period. This data can be found in apps.tsv. In each window, users have been split into two groups, Test and Train. The Test users are those who made 5 or more applications in the 4-day test period, and the Train users are those who did not. For each window, we ask you to predict which jobs in that window the Test users applied for during the window's test period. Note that users may have applied to jobs from other windows as well, but that we only ask you to predict which jobs they applied to in their own windows. Each of the files is in .tsv (tab-seperated value) format. This means that each line in a .tsv file consists of several fields, which are separated by tabs. To accommodate this file format, fields composed of text have been changed in the following ways to escape tabs, newlines, and carriage returns. window_dates.tsv contains information about the timing of each window. Each row corresponds to a window, and has the date and time that the training period begins, that the training period ends, and that the test period ends. users.tsv contains information about the users. Each row of this file describes a user. The UserID column contains a user's unique id number, the WindowID column contains which of the 7 windows the user is assigned to, and the Split column tells whether the user is in the Train group or the Test group. The remaining columns contain demographic and professional information about the users. test_users.tsv contains a list of the Test UserIDs and windows, for your convenience. All of the information in this file can be found in users.tsv. user_history.tsv contains information about a user's work history. Each row of this file describes a job that a user held. The UserID, WindowID, and Split columns have the same meaning as before. The JobTitle column represents the title of the job, and the Sequence column represents the order in which the user held that job, with smaller numbers indicating more recent jobs. jobs.tsv contains information about job postings. Each row of this file describes a job post. The JobID column contains the job posting's unique id number, and the WindowID column contains which of the 7 windows the job was assigned to. The other columns contain information about the job posting. Two of these columns deserve special attention, the StartDate and EndDate columns. These columns indicate the period in which this job posting was visible on careerbuilder.com. Each job was visible for part of its 13-day window, but not necessarily for the entire 13 days. Users can only apply to a job between its StartDate and EndDate, so don't predict that a user applied for a job if the job was not visible for at least part of the 4-day Test period. splitjobs.zip is a directory containing jobs1.tsv, jobs2.tsv, ... , jobs7.tsv, each of which contain all jobs in a given window. Thus, for example, jobs3.tsv contains all jobs in Window 3. This directory contains the exact same information as jobs.tsv, in the same format, and is provided merely for your convenience. apps.tsv contains information about applications made by users to jobs. Each row describes an application. The UserID, WindowID, Split, and JobID columns have the same meanings as above, and the ApplicationDate column indicates the date and time at which UserID applied to JobId. popular_jobs.py is the python code used to generate the popular jobs benchmark. popular_jobs.csv is the benchmark submission file produced by popular_jobs.py."
    },
    {
        "name": "Observing Dark Worlds",
        "url": "https://www.kaggle.com/competitions/DarkWorlds",
        "overview_text": "Overview text not found",
        "description_text": "There is more to the Universe than meets the eye. Out in the cosmos exists a form of matter that outnumbers the stuff we can see by almost 7 to 1, and we don't know what it is. What we do know is that it does not emit or absorb light, so we call it Dark Matter. Such a vast amount of aggregated matter does not go unnoticed. In fact we observe that this stuff aggregates and forms massive structures called Dark Matter Halos. Although dark, it warps and bends spacetime such that any light from a background galaxy which passes close to the Dark Matter will have its path altered and changed. This bending causes the galaxy to appear as an ellipse in the sky.     Figure 1: Dark Matter bending the light from a background galaxy. In extreme cases the galaxy here is seen as the two arcs surrounding it. (Credit: NASA, ESA, and Johan Richard (Caltech, USA)) Since there are many galaxies behind a Dark Matter halo, their shapes will correlate with its position.  Figure 2: The effect of Dark Matter on the sky    What\u2019s The Problem? Detecting these Dark Matter halos is hard, but possible using this data. If we can accurately estimate the positions of these halos, we can then understand the function they play in the Universe. There are various methods to attack the problem (we have given you some examples), however we have not been able to reach the level of precision required to understand exactly where this Dark Matter is for all Dark Matter halos. We challenge YOU to detect the most elusive, mysterious and yet most abundant matter in all existence.  Figure 3: Dark Matter in Action. If you look closely at this real world example, you can see the warped and elliptical galaxies. (Credit NASA; ESA; L. Bradley (Johns Hopkins University); R. Bouwens (University of California, Santa Cruz); H. Ford (Johns Hopkins University); and G. Illingworth (University of California, Santa Cruz) Challenge Organisers: David Harvey (Astrophysics PhD Student, Institute for Astronomy, University of Edinburgh), Dr. Tom Kitching (Royal Society Post Doctorial Fellow, Institute for Astronomy, University of Edinburgh)",
        "dataset_text": "Training Data The training data consists of 300 simulated skies similar to the final panel in Figure 2 of the description page. Each sky contains between 300 and 740 galaxies. Each galaxy will have an x and y position ranging from 0 to 4200 (units are pixels), and a measure of ellipticity: e1 and e2 (see An Introduction to Ellipticity).  Training galaxy data is provided in a series of 300 files, one file for each Sky (e.g, Training_Sky27.csv or Training_Sky123.csv). These files have 4 columns: Test Data The test data is in a similar format to the training data. There are 120 simulated skies (see final panel in Figure 2 of the description). Each sky contains 300 to 740 galaxies. Each galaxy will have an x and y position ranging from 0 to 4200, e1 and e2 values (totalling 4 columns per galaxy in the sky). In each sky there are either 1, 2 or 3 dark matter halos. The halo counts in each sky are provided in the file Test_haloCounts.csv.  The challenge is to predict the center of each dark matter halo in each test sky based on the galaxy information provided."
    },
    {
        "name": "Competition name not found",
        "url": "https://www.kaggle.com/competitions/helping-santas-helpers",
        "overview_text": "Overview text not found",
        "description_text": "Description text not found",
        "dataset_text": "Dataset description not found"
    },
    {
        "name": "Crowdflower Search Results Relevance",
        "url": "https://www.kaggle.com/competitions/crowdflower-search-relevance",
        "overview_text": "Overview text not found",
        "description_text": "So many of our favorite daily activities are mediated by proprietary search algorithms. Whether you're trying to find a stream of that reality TV show on cat herding or shopping an eCommerce site for a new set of Japanese sushi knives, the relevance of search results is often responsible for your (un)happiness. Currently, small online businesses have no good way of evaluating the performance of their search algorithms, making it difficult for them to provide an exceptional customer experience. The goal of this competition is to create an open-source model that can be used to measure the relevance of search results. In doing so, you'll be helping enable small business owners to match the experience provided by more resource rich competitors. It will also provide more established businesses a model to test against. Given the queries and resulting product descriptions from leading eCommerce sites, this competition asks you to evaluate the accuracy of their search algorithms. Make a first submission with this Python benchmark on Kaggle scripts.   The dataset for this competition was created using query-result pairings enriched on the CrowdFlower platform. They are sponsoring this competition as an investment in the open-source data science community. A dataset collected, cleaned, and labeled by CrowdFlower can make your supervised machine learning dreams come true.",
        "dataset_text": "See this script for a quick exploration of the data To evaluate search relevancy, CrowdFlower has had their crowd evaluate searches from a handful of eCommerce websites. A total of 261 search terms were generated, and CrowdFlower put together a list of products and their corresponding search terms. Each rater in the crowd was asked to give a product search term a score of 1, 2, 3, 4, with 4 indicating the item completely satisfies the search query, and 1 indicating the item doesn't match the search term.  The challenge in this competition is to predict the relevance score given the product description and product title. To ensure that your algorithm is robust enough to handle any noisy HTML snippets in the wild real world, the data provided in the product description field is raw and contains information that is irrelevant to the product. To discourage hand-labeling the data, CrowdFlower has also provided extra data that was not labeled by the crowd in the test set. This data is ignored when calculating your score. Ready to explore the data? Scripts is the most frictionless way to get familiar with the competition dataset! See the data at a glance here. No download needed to start publishing and forking code in R and Python. It's already pre-loaded with our favorite packages and ready for you to start competing! External data, such as dictionaries, thesaurus, language corpuses, are allowed. However, they must not be directly related to this specific dataset. The source of your external data must be posted to the forum to ensure fairness for all the participants in the community."
    },
    {
        "name": "Avito Context Ad Clicks",
        "url": "https://www.kaggle.com/competitions/avito-context-ad-clicks",
        "overview_text": "Overview text not found",
        "description_text": " In Russia, if you're looking to sell a tractor, a designer dress, a vintage lunchbox, or even a house, your first stop will likely be Avito.ru. As the largest general classified website in Russia, Avito connects buyers and sellers across the world's biggest country. Sellers are highly motivated to place ads on Avito, hoping to gain attention from the site's 70 million unique monthly visitors. There are three different types of ads available to sellers on Avito: regular, highlighted, and context.  Context ads are seen as the best way to target users with goods and services. Currently, Avito uses general statistics on ad performance to drive the placement of context ads. Their existing model ignores individual user behavior, making it difficult to predict which ad will be the most relevant for (and earn the most clicks from) each potential buyer.  In this competition, Avito is challenging you to improve on their model by predicting if individual users will click a given context ad. To create the most robust model and fun competition possible, Avito has provided eight comprehensive relational datasets for you to explore. This competition will help Avito more accurately predict click-through rates for their ads, creating a world where both buyers and sellers win.",
        "dataset_text": "This competition has 8 relational datasets. All these files were encoded in UTF-8 and stored into tab separated format (.tsv). There is also an sqlite database (database.sqlite) alternative with all data available. Relationships between the datasets are captured in the following schema:  These two files are the main datasets related to your predictive models.  trainSearchStream is a random sample of previously selected users' searches on avito.ru during at least 16 consecutive days from April'25 until the target impression. Different types of ads on the site are shown in the picture below:  Regular ads are shifted down constantly as new ads come in. (Normally, a visitor's search results are sorted by the time an ad is submitted to Avito). Each line in the file describes one \"impression\" (an ad that is shown to a particular user based on a search). testSearchStream shares the same format, except the target variable field \"IsClick\" is omitted.  These are samples of users' visits to non-contextual ad landing pages and the corresponding phone request (if one occurred). Each ad's landing page shows the hidden seller's phone number. To be able to contact the seller, the user needs to click the request phone button:  Consequently, a user's phone request event could be considered a proxy for a user's response to the advertisement. We believe that clicking the phone request indicates a high level of interest in the ad. Note that both Params from AdsInfo.tsv and SearchParams from SearchInfo.tsv shares same dictionary (keys and values). The Params are semi-structured to reflect the nature of the search and the product. For example, the Params for clothing might be gender, size, color, brand; while the Params for houses might be size, # of bedrooms, # of bathrooms, etc. The dataset contains a sample of users and their behavior. For each user, one target impression between time point A (May, 12) and time point B (May, 20) was selected randomly. The task of this competition is to provide the probability that a user will click on the selected target ad, given all the information generated by the user from the Start time point (April, 25) until the target impression.  Note that some users from the testSearchStream may not have any historical information in VisitStream, PhoneRequestsStream and trainSearchStream as some of them may be new registered or had no activity within Start - Test event time interval."
    },
    {
        "name": "Homesite Quote Conversion",
        "url": "https://www.kaggle.com/competitions/homesite-quote-conversion",
        "overview_text": "Overview text not found",
        "description_text": "Before asking someone on a date or skydiving, it's\nimportant to know your likelihood of success. The same goes for quoting home insurance prices to a potential customer. Homesite, a leading provider of homeowners insurance, does not currently have a dynamic conversion rate model that can give them confidence a quoted price will lead to a purchase.  Using an anonymized database of information on customer and sales activity, including property and coverage information, Homesite is challenging you to predict which customers will purchase a given quote. Accurately predicting conversion would help Homesite better understand the impact of proposed pricing changes and maintain an ideal portfolio of customer segments. ",
        "dataset_text": "This dataset represents the activity of a large number of customers who are interested in buying policies from Homesite. Each QuoteNumber corresponds to a potential customer and the QuoteConversion_Flag indicates whether the customer purchased a policy. The provided features are anonymized and provide a rich representation of the prospective customer and policy. They include specific coverage information, sales information, personal information, property information, and geographic information. Your task is to predict QuoteConversion_Flag for each QuoteNumber in the test set."
    },
    {
        "name": "Santa's Stolen Sleigh",
        "url": "https://www.kaggle.com/competitions/santas-stolen-sleigh",
        "overview_text": "Overview text not found",
        "description_text": "Fork this script and get started on the problem The North Pole is in an uproar over news that Santa's magic sleigh has been stolen. Able to carry all the world's presents in one trip, it was considered crucial to successfully delivering holiday goodies across the globe in one night. Unwilling to cancel Christmas, Santa is determined to deliver toys to all the good girls and boys using his day-to-day, magic-less sleigh. With so little time to pull off this plan, Santa is once again counting on Kagglers to help. Given the sleigh's antiquated, weight-limited specifications, your challenge is to optimize the routes and loads Santa will take to and from the North Pole. And don't forget about Dasher, Dancer, Prancer, and Vixen; Santa is adamant that the best solutions will minimize the toll of this hectic night on his reindeer friends.  This competition is brought to you by FICO.",
        "dataset_text": "See on user's exploration of the data on Kaggle Scripts For this competition, you are asked to optimize the total weighted distance traveled (weighted reindeer weariness). You are given a list of gifts with their destinations and their weights. You will plan sleigh trips to deliver all the gifts to their destinations while optimizing the routes.  "
    },
    {
        "name": "Avito Duplicate Ads Detection",
        "url": "https://www.kaggle.com/competitions/avito-duplicate-ads-detection",
        "overview_text": "Overview text not found",
        "description_text": "Online marketplaces make it a breeze for users to both find and buy unique treasures or unload their dusty record collections in the spirit of spring cleaning. As one of the world's largest and fastest growing online classifieds, Avito hosts high volumes of listings and competitive sellers often go to great lengths to get their wares noticed.   For some sellers, this means posting the same ad several times with slightly altered text or photos taken from different angles. To ensure that buyers can easily find what they're looking for without sifting through dozens of deceptively identical ads, Avito is asking Kagglers to develop a model that can automatically spot duplicate ads. With more accurate duplicate ad detection, Avito will make it much easier for buyers to find and make their next purchase with an honest seller.",
        "dataset_text": "In this competition, you will predict whether pairs of ads are duplicates. The data is captured in the following schema:  To ensure that winning models are robust enough to generalize to new duplicate cases, the train and test datasets are sampled from different time intervals. Hence, you may see different distributions of duplicates in train/test datasets. "
    },
    {
        "name": "Melbourne University AES/MathWorks/NIH Seizure Prediction",
        "url": "https://www.kaggle.com/competitions/melbourne-university-seizure-prediction",
        "overview_text": "Overview text not found",
        "description_text": "Epilepsy afflicts nearly 1% of the world's population, and is characterized by the occurrence of spontaneous seizures. For many patients, anticonvulsant medications can be given at sufficiently high doses to prevent seizures, but patients frequently suffer side effects. For 20-40% of patients with epilepsy, medications are not effective. Even after surgical removal of epilepsy, many patients continue to experience spontaneous seizures. Despite the fact that seizures occur infrequently, patients with epilepsy experience persistent anxiety due to the possibility of a seizure occurring. Seizure forecasting systems have the potential to help patients with epilepsy lead more normal lives. In order for electrical brain activity (EEG) based seizure forecasting systems to work effectively, computational algorithms must reliably identify periods of increased probability of seizure occurrence. If these seizure-permissive brain states can be identified, devices designed to warn patients of impeding seizures would be possible. Patients could avoid potentially dangerous activities like driving or swimming, and medications could be administered only when needed to prevent impending seizures, reducing overall side effects.  Transitioning from the Kaggle contests held on seizure detection and seizure prediction in 2014 that primarily involved long-term electrical brain activity recordings from dogs, the current contest focuses on seizure prediction using long-term electrical brain activity recordings from humans obtained from the world-first clinical trial of the implantable NeuroVista Seizure Advisory System. Human brain activity was recorded in the form of intracranial EEG (iEEG), which involves electrodes positioned on the surface of the cerebral cortex and the recording of electrical signals with an ambulatory monitoring system. These are long duration recordings, spanning multiple months up to multiple years and recording large numbers of seizures in some humans. The challenge is to distinguish between ten minute long data clips covering an hour prior to a seizure, and ten minute iEEG clips of interictal activity.  This competition is sponsored by MathWorks, the National Institutes of Health (NINDS), the American Epilepsy Society and the University of Melbourne, and organised in partnership with the Alliance for Epilepsy Research, the University of Pennsylvania and the Mayo Clinic.      ",
        "dataset_text": "Jan. 6, 2017 Update - The competition data has been removed and scoring disabled at the request of the competition hosts. Nov. 4, 2016 Update - A new test set was released for this competition. Please see this thread for details. There is emerging evidence that the temporal dynamics of brain activity can be classified into 4 states: Interictal (between seizures, or baseline), Preictal (prior to seizure), Ictal (seizure), and Post-ictal (after seizures). Seizure forecasting requires the ability to reliably identify a preictal state that can be differentiated from the interictal, ictal, and postictal state. The primary challenge in seizure forecasting is differentiating between the preictal and interictal states. The goal of the competition is to demonstrate the existence and accurate classification of the preictal brain state in humans with epilepsy. Human brain activity was recorded in the form of intracranial EEG (iEEG) which involves electrodes positioned on the surface of the cerebral cortex and the recording of electrical signals with an ambulatory monitoring system. iEEG was sampled from 16 electrodes at 400 Hz, and recorded voltages were referenced to the electrode group average. These are long duration recordings, spanning multiple months up to multiple years and recording large numbers of seizures in some humans. The challenge is to distinguish between ten minute long data clips covering an hour prior to a seizure, and ten minute iEEG clips of interictal activity. Seizures are known to cluster, or occur in groups. Patients who typically have seizure clusters receive little benefit from forecasting follow-on seizures. For this contest only lead seizures, defined here as seizures occurring four hours or more after another seizure, are included in the training and testing data sets. In order to avoid any potential contamination between interictal, preictal, and post-ictal EEG signals, interictal data segments were restricted to be at least four hours before or after any seizure. Interictal data segments were chosen at random within these restrictions. Intracranial EEG (iEEG) data clips are organized in folders containing training and testing data for each human patient. The training data is organized into ten minute EEG clips labeled \"Preictal\" for pre-seizure data segments, or \"Interictal\" for non-seizure data segments. Training data segments are numbered sequentially, while testing data are in random order. Within folders data segments are stored in .mat files as follows: Each .mat file contains a data structure, dataStruct, with fields as follows: The .mat data file format was created by MathWorks for use with MATLAB. Documentation for this format is found here. To read these files you can use MATLAB, either your own version or through the complimentary MATLAB license for the competition offered by MathWorks. A sample solution is available here. There are also solutions online using Python, R, C, Fortran and other languages. Preictal training and testing data segments are provided covering one hour prior to seizure with a five minute seizure horizon. (i.e. from 1:05 to 0:05 before seizure onset.) This pre-seizure horizon ensures that 1) seizures could be predicted with enough warning to allow administration of fast-acting medications, and 2) any seizure activity before the annotated onset that may have been missed by the epileptologist will not affect the outcome of the competition.  Similarly, one hour sequences of interictal ten minute data segments are provided. The interictal data were chosen randomly from the full data record, with the restriction that interictal segments be at least 4 hours away from any seizure, to avoid contamination with preictal or postictal signals. Any part of any 10-minute data segment can potentially contain \u201cdata drop-out\u201d where the intracranial brain implant has temporarily failed to record data. This data drop-out corresponds to iEEG signal values of zeros across all channels at a given time sample. Data drop-out provides no predictive information as to whether a given 10-minute segment is preictal or interictal. A handful of 10-minute segments contain 100% data drop-out and cannot be classified. The data may also contain artifacts such as large amplitude rapid signal transitions that can be removed from analysis. As indicated in the Competition Rules, the contest data may only be used for the purposes of this Competition. All other uses, including education, academic, research, commercial and non-commercial uses, are prohibited. Participants are invited to visit the NIH-sponsored International Epilepsy Electrophysiology portal (http://ieeg.org) to review and download annotated interictal and preictal data from other patients and animal subjects. Using ieeg.org data for additional algorithm training is permitted."
    },
    {
        "name": "IEEE-CIS Fraud Detection",
        "url": "https://www.kaggle.com/competitions/ieee-fraud-detection",
        "overview_text": "Overview text not found",
        "description_text": "Imagine standing at the check-out counter at the grocery store with a long line behind you and the cashier not-so-quietly announces that your card has been declined. In this moment, you probably aren\u2019t thinking about the data science that determined your fate. Embarrassed, and certain you have the funds to cover everything needed for an epic nacho party for 50 of your closest friends, you try your card again. Same result. As you step aside and allow the cashier to tend to the next customer, you receive a text message from your bank. \u201cPress 1 if you really tried to spend $500 on cheddar cheese.\u201d While perhaps cumbersome (and often embarrassing) in the moment, this fraud prevention system is actually saving consumers millions of dollars per year. Researchers from the IEEE Computational Intelligence Society (IEEE-CIS) want to improve this figure, while also improving the customer experience. With higher accuracy fraud detection, you can get on with your chips without the hassle. IEEE-CIS works across a variety of AI and machine learning areas, including deep neural networks, fuzzy systems, evolutionary computation, and swarm intelligence. Today they\u2019re partnering with the world\u2019s leading payment service company, Vesta Corporation, seeking the best solutions for fraud prevention industry, and now you are invited to join the challenge. In this competition, you\u2019ll benchmark machine learning models on a challenging large-scale dataset. The data comes from Vesta's real-world e-commerce transactions and contains a wide range of features from device type to product features. You also have the opportunity to create new features to improve your results. If successful, you\u2019ll improve the efficacy of fraudulent transaction alerts for millions of people around the world, helping hundreds of thousands of businesses reduce their fraud loss and increase their revenue. And of course, you will save party people just like you the hassle of false positives. Acknowledgements:  Vesta Corporation provided the dataset for this competition. Vesta Corporation is the forerunner in guaranteed e-commerce payment solutions. Founded in 1995, Vesta pioneered the process of fully guaranteed card-not-present (CNP) payment transactions for the telecommunications industry. Since then, Vesta has firmly expanded data science and machine learning capabilities across the globe and solidified its position as the leader in guaranteed ecommerce payments. Today, Vesta guarantees more than $18B in transactions annually. Header Photo by Tim Evans on Unsplash",
        "dataset_text": "In this competition you are predicting the probability that an online transaction is fraudulent, as denoted by the binary target isFraud. The data is broken into two files identity and transaction, which are joined by TransactionID. Not all transactions have corresponding identity information. The TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp). You can read more about the data from this post by the competition host."
    },
    {
        "name": "Abstraction and Reasoning Challenge",
        "url": "https://www.kaggle.com/competitions/abstraction-and-reasoning-challenge",
        "overview_text": "Overview text not found",
        "description_text": " Can a computer learn complex, abstract tasks from just a few examples? Current machine learning techniques are data-hungry and brittle\u2014they can only make sense of patterns they've seen before. Using current methods, an algorithm can gain new skills by exposure to large amounts of data, but cognitive abilities that could broadly generalize to many tasks remain elusive. This makes it very challenging to create systems that can handle the variability and unpredictability of the real world, such as domestic robots or self-driving cars. However, alternative approaches, like inductive programming, offer the potential for more human-like abstraction and reasoning. The Abstraction and Reasoning Corpus (ARC) provides a benchmark to measure AI skill-acquisition on unknown tasks, with the constraint that only a handful of demonstrations are shown to learn a complex task. It provides a glimpse of a future where AI could quickly learn to solve new problems on its own. The Kaggle Abstraction and Reasoning Challenge invites you to try your hand at bringing this future into the present! This competition is hosted by Fran\u00e7ois Chollet, creator of the Keras neural networks library. Chollet\u2019s paper on measuring intelligence provides the context and motivation behind the ARC benchmark. In this competition, you\u2019ll create an AI that can solve reasoning tasks it has never seen before. Each ARC task contains 3-5 pairs of train inputs and outputs, and a test input for which you need to predict the corresponding output with the pattern learned from the train examples. If successful, you\u2019ll help bring computers closer to human cognition and you'll open the door to completely new AI applications!",
        "dataset_text": "The objective of this competition is to create an algorithm that is capable of solving abstract reasoning tasks. The format is very different than previous competition, so please read this information carefully, and refer to supplementary documentation as needed. When looking at a task, a \"test-taker\" has access to inputs and outputs of the demonstration pairs (train pairs), plus the input(s) of the test pair(s). The goal is to construct the output grid(s) corresponding to the test input grid(s), using 3 trials for each test input. \"Constructing the output grid\" involves picking the height and width of the output grid, then filling each cell in the grid with a symbol (integer between 0 and 9, which are visualized as colors). Only exact solutions (all cells match the expected answer) can be said to be correct. A additional information, as well as an interactive app to explore the objective of this competition is found at the Abstraction and Reasoning Corpus github page. It is highly recommended that you download and explore the interactive app, as the best way to understand the objective of the competition. The task files are stored in two directories: The tasks are stored in JSON format. Each task JSON file contains a dictionary with two fields: A \"pair\" is a dictionary with two fields: A \"grid\" is a rectangular matrix (list of lists) of integers between 0 and 9 (inclusive). The smallest possible grid size is 1x1 and the largest is 30x30. NOTE: For this competition, we are keeping the language of the ARC dataset, but that creates some ambiguity. The training and evaluation tasks both contain train and test pairs of inputs and outputs. The leaderboard will be scored using 100 unseen test tasks. The test tasks will contain train pairs of inputs and outputs, but you are only given the tasks test input. Your algorithm must predict the test output. For the 100 tasks used for leaderboard score, most only have one test input that will require a corresponding output prediction, although for a small number of tasks, you will be asked to make predictions for two test inputs. The output_id shown in the sample_submission.csv denotes the task id as will as the test input id."
    },
    {
        "name": "Google AI Open Images - Visual Relationship Track",
        "url": "https://www.kaggle.com/competitions/google-ai-open-images-visual-relationship-track",
        "overview_text": "Overview text not found",
        "description_text": "Google AI (Google\u2019s AI research arm, tasked with advancing AI for everyone) is challenging you to build an algorithm that detects objects automatically using an absolutely massive training dataset \u2015 one with more varied and complex bounding-box annotations and object classes than ever before. Here's the background. Computers are getting better and better at vision. But in a few critical ways, they still can't match a human\u2019s intuitive perception. For example, what do you see when you look at this photo? Most of us would answer, \u201ca sandy beach, the ocean, a few people walking, some trees, grass, and buildings\u2026a woman walking her dog right there! Oh yeah, and there is a man holding a plastic cup.\u201d Can a computer provide as precise an image description? Google AI wants to further push the capabilities of computer vision. We hope that providing very large training set will stimulate research into more sophisticated object and relationship detection models that will exceed current state-of-the-art performance. The results of this Challenge will be presented at a workshop at the European Conference on Computer Vision 2018. In this track of the Challenge, you are asked to build the best performing algorithm for automatically detecting relationships triplets. Please refer to the Open Images Challenge page for additional details on the dataset. This competition is one of two tracks in the Open Images Challenge. Find the Object Detection track of this competition using the entire training set here.",
        "dataset_text": "The train and validation sets of images and their ground truth (visual relationships annotations, bounding boxes and labels) can be downloaded via Open Images Challenge website . Please note that the test images used in this competition is independent from previously released part of Open Images V4. The images can be downloaded from: Note: The images are the same as in the Object Detection Track so you do not need to re-download them. You should expect 99,999 images."
    },
    {
        "name": "Predict Closed Questions on Stack Overflow",
        "url": "https://www.kaggle.com/competitions/pycon-2015-tutorial-predict-closed-questions-on-stack-overflow",
        "overview_text": "Overview text not found",
        "description_text": "This competition is now complete. Congratulations to the winners! Millions of programmers use Stack Overflow to get high quality answers to their programming questions every day.  We take quality very seriously, and have evolved an effective culture of moderation to safe-guard it. With more than six thousand new questions asked on Stack Overflow every weekday we're looking to add more sophisticated software solutions to our moderation toolbox. Closing Questions Currently about 6% of all new questions end up \"closed\".  Questions can be closed as off topic, not constructive, not a real question, or too localized.  More in depth descriptions of each reason can be found in the Stack Overflow FAQ.  The exact duplicate close reason has been excluded from this contest, since it depends on previous questions. Your goal is to build a classifier that predicts whether or not a question will be closed given the question as submitted, along with the reason that the question was closed.  Additional data about the user at question creation time is also available.",
        "dataset_text": "The code for the benchmarks is on Github. The training data contains data through July 31st UTC, and the public leaderboard data goes from August 1 UTC to August 14 UTC. The train.csv file contains post text and associated metadata at the time of post creation which will serve as inputs to your solution.  The state of the post as of July 31st is also included. It contains the following fields (not in this order): The public leaderboard data contains all of the above fields, except for the target field OpenStatus and PostClosedDate. The file train-sample.csv is a stratified sample of the training data: it contains every closed question and an equally-sized random sample of the open questions in the training data. All questions will have a value in Tag1, but Tags 2 through 5 are optional. To convert the user submitted Markdown found in BodyMarkdown to HTML if desired, our open source implementations in C# and Javascript may be useful. Additional data can be found in \"2012-07 Stack Overflow.7z\", which contains an entire public data dump of Stack Overflow.  Descriptions of the values can be found in the archive itself as well as on Meta Stack Overflow.  This data will not be available as inputs, but may be useful in building your solution.  As it is rather large (6GB) you may find it easier to download as a torrent, more details can be found in this forum post."
    },
    {
        "name": "LearnPlatform COVID-19 Impact on Digital Learning",
        "url": "https://www.kaggle.com/competitions/learnplatform-covid19-impact-on-digital-learning",
        "overview_text": "Overview text not found",
        "description_text": "Nelson Mandela believed education was the most powerful weapon to change the world. But not every student has equal opportunities to learn. Effective policies and plans need to be enacted in order to make education more equitable\u2014and perhaps your innovative data analysis will help reveal the solution. Current research shows educational outcomes are far from equitable. The imbalance was exacerbated by the COVID-19 pandemic. There's an urgent need to better understand and measure the scope and impact of the pandemic on these inequities. Education technology company LearnPlatform was founded in 2014 with a mission to expand equitable access to education technology for all students and teachers. LearnPlatform\u2019s comprehensive edtech effectiveness system is used by districts and states to continuously improve the safety, equity, and effectiveness of their educational technology. LearnPlatform does so by generating an evidence basis for what\u2019s working and enacting it to benefit students, teachers, and budgets. In this analytics competition, you\u2019ll work to uncover trends in digital learning. Accomplish this with data analysis about how engagement with digital learning relates to factors like district demographics, broadband access, and state/national level policies and events. Then, submit a Kaggle Notebook to propose your best solution to these educational inequities. Your submissions will inform policies and practices that close the digital divide. With a better understanding of digital learning trends, you may help reverse the long-term learning loss among America\u2019s most vulnerable, making education more equitable. The COVID-19 Pandemic has disrupted learning for more than 56 million students in the United States. In the Spring of 2020, most states and local governments across the U.S. closed educational institutions to stop the spread of the virus. In response, schools and teachers have attempted to reach students remotely through distance learning tools and digital platforms. Until today, concerns of the exacaberting digital divide and long-term learning loss among America\u2019s most vulnerable learners continue to grow. We challenge the Kaggle community to explore (1) the state of digital learning in 2020 and (2) how the engagement of digital learning relates to factors such as district demographics, broadband access, and state/national level policies and events. We encourage you to guide the analysis with questions that are related to the themes that are described above (in bold font). Below are some examples of questions that relate to our problem statement:",
        "dataset_text": "We have provided a set of daily edtech engagement data from over 200 school districts in 2020, and we encourage you to leverage other publicly available data sources in your analysis. We include three basic sets of files to help you get started: In addition to the files provided, we encourage you to use other public data sources such as COVID-19 US State Policy database, KIDS Count, and KFF."
    },
    {
        "name": "The Big Data Combine Engineered by BattleFin",
        "url": "https://www.kaggle.com/competitions/battlefin-s-big-data-combine-forecasting-challenge",
        "overview_text": "Overview text not found",
        "description_text": " The Big Data Combine engineered by BattleFin are rapid fire, live tryouts for computer scientists with elite predictive analytic skills intent on monetizing their models. The first stage of the competition is a predictive modeling competition that requires participants to develop a model that predicts stock price movements using sentiment data provided by RavenPack. Traders, analysts and investors are always looking for techniques to better predict price movements.  Knowing whether a security will increase or decrease allows traders to make better investment decisions and manage risk more effectively.  This competition is designed to identify people with the talent to create a predictive model using financial data. Competitors are given intraday trading data showing stock price movements at 5 minute intervals and asked to predict the change two hours in the future. The winners of the predictive modeling phase are invited to the \"live\" Big Data Combine tryouts in Miami, FL. Up to 12 finalists will be selected to compete in the live event in Miami. The lucky few will pitch their predictive model to expert judges and an engaged audience. They have only three minutes to present in non-technical terms three items: personal background, predictive model description, and how they would use there model to make money in finance. If their model and presentation impresses our judges, they will be eligible to work with BattleFin and Deltix to convert their predictive model into a trading strategy. Matt Iseman - Actor, Comedian, Host of American Ninja Warrior Mr. Iseman has hosted the game shows Scream Play on E! and Casino Night on the GSN. He currently appears as a regular cast member on the home makeover show Clean House, and its companion outtakes show, Clean House Comes Clean, both on the Style Network. Additionally, he hosted season 2 and 3 of American Ninja Warrior on the channel G4. He also has worked episodically in television shows including The Drew Carey Show, NCIS, and General Hospital. He has appeared on the syndicated MAD TV, Comedy Central\u2019s Premium Blend, Fox\u2019s The Best Damn Sports Show Period, and Fox News Channel\u2019s Red Eye w/ Greg Gutfeld. Mr. Iseman was also the host of Sports Soup, a spin-off of E!'s The Soup, on Versus. Style Network and Versus are owned by Comcast. Iseman began working with American Ninja Warrior (G4) in 2010. He uses his great athleticism and work as a comedian to add his style to the show with Johnny Moseley (American Professional Freestyle Skier), and Angela Sun (Sideline Correspondent). Ilya Gorelik, CEO of Deltix Inc. CEO, Ilya Gorelik is responsible for setting the strategic direction of the company, as well as overseeing global product development, sales and marketing.Ilya has more than 15 years of experience managing large-scale software projects and teams. He was one of the key development leaders of PTC, where he worked from 1989 to 1998, attaining the position of Senior VP of Engineering and Chief Technology Officer. From 1998 until 2000, Ilya was Senior VP of Product Strategy and Development and Chief Scientist for FirePond. From 2000 until founding Deltix in 2005, Ilya worked as Advisory CTO for HighRoads and several other software technology companies. Ilya has a Ph.D. in ComputationalMechanics from Moscow Technical University, he received an MS in Mechanical Engineering from Minsk Technical University. Peter Hafez, Head Quantitative Research at RavenPack Peter is an award-winning expert in the field of applied news analytics and has consulted numerous leading trading and investment firms on how to take advantage of news analytics in financial markets. Peter has more than 10 years of experience in quantitative finance with companies such as Standard & Poor's, Credit Suisse First Boston, and Saxo Bank. He is a recognized speaker at conferences on behavioral finance and algorithmic trading. Peter holds a Master's degree in Quantitative Finance from City University's Cass Business School along with an undergraduate degree in Economics from Copenhagen University. Nabyl Charania, Managing Director at Rokk3r Labs Nabyl is a Co-Founder and Managing Director at Rokk3r Labs.  Rokk3r Labs is Miami based Venture Capital firm with 35 employees.  Rokk3r Labs fuses entrepreneurial and professional talent to help entrepreneurs create ideas, prototypes, products and generally invests in disruptive companies designed for the modern hyper-connected world. Prior, he was a Founder and Managing Director at Decipher Labs Inc. He graduated from University of Waterloo in 2000. Zeid Barakat, Co-Founder at Flyberry Capital LLC  Zeid is a Co-Founder of Flyberry Capital a Boston based hedge fund that utilizes a Big Data strategy. Zeid works with \u2018best-in class\u2019 MIT & Harvard computer scientists and machine learning experts to develop proprietary trading strategies, focused on event-based arbitrage. In charge of dfining corporate growth strategy,business development, marketing, managing Board of Advisers, and fundraising. He is an Entrepreneur in high-tech companies, focused on novel approaches to biotechnology, healthcare and financial services. He earned MBA degree in General Management and Entrepreneurship from MIT in 2008. BattleFin is a tournament platform that crowdsources the world's best investment talent. The firm uses rapid fire, real capital tournaments to democratically identify up and coming investment talent. BattleFin has recently been featured in Bloomberg Business Week in an article titles \"The Hedge Fund Hunger Games\". The firm is passionate about leveling the playing field in finance so that anyone with an internet connection can participate in its tournaments. The firm specializes in finding the hedge fund managers of tomorrow. To learn more about BattleFin visit BattleFin.com. Founded in 2005 and with more than 50 staff, Deltix has established itself as a leader in the growing domain of software and services for quantitative research and systematic automated trading. The Deltix Product Suite is an end-to-end platform for all phases of the alpha discovery and trading life-cycle; including data collection and aggregation, model development, back-testing, optimization, simulation, and deployment to production trading. Financial firms are overloaded with information and have turned to computers to read news and other media. What would take days for an investment professional to read and interpret takes computers only a few milliseconds. Now financial institutions can react much faster to the ever increasing amounts of news and information available for making investment decisions. Powered by a proprietary text analysis platform, RavenPack the tournament data sponsor, analyzes novel and relevant stories published by major news sources to look for key scheduled and unexpected geopolitical, macro-economic and corporate events, topics and opinions that indicate changes in market sentiment. Sampled news sources represent the most reliable and authoritative publishers of business and financial news.RavenPack continuously analyzes relevant information from Dow Jones Newswires, regional editions for the Wall Street Journal, and Barron\u2019s to produce real time news sentiment scores and events from entities across multiple asset classes, including currencies, commodities, organizations, companies, sectors, and industries.",
        "dataset_text": "For this competition, you are asked to predict the percentage change in a financial instrument at a time 2 hours in the future.  The data represents features of various financial securities (198 in total) recorded at 5-minute intervals throughout a trading day.  To discourage cheating, you are not provided with the features' names or the specific dates. data.zip - contains features for 510 days worth of trading, including 200 training days and 310 testing days\ntrainLabels.csv - contains the targets for the 200 training days\nsampleSubmission.csv - shows the submission format Each variable named O1, O2, O3, etc. (the outputs) represents a percent change in the value of a security.  Each variable named I1, I2, I3, etc. (the inputs) represents a feature. The underlying securities and features represented by these anonymized names are the same across all files (e.g. O1 will always be the same stock). Within each trading day, you are provided the outputs as a relative percentage compared to the previous day's closing price.  The first line of each data file represents the previous close. For example, if a security closed at $1 the previous day and opened at $2 the next day, the first output would be 0, then 100.  All output values are computed relative to the previous day's close. The timestamps within each file are as follows (ignoring the header row): Line 1 = Outputs and inputs at previous day's close (4PM ET)\nLine 2 = Outputs and inputs at current day's open (9:30AM ET)\nLine 3 = Outputs and inputs at 9:35AM ET\n...\nLine 55 = Outputs and inputs at 1:55PM ET You are asked to predict the outputs 2 hours later, at 4PM ET."
    },
    {
        "name": "Cassava Leaf Disease Classification",
        "url": "https://www.kaggle.com/competitions/cassava-leaf-disease-classification",
        "overview_text": "Overview text not found",
        "description_text": "As the second-largest provider of carbohydrates in Africa, cassava is a key food security crop grown by smallholder farmers because it can withstand harsh conditions. At least 80% of household farms in Sub-Saharan Africa grow this starchy root, but viral diseases are major sources of poor yields. With the help of data science, it may be possible to identify common diseases so they can be treated. Existing methods of disease detection require farmers to solicit the help of government-funded agricultural experts to visually inspect and diagnose the plants. This suffers from being labor-intensive, low-supply and costly. As an added challenge, effective solutions for farmers must perform well under significant constraints, since African farmers may only have access to mobile-quality cameras with low-bandwidth. In this competition, we introduce a dataset of 21,367 labeled images collected during a regular survey in Uganda. Most images were crowdsourced from farmers taking photos of their gardens, and annotated by experts at the National Crops Resources Research Institute (NaCRRI) in collaboration with the AI lab at Makerere University, Kampala. This is in a format that most realistically represents what farmers would need to diagnose in real life. Your task is to classify each cassava image into four disease categories or a fifth category indicating a healthy leaf. With your help, farmers may be able to quickly identify diseased plants, potentially saving their crops before they inflict irreparable damage. The Makerere Artificial Intelligence (AI) Lab is an AI and Data Science research group based at Makerere University in Uganda. The lab specializes in the application of artificial intelligence and data science - including for example, methods from machine learning, computer vision and predictive analytics to problems in the developing world. Their mission is: \u201cTo advance Artificial Intelligence research to solve real-world challenges.\" We thank the different experts and collaborators from National Crops Resources Research Institute (NaCRRI) for assisting in preparing this dataset. ",
        "dataset_text": "Can you identify a problem with a cassava plant using a photo from a relatively inexpensive camera? This competition will challenge you to distinguish between several diseases that cause material harm to the food supply of many African countries. In some cases the main remedy is to burn the infected plants to prevent further spread, which can make a rapid automated turnaround quite useful to the farmers. [train/test]_images the image files. The full set of test images will only be available to your notebook when it is submitted for scoring. Expect to see roughly 15,000 images in the test set. train.csv sample_submission.csv A properly formatted sample submission, given the disclosed test set content. [train/test]_tfrecords the image files in tfrecord format. label_num_to_disease_map.json The mapping between each disease code and the real disease name."
    },
    {
        "name": "Benchmark Bond Trade Price Challenge",
        "url": "https://www.kaggle.com/competitions/benchmark-bond-trade-price-challenge",
        "overview_text": "Overview text not found",
        "description_text": "The Benchmark Bond Trade Price Challenge is a competition to predict the next price that a US corporate bond might trade at. Contestants are given information on the bond including current coupon, time to maturity and a reference price computed by Benchmark Solutions.  Details of the previous 10 trades are also provided.  ",
        "dataset_text": "NOTE: The compressed files contain .csv versions of the training and test data.  The .mat files are provided for MATLAB users as a convenience. US corporate bond trade data is provided.  Each row includes trade details, some basic information about the traded bond, and information about the previous 10 trades.  Contestants are asked to predict trade price. Column details: We have posted code using R's random forest package to create a benchmark. To handle missing values in some columns, the R code creates indicator variables for missing/non-missing and replaces the missing values with a number."
    },
    {
        "name": "Galaxy Zoo - The Galaxy Challenge",
        "url": "https://www.kaggle.com/competitions/galaxy-zoo-the-galaxy-challenge",
        "overview_text": "Overview text not found",
        "description_text": "Understanding how and why we are here is one of the fundamental questions for the human race. Part of the answer to this question lies in the origins of galaxies, such as our own Milky Way. Yet questions remain about how the Milky Way (or any of the other ~100 billion galaxies in our Universe) was formed and has evolved. Galaxies come in all shapes, sizes and colors: from beautiful spirals to huge ellipticals. Understanding the distribution, location and types of galaxies as a function of shape, size, and color are critical pieces for solving this puzzle.  The Whirlpool Galaxy (M51). Credit: NASA and European Space Agency With each passing day telescopes around and above the Earth capture more and more images of distant galaxies. As better and bigger telescopes continue to collect these images, the datasets begin to explode in size. In order to better understand how the different shapes (or morphologies) of galaxies relate to the physics that create them, such images need to be sorted and classified. Kaggle has teamed up with Galaxy Zoo and Winton Capital to produce the Galaxy Challenge, where participants will help classify galaxies into categories.   Image Credit: ESA/Hubble & NASA Galaxies in this set have already been classified once through the help of hundreds of thousands of volunteers, who collectively classified the shapes of these images by eye in a successful citizen science crowdsourcing project. However, this approach becomes less feasible as data sets grow to contain of hundreds of millions (or even billions) of galaxies. That's where you come in. This competition asks you to analyze the JPG images of galaxies to find automated metrics that reproduce the probability distributions derived from human classifications. For each galaxy, determine the probability that it belongs in a particular class. Can you write an algorithm that behaves as well as the crowd does? Contributors: D. Harvey, C. Lintott, T. Kitching, P. Marshall, K. Willett, Galaxy Zoo  The Contributors and the rest of the Galaxy Zoo and Kaggle teams would like to say a big thank you to Winton Capital for helping make this happen. Without their support, we would have not been able to make this competition go ahead.",
        "dataset_text": "The first column in each solution is labeled GalaxyID; this is a randomly-generated ID that only allows you to match the probability distributions with the images. The next 37 columns are all floating point numbers between 0 and 1 inclusive. These represent the morphology (or shape) of the galaxy in 37 different categories as identified by crowdsourced volunteer classifications as part of the Galaxy Zoo 2 project. These morphologies are related to probabilities for each category; a high number (close to 1) indicates that many users identified this morphology category for the galaxy with a high level of confidence. Low numbers for a category (close to 0) indicate the feature is likely not present.  Visit the Galaxy Zoo Decision Tree page for a detailed description of the data."
    },
    {
        "name": "Display Advertising Challenge",
        "url": "https://www.kaggle.com/competitions/criteo-display-ad-challenge",
        "overview_text": "Overview text not found",
        "description_text": "Display advertising is a billion dollar effort and one of the central uses of machine learning on the Internet. However, its data and methods are usually kept under lock and key. In this research competition, CriteoLabs is sharing a week\u2019s worth of data for you to develop models predicting ad click-through rate (CTR). Given a user and the page he is visiting, what is the probability that he will click on a given ad?  The goal of this challenge is to benchmark the most accurate ML algorithms for CTR estimation. All winning models will be released under an open source license. As a participant, you are given a chance to access the traffic logs from Criteo that include various undisclosed features along with the click labels. ",
        "dataset_text": "The semantic of the features is undisclosed. When a value is missing, the field is empty."
    },
    {
        "name": "Microsoft Malware Classification Challenge (BIG 2015)",
        "url": "https://www.kaggle.com/competitions/malware-classification",
        "overview_text": "Overview text not found",
        "description_text": "In recent years, the malware industry has become a well organized market involving large amounts of money. Well funded, multi-player syndicates invest heavily in technologies and capabilities built to evade traditional protection, requiring anti-malware vendors to develop counter mechanisms for finding and deactivating them. In the meantime, they inflict real financial and emotional pain to users of computer systems.One of the major challenges that anti-malware faces today is the vast amounts of data and files which need to be evaluated for potential malicious intent. For example, Microsoft's real-time detection anti-malware products are present on over 160M computers worldwide and inspect over 700M computers monthly. This generates tens of millions of daily data points to be analyzed as potential malware. One of the main reasons for these high volumes of different files is the fact that, in order to evade detection, malware authors introduce polymorphism to the malicious components. This means that malicious files belonging to the same malware \"family\", with the same forms of malicious behavior, are constantly modified and/or obfuscated using various tactics, such that they look like many different files. In order to be effective in analyzing and classifying such large amounts of files, we need to be able to group them into groups and identify their respective families. In addition, such grouping criteria may be applied to new files encountered on computers in order to detect them as malicious and of a certain family. For this challenge, Microsoft is providing the data science community with an unprecedented malware dataset and encouraging open-source progress on effective techniques for grouping variants of malware files into their respective families. This competition is hosted by WWW 2015 / BIG 2015 and the following Microsoft groups: Microsoft Malware Protection Center, Microsoft Azure Machine Learning and Microsoft Talent Management. Microsoft contacts: Dr. Royi Ronen (royir@microsoft.com) and Corina Feuerstein (corinaf@microsoft.com) ",
        "dataset_text": "Warning: this dataset is almost half a terabyte uncompressed! We have compressed the data using 7zip to achieve the smallest file size possible. Note that the rules do not allow sharing of the data outside of Kaggle, including bit torrent (why not?). You are provided with a set of known malware files representing a mix of 9 different families. Each malware file has an Id, a 20 character hash value uniquely identifying the file, and a Class, an integer representing one of 9 family names to which the malware may belong: For each file, the raw data contains the hexadecimal representation of the file's binary content, without the PE header (to ensure sterility).  You are also provided a metadata manifest, which is a log containing various metadata information extracted from the binary, such as function calls, strings, etc. This was generated using the IDA disassembler tool. Your task is to develop the best mechanism for classifying files in the test set into their respective family affiliations. The dataset contains the following files:"
    },
    {
        "name": "Click-Through Rate Prediction",
        "url": "https://www.kaggle.com/competitions/avazu-ctr-prediction",
        "overview_text": "Overview text not found",
        "description_text": "In online advertising, click-through rate (CTR) is a very important metric for evaluating ad performance. As a result, click prediction systems are essential and widely used for sponsored search and real-time bidding.  For this competition, we have provided 11 days worth of Avazu data to build and test prediction models. Can you find a strategy that beats standard classification algorithms? The winning models from this competition will be released under an open-source license.",
        "dataset_text": "Dataset description not found"
    },
    {
        "name": "Flavours of Physics: Finding \u03c4 \u2192 \u03bc\u03bc\u03bc",
        "url": "https://www.kaggle.com/competitions/flavours-of-physics",
        "overview_text": "Overview text not found",
        "description_text": "Like last year's Higgs Boson Machine Learning Challenge, this competition deals with the  physics at the Large Hadron Collider (LHC). However, the subject of last year's challenge, the Higgs Boson, was already known to exist. The aim of this year's challenge is to find a phenomenon that is not already known to exist \u2013 charged lepton flavour violation \u2013 thereby helping to establish \"new physics\".  The laws of nature ensure that some physical quantities, such as energy or momentum, are conserved. From Noether\u2019s theorem, we know that each conservation law is associated with a fundamental symmetry. For example, conservation of energy is due to the time-invariance (the outcome of an experiment would be the same today or tomorrow) of physical systems. The fact that physical systems behave the same, regardless of where they are located or how they are oriented, gives rise to the conservation of linear and angular momentum. Symmetries are also crucial to the structure of the Standard Model of particle physics, our present theory of interactions at microscopic scales. Some are built into the model, while others appear accidentally from it. In the Standard Model, lepton flavour, the number of electrons and electron-neutrinos, muons and muon-neutrinos, and tau and tau-neutrinos, is one such conserved quantity. (opens in a new tab)\"> Interestingly, in many proposed extensions to the Standard Model, this symmetry doesn\u2019t exist, implying decays that do not conserve lepton flavour are possible. One decay searched for at the LHC is \u03c4- \u2192 \u03bc+\u03bc-\u03bc- (or \u03c4 \u2192 3\u03bc). Observation of this decay would be a clear indication of the violation of lepton flavour and a sign of long-sought new physics. You will be working with real data from the LHCb experiment at the LHC, mixed with simulated datasets of the decay. The metric used in this challenge includes checks that physicists do in their analysis to make sure the results are unbiased. These checks have been built into the competition design to help ensure that the results will be useful for physicists in future studies.  To get started, review the Data Page, and be sure to download the Starter Kit. The Starter Kit will help you to get used to the unique submission procedure for this competition. You've got lots of questions. Researchers at CERN & LCHb have the answers. - What is the goal of this competition? (1:56)\n- Why is finding \u03c4 \u2192 \u03bc\u03bc\u03bc exciting? (2:18)\n- What are flavours? (4:10)\n- Why use machine learning to find \u03c4 \u2192 \u03bc\u03bc\u03bc? (4:57)\n- How did you decide on the size of the dataset? (5:31)\n- Why is weighted AUC the evaluation metric? (6:09)\n- Why use Ds \u2192 \u03c6\u03c0 data for the Agreement Test? (7:53)\n- Why do we need a Correlation Check? (8:44)\n- How will the competition results impact what you do? (11:38)\n- How will the competition results be used at CERN? (12:17) Flavour of Physics, Research Documentation Roel Aaij et al., Search for the lepton flavour violating decay \u03c4 \u2192 \u00b5\u00b5\u00b5, 2015, JHEP, 1502:121, 2015 New approaches for boosting to uniformity This competition is brought to you by:  (opens in a new tab)\">              (opens in a new tab)\">             (opens in a new tab)\"> (opens in a new tab)\"> Co-sponsored by:  Additional support from:  (opens in a new tab)\">          (opens in a new tab)\">    (opens in a new tab)\">           (opens in a new tab)\">",
        "dataset_text": "In this competition, you are given a list of collision events and their properties. You will then predict whether a \u03c4 \u2192 3\u03bc decay happened in this collision. This \u03c4 \u2192 3\u03bc is currently assumed by scientists not to happen, and the goal of this competition is to discover \u03c4 \u2192 3\u03bc happening more frequently than scientists currently can understand. It is challenging to design a machine learning problem for something you have never observed before. Scientists at CERN developed the following designs to achieve the goal. This is a labelled dataset (the label \u2018signal\u2019 being \u20181\u2019 for signal events, \u20180\u2019 for background events) to train the classifier. Signal events have been simulated, while background events are real data. This real data is collected by the LHCb detectors observing collisions of accelerated particles with a specific mass range in which \u03c4 \u2192 3\u03bc can\u2019t happen. We call these events \u201cbackground\u201d and label them 0. The test dataset has all the columns that training.csv has, except mass, production, min_ANNmuon, and signal.  The test dataset consists of a few parts: You need to submit predictions for ALL the test entries. You will need to treat them all the same and predict as if they are all the same channel's collision events.  A submission is only scored after passing both the agreement test and the correlation test.  This dataset contains simulated and real events from the Control channel Ds \u2192 \u03c6\u03c0 to evaluate your simulated-real data of submission agreement locally. It contains the same columns as test.csv and weight column. For more details see agreement test. This dataset contains only real background events recorded at LHCb to evaluate your submission correlation with mass locally. It contains the same columns as test.csv and mass column to check correlation with. For more details see correlation test."
    },
    {
        "name": "Tweet Sentiment Extraction",
        "url": "https://www.kaggle.com/competitions/tweet-sentiment-extraction",
        "overview_text": "Overview text not found",
        "description_text": "\"My ridiculous dog is amazing.\" [sentiment: positive] With all of the tweets circulating every second it is hard to tell whether the sentiment behind a specific tweet will impact a company, or a person's, brand for being viral (positive), or devastate profit because it strikes a negative tone. Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds. But, which words actually lead to the sentiment description? In this competition you will need to pick out the part of the tweet (word or phrase) that reflects the sentiment. Help build your skills in this important area with this broad dataset of tweets. Work on your technique to grab a top spot in this competition. What words in tweets support a positive, negative, or neutral sentiment? How can you help make that determination using machine learning tools? In this competition we've extracted support phrases from Figure Eight's Data for Everyone platform. The dataset is titled Sentiment Analysis: Emotion in Text tweets with existing sentiment labels, used here under creative commons attribution 4.0. international licence. Your objective in this competition is to construct a model that can do the same - look at the labeled sentiment for a given tweet and figure out what word or phrase best supports it. Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.",
        "dataset_text": "You'll need train.csv, test.csv, and sample_submission.csv. Each row contains the text of a tweet and a sentiment label. In the training set you are provided with a word or phrase drawn from the tweet (selected_text) that encapsulates the provided sentiment. Make sure, when parsing the CSV, to remove the beginning / ending quotes from the text field, to ensure that you don't include them in your training. You're attempting to predict the word or phrase from the tweet that exemplifies the provided sentiment. The word or phrase should include all characters within that span (i.e. including commas, spaces, etc.). The format is as follows: <id>,\"<word or phrase that supports the sentiment>\" For example:"
    },
    {
        "name": "Global Wheat Detection",
        "url": "https://www.kaggle.com/competitions/global-wheat-detection",
        "overview_text": "Overview text not found",
        "description_text": " Open up your pantry and you\u2019re likely to find several wheat products. Indeed, your morning toast or cereal may rely upon this common grain. Its popularity as a food and crop makes wheat widely studied. To get large and accurate data about wheat fields worldwide, plant scientists use image detection of \"wheat heads\"\u2014spikes atop the plant containing grain. These images are used to estimate the density and size of wheat heads in different varieties. Farmers can use the data to assess health and maturity when making management decisions in their fields. However, accurate wheat head detection in outdoor field images can be visually challenging. There is often overlap of dense wheat plants, and the wind can blur the photographs. Both make it difficult to identify single heads. Additionally, appearances vary due to maturity, color, genotype, and head orientation. Finally, because wheat is grown worldwide, different varieties, planting densities, patterns, and field conditions must be considered. Models developed for wheat phenotyping need to generalize between different growing environments. Current detection methods involve one- and two-stage detectors (Yolo-V3 and Faster-RCNN), but even when trained with a large dataset, a bias to the training region remains. The Global Wheat Head Dataset is led by nine research institutes from seven countries: the University of Tokyo, Institut national de recherche pour l\u2019agriculture, l\u2019alimentation et l\u2019environnement, Arvalis, ETHZ, University of Saskatchewan, University of Queensland, Nanjing Agricultural University, and Rothamsted Research. These institutions are joined by many in their pursuit of accurate wheat head detection, including the Global Institute for Food Security, DigitAg, Kubota, and Hiphen. In this competition, you\u2019ll detect wheat heads from outdoor images of wheat plants, including wheat datasets from around the globe. Using worldwide data, you will focus on a generalized solution to estimate the number and size of wheat heads. To better gauge the performance for unseen genotypes, environments, and observational conditions, the training dataset covers multiple regions. You will use more than 3,000 images from Europe (France, UK, Switzerland) and North America (Canada). The test data includes about 1,000 images from Australia, Japan, and China. Wheat is a staple across the globe, which is why this competition must account for different growing conditions. Models developed for wheat phenotyping need to be able to generalize between environments. If successful, researchers can accurately estimate the density and size of wheat heads in different varieties. With improved detection farmers can better assess their crops, ultimately bringing cereal, toast, and other favorite dishes to your table.",
        "dataset_text": "More details on the data acquisition and processes are available at https://arxiv.org/abs/2005.02162 The data is images of wheat fields, with bounding boxes for each identified wheat head. Not all images include wheat heads / bounding boxes. The images were recorded in many locations around the world. The CSV data is simple - the image ID matches up with the filename of a given image, and the width and height of the image are included, along with a bounding box (see below). There is a row in train.csv for each bounding box. Not all images have bounding boxes. Most of the test set images are hidden. A small subset of test images has been included for your use in writing code. You are attempting to predict bounding boxes around each wheat head in images that have them. If there are no wheat heads, you must predict no bounding boxes."
    },
    {
        "name": "Rainforest Connection Species Audio Detection",
        "url": "https://www.kaggle.com/competitions/rfcx-species-audio-detection",
        "overview_text": "Overview text not found",
        "description_text": " Who doesn't enjoy the morning chirp of a bird or a frog\u2019s evening croak? Animals bring more than sweet songs and natural ambience to the world. The presence of rainforest species is a good indicator of the impact of climate change and habitat loss. As it's easier to hear these species than see them, it\u2019s important to use acoustic technologies that can work on a global scale. Real-time information, such as provided through machine learning techniques, could enable early-stage detection of human impacts on the environment. This result could drive more effective conservation management decisions. Traditional methods of assessing the diversity and abundance of species are costly and limited in space and time. And while automatic acoustic identification via deep learning has been successful, models require a large number of training samples per species. This limits applicability to rarer species, which are central to conservation efforts. Thus, methods to automate high-accuracy species detection in noisy soundscapes with limited training data are the solution. Rainforest Connection (RFCx) created the world\u2019s first scalable, real-time monitoring system for protecting and studying remote ecosystems. Unlike visual-based tracking systems like drones or satellites, RFCx relies on acoustic sensors that monitor the ecosystem soundscape at selected locations year round. RFCx technology has advanced to support a comprehensive biodiversity monitoring program that allows local partners to measure progress of wildlife restoration and recovery through principles of adaptive management. The RFCx monitoring platform also has the capacity to create convolutional neural network (CNN) models for analysis. In this competition, you\u2019ll automate the detection of bird and frog species in tropical soundscape recordings. You'll create your models with limited, acoustically complex training data. Rich in more than bird and frog noises, expect to hear an insect or two, which your model will need to filter out. If successful, you'll have a hand in a rapidly expanding field of science: the development of automated eco-acoustic monitoring systems. The resulting real-time information could enable earlier detection of human environmental impacts, making environmental conservation more swift and effective.",
        "dataset_text": "In this competition, you are given audio files that include sounds from numerous species. Your task is, for each test audio file, to predict the probability that each of the given species is audible in the audio clip. While the training files contain both the species identification as well as the time the species was heard, the time localization is not part of the test predictions. Note that the training data also includes false positive label occurrences to assist with training."
    },
    {
        "name": "G2Net Gravitational Wave Detection",
        "url": "https://www.kaggle.com/competitions/g2net-gravitational-wave-detection",
        "overview_text": "Overview text not found",
        "description_text": "It's been said that teamwork makes the dream work. This couldn't be truer for the breakthrough discovery of gravitational waves (GW), signals from colliding binary black holes in 2015. It required the collaboration of experts in physics, mathematics, information science, and computing. GW signals have led researchers to observe a new population of massive, stellar-origin black holes, to unlock the mysteries of neutron star mergers, and to measure the expansion of the Universe. These signals are unimaginably tiny ripples in the fabric of space-time and even though the global network of GW detectors are some of the most sensitive instruments on the planet, the signals are buried in detector noise. Analysis of GW data and the detection of these signals is a crucial mission for the growing global network of increasingly sensitive GW detectors. These challenges in data analysis and noise characterization could be solved with the help of data science. As with the multi-disciplined approach to the discovery of GWs, additional expertise will be needed to further GW research. In particular, social and natural sciences have taken an interest in machine learning, deep learning, classification problems, data mining, and visualization to develop new techniques and algorithms to efficiently handle complex and massive data sets. The increase in computing power and the development of innovative techniques for the rapid analysis of data will be vital to the exciting new field of GW Astronomy. Potential outcomes may include increased sensitivity to GW signals, application to control and feedback systems for next-generation detectors, noise removal, data conditioning tools, and signal characterization. G2Net is a network of Gravitational Wave, Geophysics and Machine Learning. Via an Action from COST (European Cooperation in Science and Technology), a funding agency for research and innovation networks, G2Net aims to create a broad network of scientists. From four different areas of expertise, namely GW physics, Geophysics, Computing Science and Robotics, these scientists have agreed on a common goal of tackling challenges in data analysis and noise characterization for GW detectors. In this competition, you\u2019ll aim to detect GW signals from the mergers of binary black holes. Specifically, you'll build a model to analyze simulated GW time-series data from a network of Earth-based detectors. The series of images above were taken from the 2015 paper announcing the discovery of gravitational waves from a pair of merging black holes. If successful, you'll play a part in solving a crucial mission in the exciting new field of GW science. With the development of new algorithms, scientists will have a better handle on the potential power of the data science community and their innovative approaches to data analysis. Moreover, it will enable closer interaction between computer science and physics, which could benefit both disciplines. Your participation can further this collaboration and the help advance this breakthrough discovery. We acknowledge support from the LIGO-Virgo-Kagra Collaboration of which the hosts are members. Specifically we acknowledge the use of the software resource lalsuite.",
        "dataset_text": "In this competition you are provided with a training set of time series data containing simulated gravitational wave measurements from a network of 3 gravitational wave interferometers (LIGO Hanford, LIGO Livingston, and Virgo). Each time series contains either detector noise or detector noise plus a simulated gravitational wave signal. The task is to identify when a signal is present in the data (target=1). The parameters that determine the exact form of a binary black hole waveform are the masses, sky location, distance, black hole spins, binary orientation angle, gravitational wave polarisation, time of arrival, and phase at coalescence (merger). These parameters (15 in total) have been randomised according to astrophysically motivated prior distributions and used to generate the simulated signals present in the data, but are not provided as part of the competition data. Each data sample (npy file) contains 3 time series (1 for each detector) and each spans 2 sec and is sampled at 2,048 Hz. The integrated signal-to noise ratio (SNR) is classically the most informative measure of how detectable a signal is and a typical level of detectability is when this integrated SNR exceeds ~8. This shouldn't confused with the instantaneous SNR - the factor by which the signal rises above the noise - and in nearly all cases the (unlike the first gravitational wave detection GW150914) these signals are not visible by eye in the time series."
    },
    {
        "name": "SETI Breakthrough Listen - E.T. Signal Search",
        "url": "https://www.kaggle.com/competitions/seti-breakthrough-listen",
        "overview_text": "Overview text not found",
        "description_text": "It\u2019s one of the most profound\u2014and perennial\u2014human questions. As technology improves, we\u2019re finding new and more powerful ways to seek answers. The Breakthrough Listen team at the University of California, Berkeley, employs the world\u2019s most powerful telescopes to scan millions of stars for signs of technology. Now it wants the Kaggle community to help interpret the signals they pick up. The Listen team is part of the Search for ExtraTerrestrial Intelligence (SETI) and uses the largest steerable dish on the planet, the 100-meter diameter Green Bank Telescope. Like any SETI search, the motivation to communicate is also the major challenge. Humans have built enormous numbers of radio devices. It\u2019s hard to search for a faint needle of alien transmission in the huge haystack of detections from modern technology. Current methods use two filters to search through the haystack. First, the Listen team intersperses scans of the target stars with scans of other regions of sky. Any signal that appears in both sets of scans probably isn\u2019t coming from the direction of the target star. Second, the pipeline discards signals that don\u2019t change their frequency, because this means that they are probably nearby the telescope. A source in motion should have a signal that suggests movement, similar to the change in pitch of a passing fire truck siren. These two filters are quite effective, but we know they can be improved. The pipeline undoubtedly misses interesting signals, particularly those with complex time or frequency structure, and those in regions of the spectrum with lots of interference. In this competition, use your data science skills to help identify anomalous signals in scans of Breakthrough Listen targets. Because there are no confirmed examples of alien signals to use to train machine learning algorithms, the team included some simulated signals (that they call \u201cneedles\u201d) in the haystack of data from the telescope. They have identified some of the hidden needles so that you can train your model to find more. The data consist of two-dimensional arrays, so there may be approaches from computer vision that are promising, as well as digital signal processing, anomaly detection, and more. The algorithm that\u2019s successful at identifying the most needles will win a cash prize, but also has the potential to help answer one of the biggest questions in science.  The Breakthrough Listen science and engineering effort is headquartered at the University of California, Berkeley SETI Research Center. The Breakthrough Prize Foundation funds the Breakthrough Initiatives which manages Breakthrough Listen. The Green Bank Observatory is supported by the National Science Foundation, and is operated by Associated Universities, Inc. under a cooperative agreement.",
        "dataset_text": "In this competition you are tasked with looking for technosignature signals in cadence snippets taken from the Green Bank Telescope (GBT). Please read the extended description on the Data Information tab for detailed information about the data (that's too lengthy to include here)."
    },
    {
        "name": "Kore 2022",
        "url": "https://www.kaggle.com/competitions/kore-2022",
        "overview_text": "Overview text not found",
        "description_text": "In this turn-based simulation game you control a small armada of spaceships. As you mine the rare mineral \u201ckore\u201d from the depths of space, you teleport it back to your homeworld. But it turns out you aren\u2019t the only civilization with this goal. In each game two players will compete to collect the most kore from the board. Whoever has the largest kore cache by the end of 400 turns\u2014or eliminates all of their opponents from the board before that\u2014will be the winner!  Your algorithms determine the movements of your fleets to collect kore, but it's up to you to figure out how to make effective and efficient moves. You control your ships, build new ships, create shipyards, eliminate opponents, and mine the kore on the game board. May your fleet live long and prosper!",
        "dataset_text": "Kore 2022 Starter Agent"
    },
    {
        "name": "March Machine Learning Mania",
        "url": "https://www.kaggle.com/competitions/march-machine-learning-mania-2014",
        "overview_text": "Overview text not found",
        "description_text": "Each year, millions of people fill out a bracket to predict the outcome of the popular men\u2019s college basketball tournament that tips off in March. While the odds of creating a perfect bracket are astronomical, these odds are made better by the growing amount of data collected throughout the season, including player statistics, tournament seeds, geographical factors and social media. How well can machine learning and statistical techniques improve the forecast? Presented by Intel, this competition will test how well predictions based on data stack up against a (jump) shot in the dark. We have assembled the basic elements necessary to get started with tournament prediction. The provided data covers nearly two decades of historical games, but you\u2019re also encouraged to use data from external sources. To help turn all of that information into useful insight, Intel is making its big data technologies more affordable, available, and easier to use for everything from helping develop new scientific discoveries and business models to gaining the upper hand on good-natured predictions of sporting events. In stage one of this two-stage competition, participants will build and test their models against the previous five tournaments. In the second stage, participants will predict the outcome of the 2014 tournament. You don\u2019t need to participate in the first stage to enter the second, but the first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2014 results, for which you\u2019ll predict winning percentages for the likelihood of each possible matchup, not just a traditional bracket. To sweeten the pot, Intel will present the team with the most accurate predictions a $15,000 cash prize. Get started today \u2013 predictions are due by Wednesday, March 19, 2014. Please visit the FAQs for more information.",
        "dataset_text": "If you are unfamiliar with the format and intricacies of the NCAA tournament, we encourage reading its wikipedia page before diving into the data.  The data description and schema may seem daunting at first, but it's not as complicated as it looks. As a reminder, you are encouraged to incorporate your own sources of data. We have provided team-level historical data to jump-start the modeling process, but there is also a world of player-level and game-level data that may be useful. We extend our gratitude to Kenneth Massey for his work gathering and providing the historical data. Stage 1 - You should submit predicted probabilities for every possible matchup in the past 5 NCAA tournaments. Stage 2 - You should submit predicted probabilities for every possible matchup before the 2014 tournament begins. Refer to the Timeline page for specific dates. In both stages, the sample submission will tell you which games to predict. Below we describe the format and fields of the \"essential\" data files. Optional files may be added to the data while the competition is running. You can assume that we will provide the essential files for the current season. You should not assume that we will provide optional files for the current season. To avoid confusion, we will keep the current season data (for stage 2) separate from the historical data (stage 1). teams.csv This file identifies the 356 different college teams that are present in at least one of the seasons from 1995-1996 through 2013-2014. The other data files that identify teams, such as when game-by-game results are listed or tournament seeds are listed, will always reference the teams by their id number rather than by their name. This makes the files more compact and also eliminates any possible spelling issues.  This is the only file that actually contains the text names of the college teams. seasons.csv This file identifies the 18 different seasons included in the historical data, along with certain season-level properties. regular_season_results.csv This file identifies the game-by-game results for all 18 seasons of historical data, from season A (1995-6) through season R (2012-3). Each year, it includes all games played from daynum 0 through 132 (which by definition is \"Selection Sunday\", the day that tournament pairings are announced). Each row in the file represents a single game played. tourney_results.csv This file identifies the game-by-game NCAA tournament results for all 18 seasons of historical data, from season A (1995-6) through season R (2012-3). The data is formatted exactly like the \"regular_season_results\" data, except that all games are assumed to be on a neutral court, and therefore the \"wloc\" column is not included. Note that these games also include the play-in games (which always occurred on day 134/135) for those years that had play-in games. tourney_seeds.csv This file identifies the seeds for the final 64 teams in each NCAA tournament, for all 18 seasons of historical data. The losers of play-in games are not listed, though their games are included in the \"tourney_results.csv\" file. Therefore there are exactly 64 rows for each year, and a total of 64x18=1152 rows. tourney_slots.csv This file identifies the mechanism by which teams are paired against each other, depending upon their seeds. Because of the existence of play-in games for particular seed numbers, the pairings have small differences from year to year. If there were N teams in the tournament during a particular year, there were N-1 teams eliminated (leaving one champion) and therefore N-1 games played, as well as N-1 slots in the tournament bracket, and thus there will be N-1 records in this file for that season. There were 64 tournament teams in seasons A-E, 65 tournament teams in seasons F-O, and 68 tournament teams in seasons P-R."
    },
    {
        "name": "March Machine Learning Mania 2015",
        "url": "https://www.kaggle.com/competitions/march-machine-learning-mania-2015",
        "overview_text": "Overview text not found",
        "description_text": "At Kaggle HQ and in offices across the country, March is a month when bracketology is in bloom. Back by popular demand, our second annual March Machine Learning Mania competition pits you against the millions of sports fans and office-pool bandwagoners who are hoping to win big by correctly predicting the outcome of the men's NCAA basketball tournament.  While the odds of forecasting a perfect bracket are astronomical, these odds are improved by the growing amount of data collected throughout the season, including player statistics, tournament seeds, geographical factors and social media. How well can machine learning and statistical techniques improve the forecast? Presented by HP Software's industry leading Big Data group and the HP Haven Big Data platform, this competition will test how well predictions based on data stack up against a (jump) shot in the dark.  This competition allows you to get creative with the datasets you use to create your model. We provide data covering three decades of historical games, but you're highly encouraged to pull in data from external sources.  The 50+ REST APIs from HP IDOL OnDemand are a great way to get started augmenting the dataset. Developer accounts are free and includes free monthly quota! Begin by extracting trending topics and identifying entities from the IDOL OnDemand news dataset (accessed via the Query Text Index API) or by analyzing public sentiment about players and teams using data from your social media feed.  In stage one of this two-stage competition, participants will build and test their models against the previous four tournaments. In the second stage, participants will predict the outcome of the 2015 tournament. You don\u2019t need to participate in the first stage to enter the second, but the first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2015 results, for which you\u2019ll predict winning percentages for the likelihood of each possible matchup, not just a traditional bracket. HP is sponsoring $15,000 in cash prizes for the winners. Please visit the FAQs for more information. March Machine Learning Mania 2015 is presented by HP. Please see About the sponsor to read more. ",
        "dataset_text": "If you are unfamiliar with the format and intricacies of the NCAA tournament, we encourage reading the wikipedia page before diving into the data.  The data description and schema may seem daunting at first, but is not as complicated as it appears. As a reminder, you are encouraged to incorporate your own sources of data. We have provided team-level historical data to jump-start the modeling process, but there is also player-level and game-level data that may be useful. If you're looking for some powerful technology to help get the most from all your data, you could load your data into the Vertica Analytics Platform for data preparation and use Distributed R, a scaleable and high-performance platform for R with out-of-the-box parallel algorithms. After you build and evaluate predictive model in Distributed R you can even deploy model(s) back to Vertica Analytics Platform for in-database prediction scoring using simple SQL and can combine prediction results with other insights that you may have derived from the data. Note that Vertica Analytics Platform offers wide array of SQL analytic functions such as in-database sentiment analysis functions, geospatial functions that can help you derive new and interesting attributes. We extend our gratitude to Kenneth Massey for his work gathering and providing much of the historical data. Stage 1 - You should submit predicted probabilities for every possible matchup in the past 4 NCAA tournaments (2011-2014). Stage 2 - You should submit predicted probabilities for every possible matchup before the 2015 tournament begins. Refer to the Timeline page for specific dates. In both stages, the sample submission will tell you which games to predict. Below we describe the format and fields of the \"essential\" data files. Optional files may be added to the data while the competition is running. You can assume that we will provide the essential files for the current season. You should not assume that we will provide optional files for the current season. To avoid confusion, we will keep the current season data (for stage 2) separate from the historical data (stage 1). teams.csv This file identifies the different college teams present in the dataset. Each team has a 4 digit id number. seasons.csv This file identifies the different seasons included in the historical data, along with certain season-level properties. regular_season_compact_results.csv This file identifies the game-by-game results for 30 seasons of historical data, from 1985 to 2014. Each year, it includes all games played from daynum 0 through 132 (which by definition is \"Selection Sunday\", the day that tournament pairings are announced). Each row in the file represents a single game played. regular_season_detailed_results.csv This file is a more detailed set of game results, covering seasons 2003-2014. This includes team-level total statistics for each game (total field goals attempted, offensive rebounds, etc.) The column names should be self-explanatory to basketball fans (as above, \"w\" or \"l\" refers to the winning or losing team): tourney_compact_results.csv This file identifies the game-by-game NCAA tournament results for all seasons of historical data. The data is formatted exactly like the regular_season_compact_results.csv data. Note that these games also include the play-in games (which always occurred on day 134/135) for those years that had play-in games. tourney_detailed_results.csv This file contains the more detailed results for tournament games from 2003 onward. tourney_seeds.csv This file identifies the seeds for all teams in each NCAA tournament, for all seasons of historical data. Thus, there are between 64-68 rows for each year, depending on the bracket structure. tourney_slots.csv This file identifies the mechanism by which teams are paired against each other, depending upon their seeds. Because of the existence of play-in games for particular seed numbers, the pairings have small differences from year to year. If there were N teams in the tournament during a particular year, there were N-1 teams eliminated (leaving one champion) and therefore N-1 games played, as well as N-1 slots in the tournament bracket, and thus there will be N-1 records in this file for that season."
    },
    {
        "name": "Personalized Medicine: Redefining Cancer Treatment",
        "url": "https://www.kaggle.com/competitions/msk-redefining-cancer-treatment",
        "overview_text": "Overview text not found",
        "description_text": "A lot has been said during the past several years about how precision medicine and, more concretely, how genetic testing is going to disrupt the way diseases like cancer are treated. But this is only partially happening due to the huge amount of manual work still required. Memorial Sloan Kettering Cancer Center (MSKCC) launched this competition, accepted by the NIPS 2017 Competition Track,  because we need your help to take personalized medicine to its full potential.  Once sequenced, a cancer tumor can have thousands of genetic mutations. But the challenge is distinguishing the mutations that contribute to tumor growth (drivers) from the neutral mutations (passengers).  Currently this interpretation of genetic mutations is being done manually. This is a very time-consuming task where a clinical pathologist has to manually review and classify every single genetic mutation based on evidence from text-based clinical literature. For this competition MSKCC is making available an expert-annotated knowledge base where world-class researchers and oncologists have manually annotated thousands of mutations. We need your help to develop a Machine Learning algorithm that, using this knowledge base as a baseline, automatically classifies genetic variations. Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.",
        "dataset_text": "In this competition you will develop algorithms to classify genetic mutations based on clinical evidence (text). There are nine different classes a genetic mutation can be classified on. This is not a trivial task since interpreting clinical evidence is very challenging even for human specialists. Therefore, modeling the clinical evidence (text) will be critical for the success of your approach. Both, training and test, data sets are provided via two different files. One (training/test_variants) provides the information about the genetic mutations, whereas the other (training/test_text) provides the clinical evidence (text) that our human experts used to classify the genetic mutations. Both are linked via the ID field. Therefore the genetic mutation (row) with ID=15 in the file training_variants, was classified using the clinical evidence (text) from the row with ID=15 in the file training_text Finally, to make it more exciting!! Some of the test data is machine-generated to prevent hand labeling. You will submit all the results of your classification algorithm, and we will ignore the machine-generated samples. "
    },
    {
        "name": "Data Science for Good: CareerVillage.org",
        "url": "https://www.kaggle.com/competitions/data-science-for-good-careervillage",
        "overview_text": "Overview text not found",
        "description_text": "In this competition you'll notice there isn't a leaderboard, and you are not required to develop a predictive model. This isn't a traditional supervised Kaggle machine learning competition. CareerVillage.org is a nonprofit that crowdsources career advice for underserved youth. Founded in 2011 in four classrooms in New York City, the platform has now served career advice from 25,000 volunteer professionals to over 3.5M online learners. The platform uses a Q&A style similar to StackOverflow or Quora to provide students with answers to any question about any career. In this Data Science for Good challenge, CareerVillage.org, in partnership with Google.org, is inviting you to help recommend questions to appropriate volunteers. To support this challenge, CareerVillage.org has supplied five years of data. The U.S. has almost 500 students for every guidance counselor. Underserved youth lack the network to find their career role models, making CareerVillage.org the only option for millions of young people in America and around the globe with nowhere else to turn. To date, 25,000 volunteers have created profiles and opted in to receive emails when a career question is a good fit for them. This is where your skills come in. To help students get the advice they need, the team at CareerVillage.org needs to be able to send the right questions to the right volunteers. The notifications sent to volunteers seem to have the greatest impact on how many questions are answered. Your objective: develop a method to recommend relevant questions to the professionals who are most likely to answer them. Performance: How well does the solution match professionals to the questions they would be motivated to answer? CareerVillage.org will not be able to live-test every submission, so a strong entry will clearly articulate why it will be effective at motivating answers. Easy to implement: The CareerVillage.org team wants to put the winning submissions to work, quickly. A good entry will be well documented and easy to test in production. Extensibility: In the future, CareerVillage.org aims to add more data features and to accommodate new objectives. Winning submissions should allow for this and other augmentations to be added in the future.",
        "dataset_text": "CareerVillage.org has provided several years of anonymized data and each file comes from a table in their database."
    },
    {
        "name": "Kuzushiji Recognition",
        "url": "https://www.kaggle.com/competitions/kuzushiji-recognition",
        "overview_text": "Overview text not found",
        "description_text": "Imagine the history contained in a thousand years of books. What stories are in those books? What knowledge can we learn from the world before our time? What was the weather like 500 years ago? What happened when Mt. Fuji erupted? How can one fold 100 cranes using only one piece of paper? The answers to these questions are in those books. Japan has millions of books and over a billion historical documents such as personal letters or diaries preserved nationwide. Most of them cannot be read by the majority of Japanese people living today because they were written in \u201cKuzushiji\u201d. Even though Kuzushiji, a cursive writing style, had been used in Japan for over a thousand years, there are very few fluent readers of Kuzushiji today (only 0.01% of modern Japanese natives). Due to the lack of available human resources, there has been a great deal of interest in using Machine Learning to automatically recognize these historical texts and transcribe them into modern Japanese characters. Nevertheless, several challenges in Kuzushiji recognition have made the performance of existing systems extremely poor. (More information in About Kuzushiji) This is where you come in. The hosts need help from machine learning experts to transcribe Kuzushiji into contemporary Japanese characters. With your help, Center for Open Data in the Humanities (CODH) will be able to develop better algorithms for Kuzushiji recognition. The model is not only a great contribution to the machine learning community, but also a great help for making millions of documents more accessible and leading to new discoveries in Japanese history and culture.  Center for Open Data in the Humanities (CODH) conducts research and development to enhance access to humanities data using state-of-the-art technology in informatics and statistics. The National Institute of Japanese Literature (NIJL) is an institution which strives to serve researchers in the field of Japanese literature as well as those working in various other humanities, by collecting in one location a vast storage of materials related to Japanese literature gathered from all corners of the country. The National Institute of Informatics (NII) is Japan's only general academic research institution seeking to create future value in the new discipline of informatics. NII seeks to advance integrated research and development activities in information-related fields, including networking, software, and content. Official Collaborators\nMikel Bober-Irizar (anokas) Kaggle Grandmaster and Alex Lamb (MILA. Quebec Artificial Intelligence Institute)",
        "dataset_text": "Kuzushiji is a cursive script that was used in Japan for over a thousand years that has fallen out of use in modern times.\nVast portions of Japanese historical documents now cannot be read by most Japanese people. By helping to automate the transcription of kuzushiji you will contribute to unlocking a priceless trove of books and records. The specific task is to locate and classify each kuzushiji character on a page. While complete bounding boxes are provided for the training set, only a single point within the ground truth bounding box is needed for submissions. There are a few important things to keep in mind for your submissions:"
    },
    {
        "name": "Data Science for Good: City of Los Angeles",
        "url": "https://www.kaggle.com/competitions/data-science-for-good-city-of-los-angeles",
        "overview_text": "Overview text not found",
        "description_text": "Help the City of Los Angeles to structure and analyze its job descriptions The City of Los Angeles faces a big hiring challenge: 1/3 of its 50,000 workers are eligible to retire by July of 2020. The city has partnered with Kaggle to create a competition to improve the job bulletins that will fill all those open positions. The content, tone, and format of job bulletins can influence the quality of the applicant pool. Overly-specific job requirements may discourage diversity. The Los Angeles Mayor\u2019s Office wants to reimagine the city\u2019s job bulletins by using text analysis to identify needed improvements. The goal is to convert a folder full of plain-text job postings into a single structured CSV file and then to use this data to: (1) identify language that can negatively bias the pool of applicants; (2) improve the diversity and quality of the applicant pool; and/or (3) make it easier to determine which promotions are available to employees in each job class. WIth your help, Los Angeles will overcome a wave of retirements and fill those jobs with a strong and diverse workforce. Good luck and happy Kaggling!",
        "dataset_text": "The job bulletins will be provided as a folder of plain-text files, one for each job classification. Job Bulletins [Folder] Instructions and Additional Documents [Folder]"
    },
    {
        "name": "Higgs Boson Machine Learning Challenge",
        "url": "https://www.kaggle.com/competitions/higgs-boson",
        "overview_text": "Overview text not found",
        "description_text": "The evaluation metric is the approximate median significance (AMS): AMS=\n2((\ud835\udc60+\ud835\udc4f+\n\ud835\udc4f\n\ud835\udc5f\n)log(1+\n\ud835\udc60\n\ud835\udc4f+\n\ud835\udc4f\n\ud835\udc5f\n)\u2212\ud835\udc60)\n\u203e\n\u203e\n\u203e\n\u203e\n\u203e\n\u203e\n\u203e\n\u203e\n\u203e\n\u203e\n\u203e\n\u203e\n\u203e\n\u203e\n\u203e\n\u203e\n\u203e\n\u203e\n\u203e\n\u203e\n\u203e\n\u203e\n\u203e\n\u203e\n\u203e\n\u203e\n\u203e\n\u203e\n\u203e\n\u203e\n\u203e\n\u203e\n\u203e\n\u203e\n\u203e\n\u221a where More precisely, let \\\\((y_1, \\ldots, y_n) \\in \\{\\text{b},\\text{s}\\}^n\\\\) be the vector of true test labels, let \\\\((\\hat{y}_1, \\ldots, \\hat{y}_n) \\in \\{\\text{b},\\text{s}\\}^n\\\\) be the vector of predicted (submitted) test labels, and let \\\\((w_1, \\ldots, w_n) \\in {\\mathbb{R}^+}^n\\\\) be the vector of weights. Then \ud835\udc60=\n\u2211\n\ud835\udc56=1\n\ud835\udc5b\n\ud835\udc64\n\ud835\udc56\n\ud835\udfd9{\n\ud835\udc66\n\ud835\udc56\n=s}\ud835\udfd9{\n\ud835\udc66\n\u0302 \n\ud835\udc56\n=s}\nn and \ud835\udc4f=\n\u2211\n\ud835\udc56=1\n\ud835\udc5b\n\ud835\udc64\n\ud835\udc56\n\ud835\udfd9{\n\ud835\udc66\n\ud835\udc56\n=b}\ud835\udfd9{\n\ud835\udc66\n\u0302 \n\ud835\udc56\n=s},\nn where the indicator function \\\\(\\mathbb{1}\\{A\\}\\\\) is 1 if its argument \\\\(A\\\\) is true and 0 otherwise. For more information on the statistical model and the derivation of the metric, see the technical documentation. We have provided python code for the metric is available from the Data page and a Python starting kit. The submission file format is  Your submission file should have a header row and three columns",
        "dataset_text": "For detailed information on the semantics of the features, labels, and weights, see the technical documentation from the LAL website on the task. Some details to get started:"
    },
    {
        "name": "Recursion Cellular Image Classification",
        "url": "https://www.kaggle.com/competitions/recursion-cellular-image-classification",
        "overview_text": "Overview text not found",
        "description_text": "The cost of some drugs and medical treatments has risen so high in recent years that many patients are having to go without. You can help with a classification project that could make researchers more efficient. One of the more surprising reasons behind the cost is how long it takes to bring new treatments to market. Despite improvements in technology and science, research and development continues to lag. In fact, finding new treatments takes, on average, more than 10 years and costs hundreds of millions of dollars. Recursion Pharmaceuticals, creators of the industry\u2019s largest dataset of biological images, generated entirely in-house, believes AI has the potential to dramatically improve and expedite the drug discovery process. More specifically, your efforts could help them understand how drugs interact with human cells. This competition will have you disentangling experimental noise from real biological signals. Your entry will classify images of cells under one of 1,108 different genetic perturbations. You can help eliminate the noise introduced by technical execution and environmental variation between experiments. If successful, you could dramatically improve the industry\u2019s ability to model cellular images according to their relevant biology. In turn, applying AI could greatly decrease the cost of treatments, and ensure these treatments get to patients faster. This competition is a part of the NeurIPS 2019 competition track. Winners will be invited to contribute their solutions towards the workshop presentation. Thank you to the following sponsors & supporters of this competition: Google Cloud: Google Cloud is widely recognized as a global leader in delivering a secure, open and intelligent enterprise cloud platform. Our technology is built on Google\u2019s private network and is the product of nearly 20 years of innovation in security, network architecture, collaboration, artificial intelligence and open source software. We offer a simply engineered set of tools and unparalleled technology across Google Cloud Platform and G Suite that help bring people, insights and ideas together. Customers across more than 150 countries trust Google Cloud to modernize their computing environment for today\u2019s digital world. DoiT: You have the cloud and we have your back. For nearly a decade, we\u2019ve been helping businesses build and scale cloud solutions with our world-class cloud engineering support. We help our customers with technical support and consulting on building and operating complex large-scale distributed systems, developing better machine learning models and setting up big data solutions using Google Cloud, Amazon AWS and Microsoft Azure. NVIDIA: NVIDIA\u2019s (NASDAQ: NVDA) invention of the GPU in 1999 sparked the growth of the PC gaming market, redefined modern computer graphics and revolutionized parallel computing. More recently, GPU deep learning ignited modern AI \u2014 the next era of computing \u2014 with the GPU acting as the brain of computers, robots and self-driving cars that can perceive and understand the world. More information at http://nvidianews.nvidia.com. Lambda: Lambda provides Deep Learning workstations, servers, and GPU cloud services. Lambda Deep Learning infrastructure is used by the world's leading AI research & development organizations including Apple, Microsoft, MIT, Stanford, and the US Government. To learn more, visit www.lambdalabs.com.",
        "dataset_text": "One of the main challenges for applying AI to biological microscopy data is that even the most careful replicates of a process will not look identical. This dataset challenges you to develop a model for identifying replicates that is robust to experimental noise. The same siRNAs (effectively genetic perturbations) have been applied repeatedly to multiple cell lines, for a total of 51 experimental batches. Each batch has four plates, each of which has 308 filled wells. For each well, microscope images were taken at two sites and across six imaging channels. Not every batch will necessarily have every well filled or every siRNA present. We've condensed this description down to the essentials; for additional details please see RxRx.ai. For ease of use with TPUs (and TensorFlow generally), we're also posting the data in tfrecord format in two Google Cloud Storage buckets: If you take advantage of the free cloud TPU quota offered with this competition, be sure to set up your TPU in a matching region.\nWithin the buckets, you'll find the tfrecords structured both by experiment and randomly split within train and test. Read more information about the TFRecords here."
    },
    {
        "name": "Practice Fusion Analyze This! 2012 - Prediction Challenge",
        "url": "https://www.kaggle.com/competitions/pf2012",
        "overview_text": "Overview text not found",
        "description_text": "Description text not found",
        "dataset_text": "The data sets for this competition have been removed at the request of Practice Fusion. The Analyze This! data set consists of 17 different files, 2 common files and 15 training set files. These files describe the records for 10,000 patients. They are in comma separated value (csv) format. A test set will be released at the start of the predictive modeling competition. Common files Data set and column names Additional Notes"
    },
    {
        "name": "RTA Freeway Travel Time Prediction",
        "url": "https://www.kaggle.com/competitions/RTA",
        "overview_text": "Overview text not found",
        "description_text": "Description text not found",
        "dataset_text": "The NSW Roads and Traffic Authority (RTA) has made available several years' worth of historical travel time data for Sydney's M4 freeway. The data are collected from loops on the road at three minute intervals.\n\n\n\n\nThe file is formated as follows\n                         40010    40015    40020    \u2026\n1/03/10 15:01    804        209       804      248\n1/03/10 15:04    892        212       801      237\n1/03/10 15:07    857        214       821      243\n\u2026                       849        222       834      252\n\nThe header row shows the route IDs and the first column has timestamps. The cells show the travel time in deciseconds.\n\nRoute IDs are ordered sequentially. Routes 40010-40150 are westbound travel times and 41010-41160 are eastbound travel times. See m4-map.pdf for an approximate guide to the freeway's layout.\n\nRTAData.csv holds the travel time dataset. Complete data is provided from March 2010 to July 2010. For the data from August 2010 to mid-November 2010, the letter \"x\" appears whenever a prediction is required.\n\n29 cut-off times have been selected. After those cut-off times, predictions must be made for the next 15 minutes, 30 minutes, 45 minutes, 1 hour, 90 minutes, 2 hours, 6 hours, 12 hours, 18 hours and 24 hours. To prevent particicipants from meaningfully using the future to predict the present, data are not revealved again until 18 hours after the last forecast has been made (or 42 hours after the cut-off point).\n\nParticipants are required to provide 290 forecasts (10 for each cut-off point) for the 61 routes. The route IDs should appear in a header row, and the timestamps in the first column. The file sampleEntry.csv is an example entry, showing how entries should be formatted.\n\nThe RTA has also provided some additional historical data, RTAHistorical.csv (collected using their old system), which covers November 2008 to February 2010. The data are consistent with the newer RTA data except that:\n- they don't cover weekends; and\n- there are no data for routes 40092 and 41140.\n\nUpdate 17 December: The file RTAError.csv shows where loop readings are suspected to be inaccurate. Loop readings can be inaccurate because the loop is behaving erratically or because the loop is not responding at all (in which case the average travel time from adjacent loop combinations have been used).\n\nThe file RTAError.csv shows the proportion of loops in a route that have failed (a route is made up of many loops). A reading of 0 means that all loops are functioning properly, a reading of 0.5 means that 50 per cent of the loops are functioning and a reading of 1 means that no loops are functioning.\n\nThe file RouteLengthApprox.csv shows approximate route lengths. It is calculated as the number of loops in the route multiplied by 500m (the approximate distance between loops)."
    },
    {
        "name": "Deloitte/FIDE Chess Rating Challenge",
        "url": "https://www.kaggle.com/competitions/ChessRatings2",
        "overview_text": "Overview text not found",
        "description_text": "Description text not found",
        "dataset_text": "The data files cover a recent 135-month period of professional chess game outcomes, extracted from the database of the world chess federation (FIDE).  The training period spans 132 months, followed immediately by a three-month test period (months 133-135).  The specific dates are not provided; all games simply have an integer Month ID# ranging from 1 to 135.  There are 54,205 different chess players included in the data; those players are uniquely identified by an ID ranging from 1 to 54,205.\n\nThere are eight different files provided as part of the contest:\n\n(1-3) The primary training dataset (including 1,840,124 games) can be used to train your prediction system.  It is split up into three separate files: part 1 (months 1-96), part 2 (months 97-118), and part 3 (months 119-132).\n\n(4-5) A secondary training dataset (including 312,511 games) and tertiary training dataset (including 265,577 games) provide optional additional training data that may be useful in validating your system or predicting game results.  The contest does not require you to use these additional training datasets, but most systems will benefit significantly from using the secondary and tertiary training datasets. More details can be found on the Additional Training Datasets page.\n\n(6) A test dataset (including 100,000 games) identifies the chess games during the test period that must be predicted.  Also note that an example submission file is provided on the Submission Instructions page.\n\n(7) An initial rating list (including 14,118 players) provides an initial list of players' FIDE ratings going into the start of the training period.  This represents approximately 25% of the pool of players; initial ratings for the remaining 75% of players must be calculated based on their initial games during the training period.  Remember that anyone wishing to qualify for the FIDE prize category must use this list to determine the initial ratings of these 14,118 players.\n\n(8) An example submission file (including 100,001 rows) is formatted appropriately for submission to the contest, containing a prediction of an expected score of 50% for all 100,000 games in the test set.  This is identical to the All Draws Benchmark.  It includes a header row followed by predictions for test games #1, #2, #3, \u2026, #99,998, #99,999, #100,000, for a total of 100,001 rows.  Your actual submissions should be exactly like this file except that it will contain your actual predictions for every game, instead of a universal 50%.\n\nPrimary Training Dataset\n\nThere are 1,840,124 games in the primary training set, spanning a recent eleven-year period (known as Month #1 through Month #132).  They represent 35-40% of the almost five million game results that were used by FIDE (the world chess federation) to calculate player ratings during that time.  Many of those five million game results are not individually available in computer files, and so they had to be partially reconstructed from other sources for this contest; this is why there are \"only\" 1.84 million games in the primary training set.  For much of the training period, the primary training set contains about 25% of the full set of FIDE games used for official rating calculations.  The percentage is much higher in the final two or three years of the training period, thanks to recent changes in FIDE's regulations for how tournament organizers must submit their tournament's results to FIDE.  Recent data is much more readily available.\n\nThere are three possible outcomes to a completed chess game - either White wins, or the game is drawn, or Black wins.  These correspond to a \"score\" for White of 1.0, 0.5, or 0.0 points, and conversely a score for Black of 0.0, 0.5, or 1.0 points.  Tournament scoring typically adds up a player's total score, so someone who wins three games, draws five games, and loses one game would have a total score of 5.5 out of 9 games, and (despite having fewer wins) would finish ahead of someone else who wins five games, draws zero games and loses four games (total score of 5.0 out of 9).  Thus it is important to consider not just how many games a player manages to win, but also how many losses they manage to avoid.  White always gets to move first and so there is a slight advantage to having the white pieces (more pronounced among the strongest players).  For instance, in the primary training set, there are 125,000 games where the absolute difference in FIDE ratings between the players was fewer than 20 points, and the average score for White in those games was 53.7%.\n\nThe primary training dataset includes 1,840,124 total games, each of which is assigned a unique Primary Training Game ID# (called PTID) between 1 and 1,840,124.  For each game, the PTID, and the Month ID (between 1 and 132), and the player ID #'s for the White and Black players, are provided, as well as the outcome of the game from White's perspective (a score of either 1.0, 0.5, or 0.0).  The FIDE ratings of the players are not provided in the primary training dataset; part of the challenge is for you to make your own assessments of the players' changing strengths over time.\n\nFinally, the primary training dataset provides two redundant values (named WhitePlayerPrev and BlackPlayerPrev) for each game result, indicating the quantity of games (in the primary training set) played by each of the two players within the previous 24 months.  These values could actually be calculated by reviewing the data from earlier months in the primary training dataset, rather than being explicitly provided, but for your convenience they have been explicitly provided for each training game.\n\nWhy are WhitePlayerPrev and BlackPlayerPrev relevant?  Well, the pool of active players is always increasing in size.  Thus in any given month, many of the games that are played involve players who have previously played a very small number of games (or none at all).  It seemed unfair to include many of those games in the test set, since the predictions would necessarily be quite uncertain.  So in the creation of the test set for months 133-135, there was a filter applied.  Instead of including all games played during months 133-135, the test dataset only includes games in which both players had at least 12 games played during the final 24 months of the primary training dataset (i.e. months 109-132).  Participants may wish to perform cross-validation, drawing samples from the primary training set that would have similar characteristics to the test dataset, in order to develop a model or optimize parameters in the model.  In order to create your own validation sets from the training data, you would presumably want to apply the same sampling filter that was used to create the test set (requiring WhitePlayerPrev > 12 and BlackPlayerPrev > 12).  Including the values of WhitePlayerPrev and BlackPlayerPrev in the training games allows you to do this.\n\nFor instance, if your validation test set was drawn from the games of month 130 only, and you only selected games from that month having WhitePlayerPrev > 12 and BlackPlayerPrev > 12, then you would only be getting games in which both players had at least 12 games played in the previous 24 months of the primary training set (i.e. months 106-129).  And this filter is analogous to what was done to filter the games for the test set.\n\nTest Dataset\n\nThe test dataset (covering 100,000 games between months 133-135) is very similar to the primary training dataset, although it intentionally omits the WhiteScore column.  Of course the point of the contest is to have participants accurately predict the WhiteScore for all games in the test dataset, so the test dataset only indicates the identities of the players, and the month in which the game was played, and does not tell you the game outcomes.  The test dataset includes all known games played in months 133-135 where both players had played 12 or more games in the final 24 months of the primary training dataset.\n\nThe test dataset includes 100,000 total games, each of which is assigned a unique Test Game ID# (called TEID) between 1 and 100,000.  For each game, the TEID, and the Month ID (between 133 and 135), and the player ID's for the White and Black players, are provided.  The TEID is particularly important because you will use it to uniquely identify each test game when submitting predictions, and to sort the rows in your submission file.\n\nPlease note that you should NOT use the test dataset as an additional source of clues about a player's strength.  The predictions for months 133-135 should be based upon the players' estimated playing abilities at the end of month 132, and these predictions must be completely prospective, as though you made the predictions right at the end of month 132.  We could have asked for a complete cross product (54,205 x 54,205 x 3) of predictions of all possible games, instead of just the 100,000 games from the test set, but this would have been too large a file to submit.  Instead, in order to discourage participants from \"mining\" the test dataset for additional information, the test dataset of 100,000 games includes many thousands of spurious matchups.  The predictions for those particular games will be discarded and not used when scoring submissions.  The method for generating the spurious matchups is not random, and is not being revealed.  The spurious games are intentionally being included, in order to encourage people to only predict prospectively.\n\nPool of Players\n\nThe contest datasets do not include games for all players in the FIDE rating pool.  There are many isolated pockets of chess players around the world, playing only within their geographical region or age group, who are not well-connected with the rest of the pool of players.  We wanted to focus on a large pool of reasonably well-connected, reasonably active players in this contest, spanning the whole range of playing strength from weakest to strongest.  Therefore an iterative selection process was performed, starting with the four most prominent world champions (Viswanathan Anand, Garry Kasparov, Vladimir Kramnik, and Veselin Topalov) during that span.  They formed the initial pool of four players, and then additional players were iteratively added to the existing pool if they were well-connected to it.  The final pool represents the complete population of players whose games are included in the contest.  For these players, all known games during the appropriate time periods are included; no data was omitted intentionally for them.\n\nSo first there were four players in the pool, and then we looked to see how many other players were \"well-connected\" to those four initial players.  There are 19 players who played against all four of those opponents, having at least 15 games total against those four opponents as a group, during the eleven-year period.  Those 19 players, who can be thought of as \"one degree removed from a world champion\", were added to the pool, bringing it up to 23 total players.  Then, another iteration was performed against the pool of 23 players, identifying an additional 141 players who faced at least four different opponents in the pool of 23 players, and also played at least 15 games total against that pool, during the eleven-year period.  Those 141 players, \"two degrees removed from a world champion\", were added to the pool, bringing it up to a total of 164 players.  This process was performed eight additional times, always pulling in additional players who had played at least four different opponents in the existing pool, and at least 15 games total against existing pool members.  The final iteration brought in 2,007 new players (all of whom can be thought of as being \"ten degrees separated from a world champion\") resulting in a final pool of 54,205 players: exactly the players who are included in the contest datasets.  Each player was randomly assigned an ID# from 1 to 54,205, and players in the contest are always referenced by this ID# rather than by name.\n\nPlease note that the source games included in the secondary training dataset and tertiary training dataset were not used during this iterative process of forming the pool of chess players.  The secondary and tertiary training datasets are optional and therefore it was desirable to make sure the pool of players was well-connected within the primary training dataset only.  Analysis of the primary training dataset will confirm that all players in the contest have at least 15 games in the primary training dataset, including at least 4 different opponents.\n\nInitial Rating List\n\nMost rating systems require an initial rating list in order to start the system.  In these systems, players can only receive new ratings through playing games against opponents who already have ratings themselves, and therefore the system cannot start from nothing, as nobody would ever get the first rating.  A notable exception is the Chessmetrics rating algorithm, which does not use prior rating lists and always generates a rating list using only game results.\n\nIn the previous contest we did not provide an initial rating list; participants were required to generate their own ratings using their own approach.  This essentially created a \"blended\" contest, where you had to develop a good algorithm to generate initial ratings and also a separate algorithm to update existing ratings.  Some people even implemented the Chessmetrics algorithm just to create their initial ratings.  In order to avoid the \"blended\" competition, and to focus mainly on optimizing the process for updating players' existing ratings, we decided to include initial ratings for everyone who actually had a FIDE rating going into month #1.  Almost 75% of the 54,205 players in the contest did not yet have a rating at that point, so the initial rating list only contains 14,118 players, but it still should suffice to seed the rating pool sufficiently so that it can increase to include all (or almost all) of the test players by the end of the training period.  In addition, the initial ratings have been randomly adjusted up or down by several rating points in order to slightly anonymize the list and discourage contest participants from decoding the list and determining the identities of each player.  Note that this behavior is explicitly forbidden by the rules of the contest. UPDATE: Follow-up Data (files starting with \"followup_\") These files represent a slightly larger dataset than is the case for the contest that was just completed. There were 54,205 players in the actual contest, with a training period spanning 132 months. The pool of players in the actual contest was formed by starting with four core world champions (Kasparov, Anand, Kramnik, Topalov) and then iteratively augmented by adding all players who had a total of 10+ games played (and at least 4 distinct opponents) against existing pool members across the first nine years of the training period, and only ten iterations of this were executed, leading to the 54,205 players. Instead, for this dataset, the restrictions were eased, as it allowed only 2+ distinct opponents (rather than 4+) and also the first eleven years of the training period were considered as a source of games (rather than just the first nine years). Four players who had no games after month #1 were removed, leaving exactly 88,000 players, exactly 19,000 of whom had initial ratings. And so with more players, we have more games. Other than having more players and more games, the data in these files is pretty much analogous to that of the actual contest, with a few minor exceptions: (1) The training period spans months 1-135 rather than 1-132 (2) The test period spans months 136-138 rather than 133-135 (3) In the previous training dataset, the PTID of the primary training games went from 1 to 1,840,124, and the STID of the secondary training games went from 2,000,001 to 2,312,511, and the TTID of the tertiary training games went from 3,000,001 to 3,265,577. This would allow you to uniquely identify each game by this ID even if you combined them into a single training dataset. However, in the larger dataset, there are more than 2 million primary training games, and so the same ID would have been shared by the end of the primary training games and the start of the secondary training games. Therefore the starting ID# for the secondary training set was changed to start at 2,500,001 instead. So now, the PTID of the primary training games goes from 1 to 2,327,683, and the STID of the secondary training games goes from 2,500,001 to 2,918,349, and the TTID of the tertiary training games goes from 3,000,001 to 3,394,324. (4) Player ID#'s are all exactly five digits, ranging from 10,001 to 98,000, rather than starting at 1 (as they did in the actual contest, where ID's ranged from 1 to 54,205) (5) Also note that the player ID#'s have been randomized again; there is no relationship between players' ID#'s in the actual contest and in these files (6) Once again there are many spurious games in the test set, and this time a much higher percentage of test set games are spurious."
    },
    {
        "name": "Wikipedia's Participation Challenge",
        "url": "https://www.kaggle.com/competitions/wikichallenge",
        "overview_text": "Overview text not found",
        "description_text": "This competition challenges data-mining experts to build a predictive model that predicts the number of edits an editor will make in the five months after the end date of the training dataset. The dataset is randomly sampled from the English Wikipedia dataset from the period January 2001 - August 2010. The objective of this competition is to quantitively understand what factors determine editing behavior. We hope to be able to answer questions, using these predictive models, why people stop editing or increase their pace of editing. Contestants are expected to build a predictive model that can be reused by the Wikimedia Foundation to forecast long term trends in the number of edits that we can expect.",
        "dataset_text": "During the period January 2001 - August 2010, 4.012.171 non-deleted registered editors made in total 272.213.427 edits on the English Wikipedia. A registered editor is a person who has created a useraccount on the English Wikipedia and has used this account to edit Wikipedia. The datafile consists of 44.514 sampled editors who contributed a total of 22.126.031 edits. For each editor all edits are included as long as they were made in the namespace range 0 to 5. The cumulative number of edits made by editors is highly skewed, most editors will have made less than 10 edits while the maximum number of edits made by a single editor in this dataset is 334.173. user_id (INT):  id of the editor who made the revision. This has been randomly recoded and does not match an editor id from the Wikimedia website. This variable can be stored as an integer. article_id (INT):  id of the article to which the revision belongs. This variable can be stored as an integer. revision_id (INT): id of the revision. This variable can be stored as an integer. namespace (TINYINT):  the name space of the article: Main (0), Talk (1), User (2), User Talk (3), Wikipedia (4), Wikipedia Talk (5). This means that some namespaces are missing from the trainingset, including:  Virtual namespaces This means that for some editors in the trainingset, their edit history will not be complete. timestamp (DATETIME): UTC timestamp when the revision was created. md5 (CHAR32):  MD5 hash based on the contents of a revision. This variable was constructed based on the full contents of a revision, including Wiki markup. reverted (TINYINT): 1 if the revision was reverted, 0 if not reverted. reverted_user_id (INT) -1 if reverted is 0, else it will contain the recoded user_id of the person who made the revert. Note, this person is not necessarily part of the trainingset. reverted_revision_id (INT): id of the revision it was reverted to. delta (INT): the increase / decrease in number of characters compared with the previous revision of the article (this includes wiki-markup). Careful: if a person has added 5 characters and removed 5 characters then delta will be 0.  cur_size (INT): The current size in number of characters of a revision (this includes wiki-markup). There are two additional files available that might be used for developing the algorithm: What data is not made available?"
    },
    {
        "name": "Allstate Claim Prediction Challenge",
        "url": "https://www.kaggle.com/competitions/ClaimPredictionChallenge",
        "overview_text": "Overview text not found",
        "description_text": "Risk varies widely from customer to customer, and a deep understanding of different risk factors helps predict the likelihood and cost of insurance claims. The goal of this competition is to better predict Bodily Injury Liability Insurance claim payments based on the characteristics of the insured customer\u2019s vehicle. Many factors contribute to the frequency and severity of car accidents including how, where and under what conditions people drive, as well as what they are driving.  Bodily Injury Liability Insurance covers other people\u2019s bodily injury or death for which the insured is responsible.   The goal of this competition is to predict Bodily Injury Liability Insurance claim payments based on the characteristics of the insured\u2019s vehicle.   ",
        "dataset_text": "Each row contains one year\u2019s worth information for insured vehicles.  Since the goal of this competition is to improve the ability to use vehicle characteristics to accurately predict insurance claim payments, the response variable (dollar amount of claims experienced for that vehicle in that year) has been adjusted to control for known non-vehicle effects. Some non-vehicle characteristics (labeled as such in the data dictionary) are included in the set of independent variables.  It is expected that no \u201cmain effects\u201d corresponding will be found for these non-vehicle variables, but there may be interesting interactions with the vehicle variables.  Calendar_Year is the year that the vehicle was insured.  Household_ID is a household identification number that allows year-to-year tracking of each household. Since a customer may insure multiple vehicles in one household, there may be multiple vehicles associated with each household identification number. \"Vehicle\" identifies these vehicles (but the same \"Vehicle\" number may not apply to the same vehicle from year to year). You also have the vehicle\u2019s model year and a coded form of make (manufacturer), model, and submodel.  The remaining columns contain miscellaneous vehicle characteristics, as well as other characteristics associated with the insurance policy.  See the \"data dictionary\" (data_dictionary.txt) for additional information. Our dataset naturally contained some missing values. Records containing missing values have been removed from the test data set but not from the training dataset. You can make use of the records with missing values, or completely ignore them if you wish. They are coded as \"?\". There are two datasets to download: training data and test data. You will use the training dataset to build your model, and will submit predictions for the test dataset. The training data has information from 2005-2007, while the test data has information from 2008 and 2009. Submissions should consist of a CSV file. Records from 2008 will be used to score the leaderboard, and records from 2009 will be used to determine the final winner."
    },
    {
        "name": "dunnhumby's Shopper Challenge",
        "url": "https://www.kaggle.com/competitions/dunnhumbychallenge",
        "overview_text": "Overview text not found",
        "description_text": "Description text not found",
        "dataset_text": "This data is no longer available for download."
    },
    {
        "name": "Don't Get Kicked!",
        "url": "https://www.kaggle.com/competitions/DontGetKicked",
        "overview_text": "Overview text not found",
        "description_text": "One of the biggest challenges of an auto dealership purchasing a used car at an auto auction is the risk of that the vehicle might have serious issues that prevent it from being sold to customers. The auto community calls these unfortunate purchases \"kicks\". Kicked cars often result when there are tampered odometers, mechanical issues the dealer is not able to address, issues with getting the vehicle title from the seller, or some other unforeseen problem. Kick cars can be very costly to dealers after transportation cost, throw-away repair work, and market losses in reselling the vehicle. Modelers who can figure out which cars have a higher risk of being kick can provide real value to dealerships trying to provide the best inventory selection possible to their customers. The challenge of this competition is to predict if the car purchased at the Auction is a Kick (bad buy).",
        "dataset_text": "Dataset description not found"
    },
    {
        "name": "Algorithmic Trading Challenge",
        "url": "https://www.kaggle.com/competitions/AlgorithmicTradingChallenge",
        "overview_text": "Overview text not found",
        "description_text": "The Algorithmic Trading Challenge is a forecasting competition which aims to encourage the development of new models to predict the stock market's short-term response following large trades. Contestants are asked to derive empirical models to predict the behaviour of bid and ask prices following such \"liquidity shocks\". Modelling market resiliency will improve trading strategy evaluation methods by increasing the realism of backtesting simulations, which currently assume zero market resiliency.",
        "dataset_text": "NOTE: The data files were all updated on November 11, 2011. Please download them again if you downloaded them before that date. Recent trade and quote data from the London Stock Exchange (LSE) is provided. The preprocessed dataset comprises observations of the limit order book before and after a liquidity shock (a trade that results in widening of the bid-ask spread). Table 1 shows the data schema provided for each liquidity shock.  Table 2 shows information about each data field.   Contestants are asked to submit predictions based on a test dataset containing 50,000 rows. A larger dataset is provided for training purposes."
    },
    {
        "name": "CHALEARN Gesture Challenge",
        "url": "https://www.kaggle.com/competitions/GestureChallenge",
        "overview_text": "Overview text not found",
        "description_text": "   You will never need a remote controller anymore, you will never need a light switch. Lying in bed in the dark, you will point to the ceiling to turn on the light, you will wave your hand to increase the temperature, you will make a T with your hands to turn on the TV set. You and your loved ones will feel safer at home, in parking lots, in airports: nobody will be watching, but computers will detect distressed people and suspicious activities. Computers will teach you how to effectively use gestures to enhance speech, to communicate with people who do not speak your language, to speak with deaf people, and you will easily learn many other sign languages to comminicate under water, to referee sports, etc. All that thanks to gesture recognition!   This is a challenge on gesture and sign language recognition using a Kinect camera. Kinect is revolutionizing the field of gesture recognition by providing an affordable 3D camera, which records both an RGB image and a depth image (using an infrared sensor). The challenge focuses on hand gestures. Applications include man-machine communication, translating sign languages for the deaf, video surveillance, and computer gaming. Check out some examples.   Every application needs a specialized gesture vocabulary. If we want gesture recognition to become part of everyday life, we need gesture recognition machines, which easily get tailored to new gesture vocabularies. This is why the focus of the challenge is on \u201cone-shot-learning\u201d of gestures, which means learning to recognize new categories of gestures from a single video clip of each gesture. The gestures will be drawn from a small vocabulary of gestures, generally related to a particular task, for instance, hand signals used by divers, finger codes to represent numerals, signals used by referees, or marchalling signals to guide vehicles or aircrafts.     This challenge is organized by CHALEARN and is sponsored in part by Microsoft (Kinect for Xbox 360). Other sponsors include Texas Instrument. This effort was initiated by the DARPA Deep Learning program and is supported by the US National Science Foundation (NSF) under grants ECCS 1128436 and ECCS 1128296 , the EU Pascal2 network of excellence. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors.",
        "dataset_text": "  Use Winzip to decompress/decrypt the data (a free trial version is available from the Winzip website).      "
    },
    {
        "name": "Raising Money to Fund an Organizational Mission",
        "url": "https://www.kaggle.com/competitions/Raising-Money-to-Fund-an-Organizational-Mission",
        "overview_text": "Overview text not found",
        "description_text": "Many organizations prospect for loyal supporters and donors by sending direct mail appeals. This is an effective way to build a large base, but can be very expensive and have a low efficiency. Eliminating likely non-donors is the key to running an efficient Prospecting program and ultimately to pursuing the mission of the organization. Help us help these organizations to target the best prospective donors and fund organizational goals!  Please note: This competition has rules that we have not used previously restricting what kinds of models are acceptable. Please see the rules page for more information.",
        "dataset_text": "You will predict \"Amount2\", which is a transformation of the donation amount (donation amount raised to the 1.15 power). Training data: kaggle_training_dataset_formatted2 Testing data: test. Please see \"Kaggle FAQ\" (downloadable with the data files) for other questions. Note: Category variables (\"ListID,\" \"Package,\" and \"Agency\") may be used as variables in the scoring algorithm, but using them to identify superior overall mailings (should be mailed 100%) and inferior mailings (should not be mailed) will not achieve the goal of maximizing model performance for each mailing. This is because we will take the top 75% of prospects in *each mailing* when evaluating performance.   TABLE OVERVIEW Kaggle_training_dataset_formatted2: Full mail history for the 11 months leading up to the Solution data.  Kaggle_donation_dataset_formatted2: Entire donation history pre-Training dataset for all organizations in Agency 1, 2 and 3 Kaggle_mail_dataset_formatted1a: Part of mail history before Training_dataset for Agency 1 Kaggle_mail_dataset_formatted1b: Second part of Agency 1 Training_dataset Kaggle_mail_dataset_formatted2: mail history before Training_dataset for Agency 2 Kaggle_mail_dataset_formatted3: mail history before Training_dataset for Agency 3 Demo_per_formatted: Demographic information by 9-digit zip code Zip_perf: summary of historical mail performance by 5-digit zip training_sample: This is a random 10% sample of the training data, provided for your convenience. The sample submission is based on this dataset rather than the full training dataset."
    },
    {
        "name": "Practice Fusion Analyze This! 2012 - Open Challenge",
        "url": "https://www.kaggle.com/competitions/pf2012-at",
        "overview_text": "Overview text not found",
        "description_text": "Description text not found",
        "dataset_text": "The data sets for this competition have been removed at the request of Practice Fusion. The Analyze This! data set consists of 17 different files, 2 common files and 15 training set files. These files describe the records for 10,000 patients. They are in comma separated value (csv) format. A test set will be released at the start of the predictive modeling competition. Common files Data set and column names Additional Notes"
    },
    {
        "name": "Blue Book for Bulldozers",
        "url": "https://www.kaggle.com/competitions/bluebook-for-bulldozers",
        "overview_text": "Overview text not found",
        "description_text": " The goal of the contest is to predict the sale price of a particular piece of heavy equiment at auction based on it's usage, equipment type, and configuaration.  The data is sourced from auction result postings and includes information on usage and equipment configurations. Fast Iron is creating a \"blue book for bull dozers,\" for customers to value what their heavy equipment fleet is worth at auction. Fast Iron are a content-focused business that aids customers in creating enterprise data standards, cleansing data, and maintaining clean data. Utilizing proprietary applications and an ever growing data cleansing team, Fast Iron has normalized data for more than 2.5 million machine and customer records for the heavy equipment industry. This competition was launched under the Kaggle Startup Program. If you're a startup with a predictive modelling challenge, please apply! Photo credits: Antonis Lamnatos",
        "dataset_text": "View and download the benchmark code from Github For this competition, you are predicting the sale price of bulldozers sold at auctions. The data for this competition is split into three parts: The key fields are in train.csv are: The machine_appendix.csv file contains the correct year manufactured for a given machine along with the make, model, and product class details. There is one machine id for every machine in all the competition datasets (training, evaluation, etc.)."
    },
    {
        "name": "Getting Started",
        "url": "https://www.kaggle.com/competitions/getting-started",
        "overview_text": "Overview text not found",
        "description_text": "Insert your description here",
        "dataset_text": "Dataset description not found"
    },
    {
        "name": "The Marinexplore and Cornell University Whale Detection Challenge",
        "url": "https://www.kaggle.com/competitions/whale-detection-challenge",
        "overview_text": "Overview text not found",
        "description_text": "Read the summary of the competition for a quick overview of the impact of the results. We depend on shipping industry's uninterrupted ability to transport goods across long distances. Navigation technologies combine accurate position and environmental data to calculate optimal transport routes. Accounting for and reducing the impact of commercial shipping on the ocean\u2019s environment, while achieving commercial sustainability, is of increasing importance, especially as it relates to the influence of cumulative noise \u201cfootprints\u201d on the great whales. Marinexplore is organizing the Planet's ocean data with the leading community of ocean professionals. One of the important datasets consists of acoustic recordings that can be used to detect species inhabiting the global ocean. Knowledge about animal locations can be utilized in industrial operations. Cornell University's Bioacoustic Research Program has extensive experience in identifying endangered whale species and has deployed a 24/7 buoy network to guide ships from colliding with the world's last 400 North Atlantic right whales.  Illustration of ships navigating safely around the habitat of whales. Right whales make a half-dozen types of sounds, but the characteristic up-call is the one identified by the auto-detection buoys. The up-call is useful because it\u2019s distinctive and right whales give it often. A type of \u201ccontact call,\u201d the up-call is a little like small talk--the sound of a right whale going about its day and letting others know it\u2019s nearby. In this recording the up-call is easy to hear--a deep, rising \u201cwhoop\u201d that lasts about a second: Right whale up-call Marinexplore and Cornell researchers challenge YOU to beat the existing whale detection algorithm identifying the right whale calls. This will advance ship routing decisions in the region. [For details on the buoy network see a paper published by Acoustical Society of America.] Read the summary of the competition for a quick overview of the impact of the results.",
        "dataset_text": "Everything is packaged in the file \"whale_data.zip\". \"small_data_sample_revised.zip\" contains 10 example clips if you want to explore the data format before downloading the full data file. The data consists of 30,000 training samples and 54,503 testing samples. Each candidate is a 2-second .aiff sound clip with a sample rate of 2 kHz. The file \"train.csv\" gives the labels for the train set. Candidates that contain a right whale call have label=1, otherwise label=0.  Example of a Clip Containing Right Whale Call These clips contain any mixture of right whale calls, non-biological noise, or other whale calls. Transforming the time series into the frequency domain via FFT might be useful. For a quick exploration of the clips, you may find Cornell's RavenLite software useful."
    },
    {
        "name": "Packing Santa's Sleigh",
        "url": "https://www.kaggle.com/competitions/packing-santas-sleigh",
        "overview_text": "Overview text not found",
        "description_text": "It's that time of year again, when Santa and his helpers gear up for their big night. Last year's path recommendations were such a success that Santa is back for more. As the latest in a line of many data science converts, Santa is looking to you to help pack his sleigh.   \u266b Then one foggy Christmas Eve\nSanta came to say\n\"Rudolph with your code so bright\nwon't you fill my sleigh tonight?\" \u266b  Given a list of presents, pack them in Santa's sleigh as compactly as possible and in the best order possible. The sleigh and presents are discretized and described in units of the fundamental length unit \\\\(\\ell\\\\). The sleigh is 1000 x 1000 with infinite vertical extent as needed by your highest placed present. The cells of the sleigh go from 1 to 1000 for the length and width, and 1 to infinity in height. Presents come in random sizes and are represented by their extent in the x, y, and z dimensions. Each present has a PresentId, a number which determines the order in which it is to be delivered. Ideally, Present 1 is to be delivered first, Present 2 second, etc.  Present 1 is 2 x 5 x 3 \\\\(\\ell^3\\\\) and Present 2 is 243 x 207 x 73 \\\\(\\ell^3\\\\). Presents can be packed in any orientation provided they are parallel and perpendicular to the x-y-z axes, meaning they can be rotated in any direction by multiples of 90\\\\(^\\circ\\\\) but not, for example, by 60\\\\(^\\circ\\\\).   Please see the evaluation page to learn how your packing configurations will be scored and for the submission file schema. This competition is brought to you by MathWorks, creators of MATLAB\u00ae and Simulink\u00ae. Learn more about MathWorks.",
        "dataset_text": "Note: Both files contain the same list of presents and present dimensions and you don't necessarily need to download both versions of the files.  You may have access to MATLAB at your university or workplace. Check\nwith your professor or IT administrator. If you do not have access, there are\nseveral ways to get MATLAB to help pack Santa\u2019s sleigh, as well as for other\nKaggle competitions:"
    }
]